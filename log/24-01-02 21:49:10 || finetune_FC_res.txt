Start time: 2024-01-02 21:49:10.237971
batch i:1
batch i:2
batch i:3
batch i:4
learning rate0.0013333333333333333, loss:2.853928327560425
batch i:5
learning rate0.0008888888888888888, loss:3.166938543319702
batch i:6
learning rate0.0005925925925925926, loss:2.5069050788879395
batch i:7
learning rate0.0003950617283950617, loss:2.5314598083496094
batch i:8
learning rate0.0002633744855967078, loss:2.4606828689575195
batch i:9
learning rate0.0001755829903978052, loss:2.434574604034424
batch i:10
learning rate0.0001755829903978052, loss:2.456963539123535
batch i:11
learning rate0.0001755829903978052, loss:2.536679744720459
batch i:12
learning rate0.0001755829903978052, loss:2.5381016731262207
batch i:13
learning rate0.0001755829903978052, loss:2.529843330383301
batch i:14
learning rate0.0001755829903978052, loss:2.7009947299957275
batch i:15
learning rate0.0001755829903978052, loss:2.8580315113067627
batch i:16
learning rate0.0001755829903978052, loss:2.7392983436584473
batch i:17
learning rate0.0001755829903978052, loss:2.6083827018737793
batch i:18
learning rate0.0001755829903978052, loss:2.6264498233795166
batch i:19
learning rate0.0001755829903978052, loss:2.6548423767089844
batch i:20
learning rate0.0001755829903978052, loss:2.6009020805358887
batch i:21
learning rate0.0001755829903978052, loss:2.5748629570007324
batch i:22
learning rate0.0001755829903978052, loss:2.553009510040283
batch i:23
learning rate0.0001755829903978052, loss:2.642416477203369
batch i:24
learning rate0.0001755829903978052, loss:2.6011762619018555
batch i:25
learning rate0.0001755829903978052, loss:2.5419764518737793
batch i:26
learning rate0.0001755829903978052, loss:2.5511083602905273
batch i:27
learning rate0.0001755829903978052, loss:2.539888858795166
batch i:28
learning rate0.0001755829903978052, loss:2.583465099334717
batch i:29
learning rate0.0002633744855967078, loss:2.6454970836639404
batch i:30
learning rate0.0002633744855967078, loss:2.5811991691589355
batch i:31
learning rate0.0002633744855967078, loss:2.594109058380127
batch i:32
learning rate0.0002633744855967078, loss:2.6036839485168457
batch i:33
learning rate0.0001755829903978052, loss:2.5227770805358887
batch i:34
learning rate0.0001755829903978052, loss:2.661539077758789
batch i:35
learning rate0.0001755829903978052, loss:2.604313373565674
batch i:36
learning rate0.0001755829903978052, loss:2.5142295360565186
batch i:37
learning rate0.0001755829903978052, loss:2.590653419494629
batch i:38
learning rate0.0001755829903978052, loss:2.595372200012207
batch i:39
learning rate0.0001755829903978052, loss:2.5798451900482178
batch i:40
learning rate0.0001755829903978052, loss:2.6322836875915527
batch i:41
learning rate0.0001755829903978052, loss:2.629999876022339
batch i:42
learning rate0.0001755829903978052, loss:2.4936864376068115
batch i:43
learning rate0.0001755829903978052, loss:2.5258331298828125
batch i:44
learning rate0.0001755829903978052, loss:2.5250797271728516
batch i:45
learning rate0.0001755829903978052, loss:2.533599853515625
batch i:46
learning rate0.0001755829903978052, loss:2.541736364364624
batch i:47
learning rate0.0001755829903978052, loss:2.516542434692383
batch i:48
learning rate0.0001755829903978052, loss:2.466564416885376
batch i:49
learning rate0.0001755829903978052, loss:2.502983331680298
batch i:50
learning rate0.0001755829903978052, loss:2.493802785873413
batch i:51
learning rate0.0001755829903978052, loss:2.4802417755126953
batch i:52
learning rate0.0001755829903978052, loss:2.553805351257324
batch i:53
learning rate0.0001755829903978052, loss:2.526956081390381
batch i:54
learning rate0.0001755829903978052, loss:2.5561909675598145
batch i:55
learning rate0.0001755829903978052, loss:2.5731897354125977
batch i:56
learning rate0.0001755829903978052, loss:2.5570380687713623
batch i:57
learning rate0.0001755829903978052, loss:2.5885519981384277
batch i:58
learning rate0.0001755829903978052, loss:2.4911084175109863
batch i:59
learning rate0.0001755829903978052, loss:2.505887031555176
batch i:60
learning rate0.0001755829903978052, loss:2.5367798805236816
batch i:61
learning rate0.0001755829903978052, loss:2.562161922454834
batch i:62
learning rate0.0001755829903978052, loss:2.4628682136535645
batch i:63
learning rate0.0001755829903978052, loss:2.5639901161193848
batch i:64
learning rate0.0001755829903978052, loss:2.5122838020324707
batch i:65
learning rate0.0001755829903978052, loss:2.5622482299804688
batch i:66
learning rate0.0001755829903978052, loss:2.4802026748657227
batch i:67
learning rate0.0001755829903978052, loss:2.5020253658294678
batch i:68
learning rate0.0001755829903978052, loss:2.617363452911377
batch i:69
learning rate0.0001755829903978052, loss:2.5340781211853027
batch i:70
learning rate0.0001755829903978052, loss:2.521421432495117
batch i:71
learning rate0.0001755829903978052, loss:2.5115575790405273
batch i:72
learning rate0.0001755829903978052, loss:2.496943235397339
batch i:73
learning rate0.0001755829903978052, loss:2.582435131072998
batch i:74
learning rate0.0001755829903978052, loss:2.4177584648132324
batch i:75
learning rate0.0001755829903978052, loss:2.515872001647949
batch i:76
learning rate0.0001755829903978052, loss:2.5276670455932617
batch i:77
learning rate0.0001755829903978052, loss:2.5682201385498047
batch i:78
learning rate0.0001755829903978052, loss:2.5085654258728027
batch i:79
learning rate0.0001755829903978052, loss:2.681858539581299
batch i:80
learning rate0.0001755829903978052, loss:2.5478458404541016
batch i:81
learning rate0.0001755829903978052, loss:2.645076274871826
batch i:82
learning rate0.0001755829903978052, loss:2.6716156005859375
batch i:83
learning rate0.0001755829903978052, loss:2.5970985889434814
batch i:84
learning rate0.0001755829903978052, loss:2.770240545272827
batch i:85
learning rate0.0001755829903978052, loss:2.738293170928955
batch i:86
learning rate0.0001755829903978052, loss:2.6976945400238037
batch i:87
learning rate0.0001755829903978052, loss:2.656320571899414
batch i:88
learning rate0.0001755829903978052, loss:2.6733407974243164
batch i:89
learning rate0.0001755829903978052, loss:2.6448731422424316
batch i:90
learning rate0.0001755829903978052, loss:2.6221261024475098
batch i:91
learning rate0.0001755829903978052, loss:2.7024459838867188
batch i:92
learning rate0.0001755829903978052, loss:2.5961198806762695
batch i:93
learning rate0.0001755829903978052, loss:2.5855579376220703
batch i:94
learning rate0.0001755829903978052, loss:2.5930967330932617
batch i:95
learning rate0.0001755829903978052, loss:2.520854949951172
batch i:96
learning rate0.0001755829903978052, loss:2.5920519828796387
batch i:97
learning rate0.0001755829903978052, loss:2.623776912689209
batch i:98
learning rate0.0001755829903978052, loss:2.549985647201538
batch i:99
learning rate0.0001755829903978052, loss:2.535219192504883
batch i:100
learning rate0.0001755829903978052, loss:2.6227686405181885
batch i:101
learning rate0.0001755829903978052, loss:2.5576272010803223
batch i:102
learning rate0.0001755829903978052, loss:2.593196153640747
batch i:103
learning rate0.00011705532693187012, loss:2.5823347568511963
batch i:104
learning rate0.00011705532693187012, loss:2.5842347145080566
batch i:105
learning rate0.00011705532693187012, loss:2.6431541442871094
batch i:106
learning rate0.00011705532693187012, loss:2.53895902633667
batch i:107
learning rate0.00011705532693187012, loss:2.5877535343170166
batch i:108
learning rate0.00011705532693187012, loss:2.473052978515625
batch i:109
learning rate0.00011705532693187012, loss:2.5817408561706543
batch i:110
learning rate0.00011705532693187012, loss:2.6190590858459473
batch i:111
learning rate0.00011705532693187012, loss:2.6502742767333984
batch i:112
learning rate0.00011705532693187012, loss:2.6709821224212646
batch i:113
learning rate0.00011705532693187012, loss:2.754056930541992
batch i:114
learning rate0.00011705532693187012, loss:2.772097110748291
batch i:115
learning rate0.00011705532693187012, loss:2.6217479705810547
batch i:116
learning rate0.00011705532693187012, loss:2.7693004608154297
batch i:117
learning rate0.00011705532693187012, loss:2.7461490631103516
batch i:118
learning rate0.00011705532693187012, loss:2.7505838871002197
batch i:119
learning rate0.00011705532693187012, loss:2.7709760665893555
batch i:120
learning rate0.00011705532693187012, loss:2.8200042247772217
batch i:121
learning rate0.00011705532693187012, loss:2.7482681274414062
batch i:122
learning rate0.00011705532693187012, loss:2.751333236694336
batch i:123
learning rate0.00011705532693187012, loss:2.766582489013672
batch i:124
learning rate0.00011705532693187012, loss:2.702407121658325
batch i:125
learning rate0.00011705532693187012, loss:2.7609434127807617
batch i:126
learning rate0.00011705532693187012, loss:2.769105911254883
batch i:127
learning rate0.00011705532693187012, loss:2.825438976287842
batch i:128
learning rate0.00011705532693187012, loss:2.864668369293213
batch i:129
learning rate0.00011705532693187012, loss:2.785604953765869
batch i:130
learning rate0.00011705532693187012, loss:2.8649988174438477
batch i:131
learning rate0.00011705532693187012, loss:2.817166328430176
batch i:132
learning rate0.0001755829903978052, loss:2.8121137619018555
batch i:133
learning rate0.0001755829903978052, loss:2.8108010292053223
batch i:134
learning rate0.0001755829903978052, loss:2.8834757804870605
batch i:135
learning rate0.0001755829903978052, loss:2.810004711151123
batch i:136
learning rate0.0001755829903978052, loss:2.792062520980835
batch i:137
learning rate0.0001755829903978052, loss:2.816737413406372
batch i:138
learning rate0.0001755829903978052, loss:2.742180109024048
batch i:139
learning rate0.0001755829903978052, loss:2.819657802581787
batch i:140
learning rate0.0001755829903978052, loss:2.7285988330841064
batch i:141
learning rate0.0001755829903978052, loss:2.766225814819336
batch i:142
learning rate0.0001755829903978052, loss:2.781002998352051
batch i:143
learning rate0.0001755829903978052, loss:2.7383790016174316
batch i:144
learning rate0.0001755829903978052, loss:2.7383246421813965
batch i:145
learning rate0.0001755829903978052, loss:2.8140909671783447
batch i:146
learning rate0.0001755829903978052, loss:2.728781223297119
batch i:147
learning rate0.0001755829903978052, loss:2.7889862060546875
batch i:148
learning rate0.0001755829903978052, loss:2.690845489501953
batch i:149
learning rate0.0001755829903978052, loss:2.6204934120178223
batch i:150
learning rate0.0001755829903978052, loss:2.7203545570373535
batch i:151
learning rate0.0001755829903978052, loss:2.7149953842163086
batch i:152
learning rate0.0001755829903978052, loss:2.5988669395446777
batch i:153
learning rate0.0001755829903978052, loss:2.709916353225708
batch i:154
learning rate0.0001755829903978052, loss:2.6592931747436523
batch i:155
learning rate0.0001755829903978052, loss:2.78096342086792
batch i:156
learning rate0.0001755829903978052, loss:2.666767120361328
batch i:157
learning rate0.0001755829903978052, loss:2.746898651123047
batch i:158
learning rate0.0001755829903978052, loss:2.748243570327759
batch i:159
learning rate0.0001755829903978052, loss:2.739321708679199
batch i:160
learning rate0.0001755829903978052, loss:2.799027442932129
batch i:161
learning rate0.0001755829903978052, loss:2.6942710876464844
batch i:162
learning rate0.0001755829903978052, loss:2.6330151557922363
batch i:163
learning rate0.0001755829903978052, loss:2.871044635772705
batch i:164
learning rate0.0001755829903978052, loss:2.7725679874420166
batch i:165
learning rate0.0001755829903978052, loss:2.8336424827575684
batch i:166
learning rate0.0001755829903978052, loss:2.8808164596557617
batch i:167
learning rate0.0001755829903978052, loss:2.788020133972168
batch i:168
learning rate0.0001755829903978052, loss:2.880054473876953
batch i:169
learning rate0.0001755829903978052, loss:2.948133707046509
batch i:170
learning rate0.0001755829903978052, loss:2.922482490539551
batch i:171
learning rate0.0001755829903978052, loss:2.7924485206604004
batch i:172
learning rate0.0001755829903978052, loss:2.786044120788574
batch i:173
learning rate0.0001755829903978052, loss:2.808115243911743
batch i:174
learning rate0.0001755829903978052, loss:2.836325168609619
batch i:175
learning rate0.0001755829903978052, loss:2.9145116806030273
batch i:176
learning rate0.0001755829903978052, loss:2.9695699214935303
batch i:177
learning rate0.0001755829903978052, loss:2.8050665855407715
batch i:178
learning rate0.0001755829903978052, loss:2.7682745456695557
batch i:179
learning rate0.0001755829903978052, loss:2.6657462120056152
batch i:180
learning rate0.0001755829903978052, loss:2.725588321685791
batch i:181
learning rate0.0001755829903978052, loss:2.7098169326782227
batch i:182
learning rate0.0001755829903978052, loss:2.7169032096862793
batch i:183
learning rate0.0001755829903978052, loss:2.8099498748779297
batch i:184
learning rate0.0001755829903978052, loss:2.842531442642212
batch i:185
learning rate0.0001755829903978052, loss:2.7904584407806396
batch i:186
learning rate0.0001755829903978052, loss:2.6934332847595215
batch i:187
learning rate0.0001755829903978052, loss:2.7610363960266113
batch i:188
learning rate0.00011705532693187012, loss:2.802731990814209
batch i:189
learning rate0.00011705532693187012, loss:2.772287130355835
batch i:190
learning rate0.00011705532693187012, loss:2.842468500137329
batch i:191
learning rate0.00011705532693187012, loss:2.82210373878479
batch i:192
learning rate0.00011705532693187012, loss:2.790379047393799
batch i:193
learning rate0.00011705532693187012, loss:2.7604265213012695
batch i:194
learning rate0.00011705532693187012, loss:2.802800178527832
batch i:195
learning rate0.00011705532693187012, loss:2.693739175796509
batch i:196
learning rate0.00011705532693187012, loss:2.797285556793213
batch i:197
learning rate0.00011705532693187012, loss:2.8080625534057617
batch i:198
learning rate0.00011705532693187012, loss:2.8076517581939697
batch i:199
learning rate0.00011705532693187012, loss:2.7269983291625977
batch i:200
learning rate0.00011705532693187012, loss:2.7746033668518066
batch i:201
learning rate0.00011705532693187012, loss:2.7912282943725586
batch i:202
learning rate0.00011705532693187012, loss:2.729785203933716
batch i:203
learning rate0.00011705532693187012, loss:2.7290961742401123
batch i:204
learning rate0.00011705532693187012, loss:2.7359914779663086
batch i:205
learning rate0.00011705532693187012, loss:2.7711949348449707
batch i:206
learning rate0.00011705532693187012, loss:2.7221484184265137
batch i:207
learning rate0.00011705532693187012, loss:2.804567813873291
batch i:208
learning rate0.00011705532693187012, loss:2.742429494857788
batch i:209
learning rate0.00011705532693187012, loss:2.8191957473754883
batch i:210
learning rate0.00011705532693187012, loss:2.7542126178741455
batch i:211
learning rate0.00011705532693187012, loss:2.6922624111175537
batch i:212
learning rate0.00011705532693187012, loss:2.7932307720184326
batch i:213
learning rate0.00011705532693187012, loss:2.7620761394500732
batch i:214
learning rate0.00011705532693187012, loss:2.785367488861084
batch i:215
learning rate0.00011705532693187012, loss:2.7935750484466553
batch i:216
learning rate0.00011705532693187012, loss:2.8287651538848877
batch i:217
learning rate0.00011705532693187012, loss:2.7212164402008057
batch i:218
learning rate0.00011705532693187012, loss:2.7789199352264404
batch i:219
learning rate0.00011705532693187012, loss:2.761445999145508
batch i:220
learning rate0.00011705532693187012, loss:2.8212246894836426
batch i:221
learning rate0.00011705532693187012, loss:2.7206339836120605
batch i:222
learning rate0.00011705532693187012, loss:2.783810615539551
batch i:223
learning rate0.00011705532693187012, loss:2.850257635116577
batch i:224
learning rate0.00011705532693187012, loss:2.808001756668091
batch i:225
learning rate0.00011705532693187012, loss:2.739962577819824
batch i:226
learning rate0.00011705532693187012, loss:2.7692418098449707
batch i:227
learning rate0.00011705532693187012, loss:2.905418872833252
batch i:228
learning rate0.00011705532693187012, loss:2.7652289867401123
batch i:229
learning rate0.00011705532693187012, loss:2.7951595783233643
batch i:230
learning rate0.00011705532693187012, loss:2.7544407844543457
batch i:231
learning rate0.00011705532693187012, loss:2.771472454071045
batch i:232
learning rate0.00011705532693187012, loss:2.7817459106445312
batch i:233
learning rate0.00011705532693187012, loss:2.689845085144043
batch i:234
learning rate0.00011705532693187012, loss:2.7293598651885986
batch i:235
learning rate0.00011705532693187012, loss:2.820969820022583
batch i:236
learning rate0.00011705532693187012, loss:2.7273592948913574
batch i:237
learning rate0.00011705532693187012, loss:2.7718584537506104
batch i:238
learning rate0.00011705532693187012, loss:2.8479576110839844
batch i:239
learning rate0.00011705532693187012, loss:2.8309803009033203
batch i:240
learning rate0.00011705532693187012, loss:2.7806949615478516
batch i:241
learning rate0.00011705532693187012, loss:2.840038299560547
batch i:242
learning rate0.00011705532693187012, loss:2.925442695617676
batch i:243
learning rate0.00011705532693187012, loss:2.8092124462127686
batch i:244
learning rate0.00011705532693187012, loss:2.7850189208984375
batch i:245
learning rate0.00011705532693187012, loss:2.9794321060180664
batch i:246
learning rate0.00011705532693187012, loss:2.83966064453125
batch i:247
learning rate0.00011705532693187012, loss:2.8465561866760254
batch i:248
learning rate0.00011705532693187012, loss:2.874088764190674
batch i:249
learning rate0.00011705532693187012, loss:2.9312245845794678
batch i:250
learning rate0.00011705532693187012, loss:2.767824411392212
current self-play batch: 250
num_playouts:1000, win: 10, lose: 0, tie:0
average time: 155.7620092153549
New best policy from pure MCTS
batch i:251
learning rate0.00011705532693187012, loss:2.7954070568084717
batch i:252
learning rate0.00011705532693187012, loss:2.792714834213257
batch i:253
learning rate0.00011705532693187012, loss:2.7388086318969727
batch i:254
learning rate0.00011705532693187012, loss:2.8603949546813965
batch i:255
learning rate0.00011705532693187012, loss:2.725739002227783
batch i:256
learning rate0.00011705532693187012, loss:2.851090431213379
batch i:257
learning rate0.00011705532693187012, loss:2.8206100463867188
batch i:258
learning rate0.00011705532693187012, loss:2.8141486644744873
batch i:259
learning rate0.00011705532693187012, loss:2.817924976348877
batch i:260
learning rate0.00011705532693187012, loss:2.8292646408081055
batch i:261
learning rate0.00011705532693187012, loss:2.806612014770508
batch i:262
learning rate0.00011705532693187012, loss:2.852067708969116
batch i:263
learning rate0.00011705532693187012, loss:2.8464789390563965
batch i:264
learning rate0.00011705532693187012, loss:2.8923327922821045
batch i:265
learning rate0.00011705532693187012, loss:2.796389102935791
batch i:266
learning rate0.00011705532693187012, loss:2.8359389305114746
batch i:267
learning rate0.00011705532693187012, loss:2.8489108085632324
batch i:268
learning rate0.00011705532693187012, loss:2.884622097015381
batch i:269
learning rate0.00011705532693187012, loss:2.882793664932251
batch i:270
learning rate0.00011705532693187012, loss:2.9059739112854004
batch i:271
learning rate0.00011705532693187012, loss:2.856139659881592
batch i:272
learning rate0.00011705532693187012, loss:2.884932518005371
batch i:273
learning rate0.00011705532693187012, loss:2.8770980834960938
batch i:274
learning rate0.00011705532693187012, loss:3.002674102783203
batch i:275
learning rate0.00011705532693187012, loss:2.9302451610565186
batch i:276
learning rate0.00011705532693187012, loss:2.915579319000244
batch i:277
learning rate0.00011705532693187012, loss:2.9150588512420654
batch i:278
learning rate0.00011705532693187012, loss:2.859436511993408
batch i:279
learning rate0.00011705532693187012, loss:2.8807458877563477
batch i:280
learning rate0.00011705532693187012, loss:2.863093852996826
batch i:281
learning rate0.00011705532693187012, loss:2.904895305633545
batch i:282
learning rate0.00011705532693187012, loss:2.882230758666992
batch i:283
learning rate0.00011705532693187012, loss:2.899913787841797
batch i:284
learning rate0.00011705532693187012, loss:3.0171310901641846
batch i:285
learning rate0.00011705532693187012, loss:2.9126973152160645
batch i:286
learning rate0.00011705532693187012, loss:2.920865058898926
batch i:287
learning rate0.00011705532693187012, loss:2.863129138946533
batch i:288
learning rate0.00011705532693187012, loss:3.0160655975341797
batch i:289
learning rate0.00011705532693187012, loss:3.045487403869629
batch i:290
learning rate0.00011705532693187012, loss:2.8106541633605957
batch i:291
learning rate0.00011705532693187012, loss:2.9862537384033203
batch i:292
learning rate0.00011705532693187012, loss:2.9566736221313477
batch i:293
learning rate0.00011705532693187012, loss:3.0537009239196777
batch i:294
learning rate0.00011705532693187012, loss:2.9676315784454346
batch i:295
learning rate0.00011705532693187012, loss:2.9780688285827637
batch i:296
learning rate0.00011705532693187012, loss:2.991440773010254
batch i:297
learning rate0.00011705532693187012, loss:2.8098158836364746
batch i:298
learning rate0.00011705532693187012, loss:2.9068305492401123
batch i:299
learning rate0.00011705532693187012, loss:2.855194330215454
batch i:300
learning rate0.00011705532693187012, loss:2.946254253387451
batch i:301
learning rate0.00011705532693187012, loss:2.8668863773345947
batch i:302
learning rate0.00011705532693187012, loss:2.862029790878296
batch i:303
learning rate0.00011705532693187012, loss:2.8835744857788086
batch i:304
learning rate0.00011705532693187012, loss:2.9009087085723877
batch i:305
learning rate0.00011705532693187012, loss:2.965385913848877
batch i:306
learning rate0.00011705532693187012, loss:2.956449031829834
batch i:307
learning rate0.00011705532693187012, loss:2.9222609996795654
batch i:308
learning rate0.00011705532693187012, loss:2.9458508491516113
batch i:309
learning rate0.00011705532693187012, loss:2.9641361236572266
batch i:310
learning rate0.00011705532693187012, loss:2.8991973400115967
batch i:311
learning rate0.00011705532693187012, loss:2.7726781368255615
batch i:312
learning rate0.00011705532693187012, loss:2.9855663776397705
batch i:313
learning rate0.00011705532693187012, loss:2.9333887100219727
batch i:314
learning rate0.00011705532693187012, loss:2.9562480449676514
batch i:315
learning rate0.00011705532693187012, loss:2.983664035797119
batch i:316
learning rate0.00011705532693187012, loss:2.943236827850342
batch i:317
learning rate0.00011705532693187012, loss:2.965137481689453
batch i:318
learning rate0.00011705532693187012, loss:2.947096824645996
batch i:319
learning rate0.00011705532693187012, loss:2.9663496017456055
batch i:320
learning rate0.00011705532693187012, loss:2.8792943954467773
batch i:321
learning rate0.00011705532693187012, loss:2.980104923248291
batch i:322
learning rate0.00011705532693187012, loss:3.0287740230560303
batch i:323
learning rate0.00011705532693187012, loss:2.8660988807678223
batch i:324
learning rate0.00011705532693187012, loss:2.9842028617858887
batch i:325
learning rate0.00011705532693187012, loss:2.9270918369293213
batch i:326
learning rate0.00011705532693187012, loss:2.9512858390808105
batch i:327
learning rate0.00011705532693187012, loss:2.9372737407684326
batch i:328
learning rate0.00011705532693187012, loss:2.927432060241699
batch i:329
learning rate0.00011705532693187012, loss:2.881542205810547
batch i:330
learning rate0.00011705532693187012, loss:3.080099582672119
batch i:331
learning rate0.00011705532693187012, loss:3.006985664367676
batch i:332
learning rate0.00011705532693187012, loss:2.867917537689209
batch i:333
learning rate0.00011705532693187012, loss:2.8834760189056396
batch i:334
learning rate0.00011705532693187012, loss:2.948057174682617
batch i:335
learning rate0.00011705532693187012, loss:3.0267539024353027
batch i:336
learning rate0.00011705532693187012, loss:3.0239760875701904
batch i:337
learning rate0.00011705532693187012, loss:3.0139522552490234
batch i:338
learning rate0.00011705532693187012, loss:2.828732490539551
batch i:339
learning rate0.00011705532693187012, loss:2.8606507778167725
batch i:340
learning rate0.00011705532693187012, loss:2.9207606315612793
batch i:341
learning rate0.00011705532693187012, loss:2.8626320362091064
batch i:342
learning rate0.00011705532693187012, loss:2.7967886924743652
batch i:343
learning rate0.00011705532693187012, loss:2.765052080154419
batch i:344
learning rate0.00011705532693187012, loss:2.7549824714660645
batch i:345
learning rate0.00011705532693187012, loss:2.7966394424438477
batch i:346
learning rate0.00011705532693187012, loss:2.7578201293945312
batch i:347
learning rate0.00011705532693187012, loss:2.777050018310547
batch i:348
learning rate0.00011705532693187012, loss:2.849269390106201
batch i:349
learning rate0.00011705532693187012, loss:2.789827346801758
batch i:350
learning rate0.00011705532693187012, loss:2.7909023761749268
batch i:351
learning rate0.00011705532693187012, loss:2.773254871368408
batch i:352
learning rate0.00011705532693187012, loss:2.7682595252990723
batch i:353
learning rate0.00011705532693187012, loss:2.764070510864258
batch i:354
learning rate0.00011705532693187012, loss:2.778846025466919
batch i:355
learning rate0.00011705532693187012, loss:2.831965684890747
batch i:356
learning rate0.00011705532693187012, loss:2.867124319076538
batch i:357
learning rate0.00011705532693187012, loss:2.794224262237549
batch i:358
learning rate0.00011705532693187012, loss:2.7665486335754395
batch i:359
learning rate0.00011705532693187012, loss:2.7822060585021973
batch i:360
learning rate0.00011705532693187012, loss:2.6688642501831055
batch i:361
learning rate0.00011705532693187012, loss:2.769440174102783
batch i:362
learning rate0.00011705532693187012, loss:2.8371763229370117
batch i:363
learning rate0.00011705532693187012, loss:2.7231974601745605
batch i:364
learning rate0.00011705532693187012, loss:2.7850797176361084
batch i:365
learning rate0.00011705532693187012, loss:2.7815144062042236
batch i:366
learning rate0.00011705532693187012, loss:2.7264621257781982
batch i:367
learning rate0.00011705532693187012, loss:2.82623553276062
batch i:368
learning rate0.00011705532693187012, loss:2.764788866043091
batch i:369
learning rate0.00011705532693187012, loss:2.8042521476745605
batch i:370
learning rate0.00011705532693187012, loss:2.8012585639953613
batch i:371
learning rate0.00011705532693187012, loss:2.8644611835479736
batch i:372
learning rate0.00011705532693187012, loss:2.7918944358825684
batch i:373
learning rate0.00011705532693187012, loss:2.7999448776245117
batch i:374
learning rate0.00011705532693187012, loss:2.835141181945801
batch i:375
learning rate0.00011705532693187012, loss:2.732950210571289
batch i:376
learning rate0.00011705532693187012, loss:2.7852654457092285
batch i:377
learning rate0.00011705532693187012, loss:2.8468899726867676
batch i:378
learning rate0.00011705532693187012, loss:2.7539312839508057
batch i:379
learning rate0.00011705532693187012, loss:2.7539896965026855
batch i:380
learning rate0.00011705532693187012, loss:2.7468602657318115
batch i:381
learning rate0.00011705532693187012, loss:2.8357839584350586
batch i:382
learning rate0.00011705532693187012, loss:2.738898515701294
batch i:383
learning rate0.00011705532693187012, loss:2.7424583435058594
batch i:384
learning rate0.00011705532693187012, loss:2.737762451171875
batch i:385
learning rate0.00011705532693187012, loss:2.67995548248291
batch i:386
learning rate0.00011705532693187012, loss:2.741715908050537
batch i:387
learning rate0.00011705532693187012, loss:2.7366809844970703
batch i:388
learning rate0.00011705532693187012, loss:2.7820005416870117
batch i:389
learning rate0.00011705532693187012, loss:2.798184871673584
batch i:390
learning rate0.00011705532693187012, loss:2.7073192596435547
batch i:391
learning rate0.00011705532693187012, loss:2.7476613521575928
batch i:392
learning rate0.00011705532693187012, loss:2.784294605255127
batch i:393
learning rate0.00011705532693187012, loss:2.8095598220825195
batch i:394
learning rate0.00011705532693187012, loss:2.704681396484375
batch i:395
learning rate0.00011705532693187012, loss:2.891131639480591
batch i:396
learning rate0.00011705532693187012, loss:2.7221999168395996
batch i:397
learning rate0.00011705532693187012, loss:2.8421497344970703
batch i:398
learning rate0.00011705532693187012, loss:2.7429490089416504
batch i:399
learning rate0.00011705532693187012, loss:2.7581627368927
batch i:400
learning rate0.00011705532693187012, loss:2.7417397499084473
batch i:401
learning rate0.00011705532693187012, loss:2.78851318359375
batch i:402
learning rate0.00011705532693187012, loss:2.806713342666626
batch i:403
learning rate0.0001755829903978052, loss:2.7702598571777344
batch i:404
learning rate0.0001755829903978052, loss:2.8069398403167725
batch i:405
learning rate0.0001755829903978052, loss:2.7386884689331055
batch i:406
learning rate0.0001755829903978052, loss:2.695744037628174
batch i:407
learning rate0.0001755829903978052, loss:2.7895193099975586
batch i:408
learning rate0.0001755829903978052, loss:2.70149564743042
batch i:409
learning rate0.0001755829903978052, loss:2.9037997722625732
batch i:410
learning rate0.0001755829903978052, loss:2.787816047668457
batch i:411
learning rate0.0001755829903978052, loss:2.7480087280273438
batch i:412
learning rate0.0001755829903978052, loss:2.9129321575164795
batch i:413
learning rate0.0001755829903978052, loss:2.677384853363037
batch i:414
learning rate0.0001755829903978052, loss:2.8634133338928223
batch i:415
learning rate0.0001755829903978052, loss:2.879451036453247
batch i:416
learning rate0.0001755829903978052, loss:2.791532039642334
batch i:417
learning rate0.0001755829903978052, loss:2.7987184524536133
batch i:418
learning rate0.0001755829903978052, loss:2.7202377319335938
batch i:419
learning rate0.00011705532693187012, loss:2.8557374477386475
batch i:420
learning rate0.00011705532693187012, loss:2.8036608695983887
batch i:421
learning rate0.00011705532693187012, loss:2.80479097366333
batch i:422
learning rate0.00011705532693187012, loss:2.797393321990967
batch i:423
learning rate0.00011705532693187012, loss:2.7689828872680664
batch i:424
learning rate0.00011705532693187012, loss:2.8200716972351074
batch i:425
learning rate0.00011705532693187012, loss:2.8233437538146973
batch i:426
learning rate0.00011705532693187012, loss:2.7597928047180176
batch i:427
learning rate0.00011705532693187012, loss:2.7834928035736084
batch i:428
learning rate0.00011705532693187012, loss:2.7261476516723633
batch i:429
learning rate0.00011705532693187012, loss:2.7925984859466553
batch i:430
learning rate0.00011705532693187012, loss:2.735461711883545
batch i:431
learning rate0.00011705532693187012, loss:2.666961193084717
batch i:432
learning rate0.00011705532693187012, loss:2.7842631340026855
batch i:433
learning rate0.00011705532693187012, loss:2.712622880935669
batch i:434
learning rate0.00011705532693187012, loss:2.789677143096924
batch i:435
learning rate0.00011705532693187012, loss:2.8304951190948486
batch i:436
learning rate0.00011705532693187012, loss:2.7561020851135254
batch i:437
learning rate0.00011705532693187012, loss:2.6972079277038574
batch i:438
learning rate0.00011705532693187012, loss:2.633239269256592
batch i:439
learning rate0.00011705532693187012, loss:2.7680916786193848
batch i:440
learning rate0.00011705532693187012, loss:2.7405409812927246
batch i:441
learning rate0.00011705532693187012, loss:2.8141798973083496
batch i:442
learning rate0.00011705532693187012, loss:2.7083821296691895
batch i:443
learning rate0.00011705532693187012, loss:2.771916389465332
batch i:444
learning rate0.00011705532693187012, loss:2.792433261871338
batch i:445
learning rate0.00011705532693187012, loss:2.803802490234375
batch i:446
learning rate0.00011705532693187012, loss:2.791489601135254
batch i:447
learning rate0.00011705532693187012, loss:2.68019962310791
batch i:448
learning rate0.00011705532693187012, loss:2.6988697052001953
batch i:449
learning rate0.00011705532693187012, loss:2.712864875793457
batch i:450
learning rate0.00011705532693187012, loss:2.8060624599456787
batch i:451
learning rate0.00011705532693187012, loss:2.7455484867095947
batch i:452
learning rate0.00011705532693187012, loss:2.6706840991973877
batch i:453
learning rate0.00011705532693187012, loss:2.6796188354492188
batch i:454
learning rate0.00011705532693187012, loss:2.8282060623168945
batch i:455
learning rate0.00011705532693187012, loss:2.8265676498413086
batch i:456
learning rate0.00011705532693187012, loss:2.7959537506103516
batch i:457
learning rate0.00011705532693187012, loss:2.8467578887939453
batch i:458
learning rate0.00011705532693187012, loss:2.793980598449707
batch i:459
learning rate0.00011705532693187012, loss:2.745659351348877
batch i:460
learning rate0.00011705532693187012, loss:2.688689708709717
batch i:461
learning rate0.00011705532693187012, loss:2.7628512382507324
batch i:462
learning rate0.00011705532693187012, loss:2.809210777282715
batch i:463
learning rate0.00011705532693187012, loss:2.7545835971832275
batch i:464
learning rate0.00011705532693187012, loss:2.845360279083252
batch i:465
learning rate0.00011705532693187012, loss:2.777745008468628
batch i:466
learning rate0.00011705532693187012, loss:2.7630503177642822
batch i:467
learning rate0.00011705532693187012, loss:2.7303552627563477
batch i:468
learning rate0.00011705532693187012, loss:2.679264545440674
batch i:469
learning rate0.00011705532693187012, loss:2.8004841804504395
batch i:470
learning rate0.00011705532693187012, loss:2.787700653076172
batch i:471
learning rate0.00011705532693187012, loss:2.7525017261505127
batch i:472
learning rate0.00011705532693187012, loss:2.758456230163574
batch i:473
learning rate0.00011705532693187012, loss:2.7874205112457275
batch i:474
learning rate0.00011705532693187012, loss:2.6865081787109375
batch i:475
learning rate0.00011705532693187012, loss:2.6761064529418945
batch i:476
learning rate0.00011705532693187012, loss:2.7624504566192627
batch i:477
learning rate0.00011705532693187012, loss:2.709639549255371
batch i:478
learning rate0.00011705532693187012, loss:2.7196104526519775
batch i:479
learning rate0.00011705532693187012, loss:2.7135016918182373
batch i:480
learning rate0.00011705532693187012, loss:2.6545486450195312
batch i:481
learning rate0.00011705532693187012, loss:2.7021517753601074
batch i:482
learning rate0.00011705532693187012, loss:2.696268320083618
batch i:483
learning rate0.00011705532693187012, loss:2.679917812347412
batch i:484
learning rate0.00011705532693187012, loss:2.7579524517059326
batch i:485
learning rate0.00011705532693187012, loss:2.714519500732422
batch i:486
learning rate0.00011705532693187012, loss:2.7651805877685547
batch i:487
learning rate0.00011705532693187012, loss:2.6632332801818848
batch i:488
learning rate0.00011705532693187012, loss:2.594574451446533
batch i:489
learning rate0.00011705532693187012, loss:2.701242446899414
batch i:490
learning rate0.00011705532693187012, loss:2.65224552154541
batch i:491
learning rate0.00011705532693187012, loss:2.763918876647949
batch i:492
learning rate0.00011705532693187012, loss:2.676361083984375
batch i:493
learning rate0.00011705532693187012, loss:2.7019057273864746
batch i:494
learning rate0.00011705532693187012, loss:2.718001365661621
batch i:495
learning rate0.00011705532693187012, loss:2.6497583389282227
batch i:496
learning rate0.00011705532693187012, loss:2.7130661010742188
batch i:497
learning rate0.00011705532693187012, loss:2.628936767578125
batch i:498
learning rate0.00011705532693187012, loss:2.6658923625946045
batch i:499
learning rate0.00011705532693187012, loss:2.6872198581695557
batch i:500
learning rate0.00011705532693187012, loss:2.6969046592712402
current self-play batch: 500
num_playouts:2000, win: 9, lose: 1, tie:0
average time: 436.6022173166275
New best policy from pure MCTS
batch i:501
learning rate0.00011705532693187012, loss:2.684720993041992
batch i:502
learning rate0.00011705532693187012, loss:2.6901822090148926
batch i:503
learning rate0.00011705532693187012, loss:2.6846203804016113
batch i:504
learning rate0.00011705532693187012, loss:2.61376953125
batch i:505
learning rate0.00011705532693187012, loss:2.7084364891052246
batch i:506
learning rate0.00011705532693187012, loss:2.6172924041748047
batch i:507
learning rate0.00011705532693187012, loss:2.7045845985412598
batch i:508
learning rate0.00011705532693187012, loss:2.6751599311828613
batch i:509
learning rate0.00011705532693187012, loss:2.6538522243499756
batch i:510
learning rate0.00011705532693187012, loss:2.544053077697754
batch i:511
learning rate0.00011705532693187012, loss:2.594525098800659
batch i:512
learning rate0.00011705532693187012, loss:2.5819008350372314
batch i:513
learning rate0.00011705532693187012, loss:2.54229736328125
batch i:514
learning rate0.00011705532693187012, loss:2.7013072967529297
batch i:515
learning rate0.00011705532693187012, loss:2.6455817222595215
batch i:516
learning rate0.00011705532693187012, loss:2.5744786262512207
batch i:517
learning rate0.00011705532693187012, loss:2.64046049118042
batch i:518
learning rate0.00011705532693187012, loss:2.572430372238159
batch i:519
learning rate0.00011705532693187012, loss:2.5869369506835938
batch i:520
learning rate0.00011705532693187012, loss:2.755244255065918
batch i:521
learning rate0.00011705532693187012, loss:2.6118521690368652
batch i:522
learning rate0.00011705532693187012, loss:2.6453213691711426
batch i:523
learning rate0.00011705532693187012, loss:2.465416431427002
batch i:524
learning rate0.00011705532693187012, loss:2.6020522117614746
batch i:525
learning rate0.00011705532693187012, loss:2.6785643100738525
batch i:526
learning rate0.00011705532693187012, loss:2.599116563796997
batch i:527
learning rate0.00011705532693187012, loss:2.6081645488739014
batch i:528
learning rate0.00011705532693187012, loss:2.681128740310669
batch i:529
learning rate0.00011705532693187012, loss:2.544738531112671
batch i:530
learning rate0.00011705532693187012, loss:2.591628313064575
batch i:531
learning rate0.00011705532693187012, loss:2.597660541534424
batch i:532
learning rate0.00011705532693187012, loss:2.6709814071655273
batch i:533
learning rate0.00011705532693187012, loss:2.6759650707244873
batch i:534
learning rate0.00011705532693187012, loss:2.790461778640747
batch i:535
learning rate0.00011705532693187012, loss:2.624610424041748
batch i:536
learning rate0.00011705532693187012, loss:2.6106085777282715
batch i:537
learning rate0.00011705532693187012, loss:2.6243250370025635
batch i:538
learning rate0.00011705532693187012, loss:2.649947166442871
batch i:539
learning rate0.00011705532693187012, loss:2.5861034393310547
batch i:540
learning rate0.00011705532693187012, loss:2.52976655960083
batch i:541
learning rate0.00011705532693187012, loss:2.637742042541504
batch i:542
learning rate0.00011705532693187012, loss:2.706122875213623
batch i:543
learning rate0.00011705532693187012, loss:2.5379281044006348
batch i:544
learning rate0.00011705532693187012, loss:2.668517589569092
batch i:545
learning rate0.00011705532693187012, loss:2.5623669624328613
batch i:546
learning rate0.00011705532693187012, loss:2.5689520835876465
batch i:547
learning rate0.00011705532693187012, loss:2.6380510330200195
batch i:548
learning rate0.00011705532693187012, loss:2.5887415409088135
batch i:549
learning rate0.00011705532693187012, loss:2.6251320838928223
batch i:550
learning rate0.00011705532693187012, loss:2.6229655742645264
batch i:551
learning rate0.00011705532693187012, loss:2.636326789855957
batch i:552
learning rate0.00011705532693187012, loss:2.5832324028015137
batch i:553
learning rate0.00011705532693187012, loss:2.6361567974090576
batch i:554
learning rate0.00011705532693187012, loss:2.6555018424987793
batch i:555
learning rate0.00011705532693187012, loss:2.568272590637207
batch i:556
learning rate0.00011705532693187012, loss:2.5630998611450195
batch i:557
learning rate0.00011705532693187012, loss:2.5833024978637695
batch i:558
learning rate0.00011705532693187012, loss:2.633939743041992
batch i:559
learning rate0.00011705532693187012, loss:2.561927556991577
batch i:560
learning rate0.00011705532693187012, loss:2.4875268936157227
batch i:561
learning rate0.00011705532693187012, loss:2.6580562591552734
batch i:562
learning rate0.00011705532693187012, loss:2.6273231506347656
batch i:563
learning rate0.00011705532693187012, loss:2.6869492530822754
batch i:564
learning rate0.00011705532693187012, loss:2.6821346282958984
batch i:565
learning rate0.00011705532693187012, loss:2.6231164932250977
batch i:566
learning rate0.00011705532693187012, loss:2.5934648513793945
batch i:567
learning rate0.00011705532693187012, loss:2.536248207092285
batch i:568
learning rate0.00011705532693187012, loss:2.5681750774383545
batch i:569
learning rate0.00011705532693187012, loss:2.5847880840301514
batch i:570
learning rate0.00011705532693187012, loss:2.6028523445129395
batch i:571
learning rate0.00011705532693187012, loss:2.5283215045928955
batch i:572
learning rate0.00011705532693187012, loss:2.516841173171997
batch i:573
learning rate0.00011705532693187012, loss:2.553652048110962
batch i:574
learning rate0.00011705532693187012, loss:2.514463186264038
batch i:575
learning rate0.00011705532693187012, loss:2.5638279914855957
batch i:576
learning rate0.00011705532693187012, loss:2.5681304931640625
batch i:577
learning rate0.00011705532693187012, loss:2.542738437652588
batch i:578
learning rate0.00011705532693187012, loss:2.514305353164673
batch i:579
learning rate0.00011705532693187012, loss:2.541658401489258
batch i:580
learning rate0.00011705532693187012, loss:2.5028982162475586
batch i:581
learning rate0.00011705532693187012, loss:2.507300853729248
batch i:582
learning rate0.00011705532693187012, loss:2.651287794113159
batch i:583
learning rate0.00011705532693187012, loss:2.5592079162597656
batch i:584
learning rate0.00011705532693187012, loss:2.5382304191589355
batch i:585
learning rate0.00011705532693187012, loss:2.5935280323028564
batch i:586
learning rate0.00011705532693187012, loss:2.527937889099121
batch i:587
learning rate0.00011705532693187012, loss:2.5800929069519043
batch i:588
learning rate0.00011705532693187012, loss:2.5836188793182373
batch i:589
learning rate0.00011705532693187012, loss:2.5049872398376465
batch i:590
learning rate0.00011705532693187012, loss:2.5976192951202393
batch i:591
learning rate0.00011705532693187012, loss:2.599348545074463
batch i:592
learning rate0.00011705532693187012, loss:2.4990439414978027
batch i:593
learning rate0.00011705532693187012, loss:2.536266803741455
batch i:594
learning rate0.00011705532693187012, loss:2.566600799560547
batch i:595
learning rate0.00011705532693187012, loss:2.3986105918884277
batch i:596
learning rate0.00011705532693187012, loss:2.5049896240234375
batch i:597
learning rate0.00011705532693187012, loss:2.484908103942871
batch i:598
learning rate0.00011705532693187012, loss:2.497213840484619
batch i:599
learning rate0.00011705532693187012, loss:2.4822449684143066
batch i:600
learning rate0.00011705532693187012, loss:2.5778918266296387
batch i:601
learning rate0.00011705532693187012, loss:2.6420345306396484
batch i:602
learning rate0.00011705532693187012, loss:2.528705358505249
batch i:603
learning rate0.00011705532693187012, loss:2.5394246578216553
batch i:604
learning rate0.00011705532693187012, loss:2.441361427307129
batch i:605
learning rate0.00011705532693187012, loss:2.457226514816284
batch i:606
learning rate0.00011705532693187012, loss:2.5747618675231934
batch i:607
learning rate0.00011705532693187012, loss:2.579092502593994
batch i:608
learning rate0.00011705532693187012, loss:2.570389747619629
batch i:609
learning rate0.00011705532693187012, loss:2.6092658042907715
batch i:610
learning rate0.00011705532693187012, loss:2.547645330429077
batch i:611
learning rate0.00011705532693187012, loss:2.527515172958374
batch i:612
learning rate0.00011705532693187012, loss:2.5685179233551025
batch i:613
learning rate0.00011705532693187012, loss:2.5771493911743164
batch i:614
learning rate0.00011705532693187012, loss:2.6444644927978516
batch i:615
learning rate0.00011705532693187012, loss:2.669750690460205
batch i:616
learning rate0.00011705532693187012, loss:2.56314754486084
batch i:617
learning rate0.00011705532693187012, loss:2.5722508430480957
batch i:618
learning rate0.00011705532693187012, loss:2.4977426528930664
batch i:619
learning rate0.00011705532693187012, loss:2.532543420791626
batch i:620
learning rate0.00011705532693187012, loss:2.6150341033935547
batch i:621
learning rate0.00011705532693187012, loss:2.572310209274292
batch i:622
learning rate0.00011705532693187012, loss:2.480006456375122
batch i:623
learning rate0.00011705532693187012, loss:2.440323829650879
batch i:624
learning rate0.00011705532693187012, loss:2.508026599884033
batch i:625
learning rate0.00011705532693187012, loss:2.6067004203796387
batch i:626
learning rate0.00011705532693187012, loss:2.4945151805877686
batch i:627
learning rate0.00011705532693187012, loss:2.478332996368408
batch i:628
learning rate0.00011705532693187012, loss:2.5769684314727783
batch i:629
learning rate0.00011705532693187012, loss:2.570697784423828
batch i:630
learning rate0.00011705532693187012, loss:2.5661230087280273
batch i:631
learning rate0.00011705532693187012, loss:2.520235538482666
batch i:632
learning rate0.00011705532693187012, loss:2.474494457244873
batch i:633
learning rate0.00011705532693187012, loss:2.653172492980957
batch i:634
learning rate0.00011705532693187012, loss:2.4286704063415527
batch i:635
learning rate0.00011705532693187012, loss:2.5605363845825195
batch i:636
learning rate0.00011705532693187012, loss:2.6124415397644043
batch i:637
learning rate0.00011705532693187012, loss:2.6395888328552246
batch i:638
learning rate0.00011705532693187012, loss:2.6261048316955566
batch i:639
learning rate0.00011705532693187012, loss:2.556213855743408
batch i:640
learning rate0.00011705532693187012, loss:2.4944357872009277
batch i:641
learning rate0.00011705532693187012, loss:2.5372376441955566
batch i:642
learning rate0.00011705532693187012, loss:2.5271711349487305
batch i:643
learning rate0.00011705532693187012, loss:2.561007499694824
batch i:644
learning rate0.00011705532693187012, loss:2.5942602157592773
batch i:645
learning rate0.00011705532693187012, loss:2.492117404937744
batch i:646
learning rate0.00011705532693187012, loss:2.6198761463165283
batch i:647
learning rate0.00011705532693187012, loss:2.6453499794006348
batch i:648
learning rate0.00011705532693187012, loss:2.491877555847168
batch i:649
learning rate0.00011705532693187012, loss:2.537994146347046
batch i:650
learning rate0.00011705532693187012, loss:2.461169719696045
batch i:651
learning rate0.00011705532693187012, loss:2.549546718597412
batch i:652
learning rate0.00011705532693187012, loss:2.611143112182617
batch i:653
learning rate0.00011705532693187012, loss:2.546659469604492
batch i:654
learning rate0.00011705532693187012, loss:2.603911876678467
batch i:655
learning rate0.00011705532693187012, loss:2.5352463722229004
batch i:656
learning rate0.00011705532693187012, loss:2.650803327560425
batch i:657
learning rate0.00011705532693187012, loss:2.564303159713745
batch i:658
learning rate0.00011705532693187012, loss:2.550525665283203
batch i:659
learning rate0.00011705532693187012, loss:2.5409719944000244
batch i:660
learning rate0.00011705532693187012, loss:2.617419481277466
batch i:661
learning rate0.00011705532693187012, loss:2.5953266620635986
batch i:662
learning rate0.00011705532693187012, loss:2.716808795928955
batch i:663
learning rate0.00011705532693187012, loss:2.605618953704834
batch i:664
learning rate0.00011705532693187012, loss:2.5719194412231445
batch i:665
learning rate0.00011705532693187012, loss:2.6691842079162598
batch i:666
learning rate0.00011705532693187012, loss:2.643836259841919
batch i:667
learning rate0.00011705532693187012, loss:2.5182082653045654
batch i:668
learning rate0.00011705532693187012, loss:2.615361452102661
batch i:669
learning rate0.00011705532693187012, loss:2.545236825942993
batch i:670
learning rate0.00011705532693187012, loss:2.4988884925842285
batch i:671
learning rate0.00011705532693187012, loss:2.6067755222320557
batch i:672
learning rate0.00011705532693187012, loss:2.484794855117798
batch i:673
learning rate0.00011705532693187012, loss:2.435838222503662
batch i:674
learning rate0.00011705532693187012, loss:2.4259233474731445
batch i:675
learning rate0.00011705532693187012, loss:2.474283218383789
batch i:676
learning rate0.00011705532693187012, loss:2.522202491760254
batch i:677
learning rate0.00011705532693187012, loss:2.5026462078094482
batch i:678
learning rate0.00011705532693187012, loss:2.517246961593628
batch i:679
learning rate0.00011705532693187012, loss:2.5413002967834473
batch i:680
learning rate0.00011705532693187012, loss:2.560001850128174
batch i:681
learning rate0.00011705532693187012, loss:2.4750592708587646
batch i:682
learning rate0.00011705532693187012, loss:2.600918769836426
batch i:683
learning rate0.00011705532693187012, loss:2.5616750717163086
batch i:684
learning rate0.00011705532693187012, loss:2.5204882621765137
batch i:685
learning rate0.00011705532693187012, loss:2.506969928741455
batch i:686
learning rate0.00011705532693187012, loss:2.529324531555176
batch i:687
learning rate0.00011705532693187012, loss:2.5979392528533936
batch i:688
learning rate0.00011705532693187012, loss:2.4957423210144043
batch i:689
learning rate0.00011705532693187012, loss:2.5053703784942627
batch i:690
learning rate0.00011705532693187012, loss:2.528996229171753
batch i:691
learning rate0.00011705532693187012, loss:2.387570381164551
batch i:692
learning rate0.00011705532693187012, loss:2.485515832901001
batch i:693
learning rate0.00011705532693187012, loss:2.331930637359619
batch i:694
learning rate0.00011705532693187012, loss:2.4003188610076904
batch i:695
learning rate0.00011705532693187012, loss:2.478348731994629
batch i:696
learning rate0.00011705532693187012, loss:2.438345432281494
batch i:697
learning rate0.00011705532693187012, loss:2.5840518474578857
batch i:698
learning rate0.00011705532693187012, loss:2.5254807472229004
batch i:699
learning rate0.00011705532693187012, loss:2.558596134185791
batch i:700
learning rate0.00011705532693187012, loss:2.4925119876861572
batch i:701
learning rate0.00011705532693187012, loss:2.5355224609375
batch i:702
learning rate0.00011705532693187012, loss:2.4714183807373047
batch i:703
learning rate0.00011705532693187012, loss:2.485527515411377
batch i:704
learning rate0.00011705532693187012, loss:2.5511555671691895
batch i:705
learning rate0.00011705532693187012, loss:2.5211238861083984
batch i:706
learning rate0.00011705532693187012, loss:2.494168996810913
batch i:707
learning rate0.00011705532693187012, loss:2.3602919578552246
batch i:708
learning rate0.00011705532693187012, loss:2.4681787490844727
batch i:709
learning rate0.00011705532693187012, loss:2.4847021102905273
batch i:710
learning rate0.00011705532693187012, loss:2.392115831375122
batch i:711
learning rate0.00011705532693187012, loss:2.3634824752807617
batch i:712
learning rate0.00011705532693187012, loss:2.447805643081665
batch i:713
learning rate0.00011705532693187012, loss:2.428194999694824
batch i:714
learning rate0.00011705532693187012, loss:2.4231724739074707
batch i:715
learning rate0.00011705532693187012, loss:2.408278703689575
batch i:716
learning rate0.00011705532693187012, loss:2.2938008308410645
batch i:717
learning rate0.00011705532693187012, loss:2.354196548461914
batch i:718
learning rate0.00011705532693187012, loss:2.370206356048584
batch i:719
learning rate0.00011705532693187012, loss:2.4704549312591553
batch i:720
learning rate0.00011705532693187012, loss:2.517075538635254
batch i:721
learning rate0.00011705532693187012, loss:2.482574462890625
batch i:722
learning rate0.00011705532693187012, loss:2.4755868911743164
batch i:723
learning rate0.00011705532693187012, loss:2.4050307273864746
batch i:724
learning rate0.00011705532693187012, loss:2.412843704223633
batch i:725
learning rate0.00011705532693187012, loss:2.4461445808410645
batch i:726
learning rate0.00011705532693187012, loss:2.284343719482422
batch i:727
learning rate0.00011705532693187012, loss:2.360330820083618
batch i:728
learning rate0.00011705532693187012, loss:2.462860345840454
batch i:729
learning rate0.00011705532693187012, loss:2.5602424144744873
batch i:730
learning rate0.00011705532693187012, loss:2.429037094116211
batch i:731
learning rate0.00011705532693187012, loss:2.4463796615600586
batch i:732
learning rate0.00011705532693187012, loss:2.469658374786377
batch i:733
learning rate0.00011705532693187012, loss:2.4597885608673096
batch i:734
learning rate0.00011705532693187012, loss:2.3978941440582275
batch i:735
learning rate0.00011705532693187012, loss:2.4852705001831055
batch i:736
learning rate0.00011705532693187012, loss:2.4365315437316895
batch i:737
learning rate0.00011705532693187012, loss:2.3831627368927
batch i:738
learning rate0.00011705532693187012, loss:2.3785226345062256
batch i:739
learning rate0.00011705532693187012, loss:2.425614833831787
batch i:740
learning rate0.00011705532693187012, loss:2.5274267196655273
batch i:741
learning rate0.00011705532693187012, loss:2.429567813873291
batch i:742
learning rate0.00011705532693187012, loss:2.322540283203125
batch i:743
learning rate0.00011705532693187012, loss:2.3604674339294434
batch i:744
learning rate0.00011705532693187012, loss:2.39251708984375
batch i:745
learning rate0.00011705532693187012, loss:2.353055715560913
batch i:746
learning rate0.00011705532693187012, loss:2.4024760723114014
batch i:747
learning rate0.00011705532693187012, loss:2.3706014156341553
batch i:748
learning rate0.00011705532693187012, loss:2.430652379989624
batch i:749
learning rate0.00011705532693187012, loss:2.471050500869751
batch i:750
learning rate0.00011705532693187012, loss:2.3419547080993652
current self-play batch: 750
num_playouts:2000, win: 10, lose: 0, tie:0
average time: 298.4683158636093
New best policy from pure MCTS
batch i:751
learning rate0.00011705532693187012, loss:2.34916353225708
batch i:752
learning rate0.00011705532693187012, loss:2.3734402656555176
batch i:753
learning rate0.00011705532693187012, loss:2.397576093673706
batch i:754
learning rate0.00011705532693187012, loss:2.33182954788208
batch i:755
learning rate0.00011705532693187012, loss:2.3532750606536865
batch i:756
learning rate0.00011705532693187012, loss:2.3647212982177734
batch i:757
learning rate0.00011705532693187012, loss:2.3551220893859863
batch i:758
learning rate0.00011705532693187012, loss:2.4542534351348877
batch i:759
learning rate0.00011705532693187012, loss:2.339611053466797
batch i:760
learning rate0.00011705532693187012, loss:2.370673179626465
batch i:761
learning rate0.00011705532693187012, loss:2.27725887298584
batch i:762
learning rate0.00011705532693187012, loss:2.3815765380859375
batch i:763
learning rate0.00011705532693187012, loss:2.3163304328918457
batch i:764
learning rate0.00011705532693187012, loss:2.401768684387207
batch i:765
learning rate0.00011705532693187012, loss:2.3663244247436523
batch i:766
learning rate0.00011705532693187012, loss:2.2907066345214844
batch i:767
learning rate0.00011705532693187012, loss:2.3340978622436523
batch i:768
learning rate0.00011705532693187012, loss:2.279633045196533
batch i:769
learning rate0.00011705532693187012, loss:2.2860047817230225
batch i:770
learning rate0.00011705532693187012, loss:2.2672946453094482
batch i:771
learning rate0.00011705532693187012, loss:2.192221164703369
batch i:772
learning rate0.00011705532693187012, loss:2.3459649085998535
batch i:773
learning rate0.00011705532693187012, loss:2.386187791824341
batch i:774
learning rate0.00011705532693187012, loss:2.3301897048950195
batch i:775
learning rate0.0001755829903978052, loss:2.2093000411987305
batch i:776
learning rate0.0001755829903978052, loss:2.339628219604492
batch i:777
learning rate0.0001755829903978052, loss:2.266127824783325
batch i:778
learning rate0.0001755829903978052, loss:2.219202756881714
batch i:779
learning rate0.0001755829903978052, loss:2.3521790504455566
batch i:780
learning rate0.0001755829903978052, loss:2.4400746822357178
batch i:781
learning rate0.0001755829903978052, loss:2.3117313385009766
batch i:782
learning rate0.0001755829903978052, loss:2.427802324295044
batch i:783
learning rate0.0001755829903978052, loss:2.4619669914245605
batch i:784
learning rate0.0001755829903978052, loss:2.4181532859802246
batch i:785
learning rate0.0001755829903978052, loss:2.4077048301696777
batch i:786
learning rate0.0001755829903978052, loss:2.31352162361145
batch i:787
learning rate0.0001755829903978052, loss:2.397594928741455
batch i:788
learning rate0.0001755829903978052, loss:2.4149861335754395
batch i:789
learning rate0.0001755829903978052, loss:2.347409248352051
batch i:790
learning rate0.0001755829903978052, loss:2.3321986198425293
batch i:791
learning rate0.00011705532693187012, loss:2.429086208343506
batch i:792
learning rate0.00011705532693187012, loss:2.4701614379882812
batch i:793
learning rate0.00011705532693187012, loss:2.3463945388793945
batch i:794
learning rate0.00011705532693187012, loss:2.4598171710968018
batch i:795
learning rate0.00011705532693187012, loss:2.3942790031433105
batch i:796
learning rate0.00011705532693187012, loss:2.3712644577026367
batch i:797
learning rate0.00011705532693187012, loss:2.3855485916137695
batch i:798
learning rate0.00011705532693187012, loss:2.392396926879883
batch i:799
learning rate0.00011705532693187012, loss:2.3834927082061768
batch i:800
learning rate0.00011705532693187012, loss:2.3757729530334473
batch i:801
learning rate0.00011705532693187012, loss:2.3531575202941895
batch i:802
learning rate0.00011705532693187012, loss:2.266054630279541
batch i:803
learning rate0.00011705532693187012, loss:2.4044113159179688
batch i:804
learning rate0.00011705532693187012, loss:2.3096532821655273
batch i:805
learning rate0.00011705532693187012, loss:2.2479896545410156
batch i:806
learning rate0.00011705532693187012, loss:2.3143627643585205
batch i:807
learning rate0.00011705532693187012, loss:2.3116097450256348
batch i:808
learning rate0.00011705532693187012, loss:2.4335646629333496
batch i:809
learning rate0.00011705532693187012, loss:2.323211669921875
batch i:810
learning rate0.00011705532693187012, loss:2.3032114505767822
batch i:811
learning rate0.00011705532693187012, loss:2.3954665660858154
batch i:812
learning rate0.00011705532693187012, loss:2.391559600830078
batch i:813
learning rate0.00011705532693187012, loss:2.305443286895752
batch i:814
learning rate0.00011705532693187012, loss:2.3481640815734863
batch i:815
learning rate0.00011705532693187012, loss:2.318305015563965
batch i:816
learning rate0.00011705532693187012, loss:2.2979776859283447
batch i:817
learning rate0.00011705532693187012, loss:2.376145839691162
batch i:818
learning rate0.00011705532693187012, loss:2.414381742477417
batch i:819
learning rate0.00011705532693187012, loss:2.4275240898132324
batch i:820
learning rate0.00011705532693187012, loss:2.388038158416748
batch i:821
learning rate0.00011705532693187012, loss:2.4682278633117676
batch i:822
learning rate0.00011705532693187012, loss:2.3282220363616943
batch i:823
learning rate0.00011705532693187012, loss:2.392650604248047
batch i:824
learning rate0.00011705532693187012, loss:2.339919090270996
batch i:825
learning rate0.00011705532693187012, loss:2.4189658164978027
batch i:826
learning rate0.00011705532693187012, loss:2.346644878387451
batch i:827
learning rate0.00011705532693187012, loss:2.397007465362549
batch i:828
learning rate0.00011705532693187012, loss:2.437612771987915
batch i:829
learning rate0.00011705532693187012, loss:2.4576404094696045
batch i:830
learning rate0.00011705532693187012, loss:2.419203758239746
batch i:831
learning rate0.00011705532693187012, loss:2.4065866470336914
batch i:832
learning rate0.00011705532693187012, loss:2.3681490421295166
batch i:833
learning rate0.00011705532693187012, loss:2.3294565677642822
batch i:834
learning rate0.00011705532693187012, loss:2.3950204849243164
batch i:835
learning rate0.00011705532693187012, loss:2.46897029876709
batch i:836
learning rate0.00011705532693187012, loss:2.32167387008667
batch i:837
learning rate0.00011705532693187012, loss:2.448227882385254
batch i:838
learning rate0.00011705532693187012, loss:2.419304847717285
batch i:839
learning rate0.00011705532693187012, loss:2.4200825691223145
batch i:840
learning rate0.00011705532693187012, loss:2.331699848175049
batch i:841
learning rate0.00011705532693187012, loss:2.351583480834961
batch i:842
learning rate0.00011705532693187012, loss:2.539642810821533
batch i:843
learning rate0.00011705532693187012, loss:2.3783605098724365
batch i:844
learning rate0.00011705532693187012, loss:2.389328718185425
batch i:845
learning rate0.00011705532693187012, loss:2.3975257873535156
batch i:846
learning rate0.00011705532693187012, loss:2.5166521072387695
batch i:847
learning rate0.00011705532693187012, loss:2.360959053039551
batch i:848
learning rate0.00011705532693187012, loss:2.5775563716888428
batch i:849
learning rate0.00011705532693187012, loss:2.4601049423217773
batch i:850
learning rate0.00011705532693187012, loss:2.5207905769348145
batch i:851
learning rate0.00011705532693187012, loss:2.507774829864502
batch i:852
learning rate0.00011705532693187012, loss:2.574673891067505
batch i:853
learning rate0.00011705532693187012, loss:2.593421220779419
batch i:854
learning rate0.00011705532693187012, loss:2.450225353240967
batch i:855
learning rate0.00011705532693187012, loss:2.5855069160461426
batch i:856
learning rate0.00011705532693187012, loss:2.4971320629119873
batch i:857
learning rate0.00011705532693187012, loss:2.4489517211914062
batch i:858
learning rate0.00011705532693187012, loss:2.498082399368286
batch i:859
learning rate0.00011705532693187012, loss:2.4226393699645996
batch i:860
learning rate0.00011705532693187012, loss:2.4425461292266846
batch i:861
learning rate0.00011705532693187012, loss:2.4120430946350098
batch i:862
learning rate0.00011705532693187012, loss:2.509335994720459
batch i:863
learning rate0.00011705532693187012, loss:2.5475425720214844
batch i:864
learning rate0.00011705532693187012, loss:2.5110559463500977
batch i:865
learning rate0.00011705532693187012, loss:2.500154972076416
batch i:866
learning rate0.00011705532693187012, loss:2.472029685974121
batch i:867
learning rate0.00011705532693187012, loss:2.376325845718384
batch i:868
learning rate0.00011705532693187012, loss:2.5414929389953613
batch i:869
learning rate0.00011705532693187012, loss:2.566469669342041
batch i:870
learning rate0.00011705532693187012, loss:2.5226550102233887
batch i:871
learning rate0.00011705532693187012, loss:2.5679357051849365
batch i:872
learning rate0.00011705532693187012, loss:2.582367420196533
batch i:873
learning rate0.00011705532693187012, loss:2.481388807296753
batch i:874
learning rate0.00011705532693187012, loss:2.54284930229187
batch i:875
learning rate0.00011705532693187012, loss:2.5336670875549316
batch i:876
learning rate0.00011705532693187012, loss:2.543914794921875
batch i:877
learning rate0.00011705532693187012, loss:2.551438808441162
batch i:878
learning rate0.00011705532693187012, loss:2.531508445739746
batch i:879
learning rate0.00011705532693187012, loss:2.557772397994995
batch i:880
learning rate0.00011705532693187012, loss:2.466602325439453
batch i:881
learning rate0.00011705532693187012, loss:2.5078139305114746
batch i:882
learning rate0.00011705532693187012, loss:2.5405638217926025
batch i:883
learning rate0.00011705532693187012, loss:2.5269432067871094
batch i:884
learning rate0.00011705532693187012, loss:2.5007941722869873
batch i:885
learning rate0.00011705532693187012, loss:2.5249171257019043
batch i:886
learning rate0.00011705532693187012, loss:2.5879862308502197
batch i:887
learning rate0.00011705532693187012, loss:2.5999698638916016
batch i:888
learning rate0.00011705532693187012, loss:2.4648265838623047
batch i:889
learning rate0.00011705532693187012, loss:2.499753475189209
batch i:890
learning rate0.00011705532693187012, loss:2.5200352668762207
batch i:891
learning rate0.00011705532693187012, loss:2.4739294052124023
batch i:892
learning rate0.00011705532693187012, loss:2.5987541675567627
batch i:893
learning rate0.00011705532693187012, loss:2.4938106536865234
batch i:894
learning rate0.00011705532693187012, loss:2.529918670654297
batch i:895
learning rate0.00011705532693187012, loss:2.515916585922241
batch i:896
learning rate0.00011705532693187012, loss:2.5148043632507324
batch i:897
learning rate0.00011705532693187012, loss:2.4908969402313232
batch i:898
learning rate0.00011705532693187012, loss:2.585216522216797
batch i:899
learning rate0.00011705532693187012, loss:2.49753999710083
batch i:900
learning rate0.00011705532693187012, loss:2.567478656768799
batch i:901
learning rate0.00011705532693187012, loss:2.5698251724243164
batch i:902
learning rate0.00011705532693187012, loss:2.4678282737731934
batch i:903
learning rate0.00011705532693187012, loss:2.50323748588562
batch i:904
learning rate0.00011705532693187012, loss:2.488633155822754
batch i:905
learning rate0.00011705532693187012, loss:2.358516216278076
batch i:906
learning rate0.00011705532693187012, loss:2.5484893321990967
batch i:907
learning rate0.00011705532693187012, loss:2.547891139984131
batch i:908
learning rate0.00011705532693187012, loss:2.537226676940918
batch i:909
learning rate0.00011705532693187012, loss:2.546705484390259
batch i:910
learning rate0.00011705532693187012, loss:2.5350594520568848
batch i:911
learning rate0.00011705532693187012, loss:2.4779436588287354
batch i:912
learning rate0.00011705532693187012, loss:2.5155282020568848
batch i:913
learning rate0.00011705532693187012, loss:2.6347343921661377
batch i:914
learning rate0.00011705532693187012, loss:2.674192428588867
batch i:915
learning rate0.00011705532693187012, loss:2.559229850769043
batch i:916
learning rate0.00011705532693187012, loss:2.5330686569213867
batch i:917
learning rate0.00011705532693187012, loss:2.4735970497131348
batch i:918
learning rate0.00011705532693187012, loss:2.584880828857422
batch i:919
learning rate0.00011705532693187012, loss:2.5037450790405273
batch i:920
learning rate0.00011705532693187012, loss:2.515580177307129
batch i:921
learning rate0.00011705532693187012, loss:2.5663962364196777
batch i:922
learning rate0.00011705532693187012, loss:2.531210422515869
batch i:923
learning rate0.00011705532693187012, loss:2.5445261001586914
batch i:924
learning rate0.00011705532693187012, loss:2.4174036979675293
batch i:925
learning rate0.00011705532693187012, loss:2.564537525177002
batch i:926
learning rate0.00011705532693187012, loss:2.5571134090423584
batch i:927
learning rate0.00011705532693187012, loss:2.675881862640381
batch i:928
learning rate0.00011705532693187012, loss:2.645186185836792
batch i:929
learning rate0.00011705532693187012, loss:2.6935200691223145
batch i:930
learning rate0.00011705532693187012, loss:2.5724172592163086
batch i:931
learning rate0.00011705532693187012, loss:2.614530563354492
batch i:932
learning rate0.00011705532693187012, loss:2.5587525367736816
batch i:933
learning rate0.00011705532693187012, loss:2.6035196781158447
batch i:934
learning rate0.00011705532693187012, loss:2.5837621688842773
batch i:935
learning rate0.00011705532693187012, loss:2.547837734222412
batch i:936
learning rate0.00011705532693187012, loss:2.49716854095459
batch i:937
learning rate0.00011705532693187012, loss:2.5286521911621094
batch i:938
learning rate0.00011705532693187012, loss:2.482710838317871
batch i:939
learning rate0.00011705532693187012, loss:2.611025094985962
batch i:940
learning rate0.00011705532693187012, loss:2.556859016418457
batch i:941
learning rate0.00011705532693187012, loss:2.5673296451568604
batch i:942
learning rate0.00011705532693187012, loss:2.440558910369873
batch i:943
learning rate0.00011705532693187012, loss:2.662973403930664
batch i:944
learning rate0.00011705532693187012, loss:2.5823850631713867
batch i:945
learning rate0.00011705532693187012, loss:2.572812557220459
batch i:946
learning rate0.00011705532693187012, loss:2.632906913757324
batch i:947
learning rate0.00011705532693187012, loss:2.6379358768463135
batch i:948
learning rate0.00011705532693187012, loss:2.6081061363220215
batch i:949
learning rate0.00011705532693187012, loss:2.6389048099517822
batch i:950
learning rate0.00011705532693187012, loss:2.670572280883789
batch i:951
learning rate0.00011705532693187012, loss:2.6562695503234863
batch i:952
learning rate0.00011705532693187012, loss:2.56528377532959
batch i:953
learning rate0.00011705532693187012, loss:2.5938353538513184
batch i:954
learning rate0.00011705532693187012, loss:2.616462230682373
batch i:955
learning rate0.00011705532693187012, loss:2.6177425384521484
batch i:956
learning rate0.00011705532693187012, loss:2.6847589015960693
batch i:957
learning rate0.00011705532693187012, loss:2.606128692626953
batch i:958
learning rate0.00011705532693187012, loss:2.6280150413513184
batch i:959
learning rate0.00011705532693187012, loss:2.655764579772949
batch i:960
learning rate0.00011705532693187012, loss:2.721280097961426
batch i:961
learning rate0.00011705532693187012, loss:2.6886465549468994
batch i:962
learning rate0.00011705532693187012, loss:2.603297233581543
batch i:963
learning rate0.00011705532693187012, loss:2.572371482849121
batch i:964
learning rate0.00011705532693187012, loss:2.592562198638916
batch i:965
learning rate0.00011705532693187012, loss:2.5496315956115723
batch i:966
learning rate0.00011705532693187012, loss:2.513392210006714
batch i:967
learning rate0.00011705532693187012, loss:2.591662883758545
batch i:968
learning rate0.00011705532693187012, loss:2.5892605781555176
batch i:969
learning rate0.00011705532693187012, loss:2.728634834289551
batch i:970
learning rate0.00011705532693187012, loss:2.5385217666625977
batch i:971
learning rate0.00011705532693187012, loss:2.587602138519287
batch i:972
learning rate0.00011705532693187012, loss:2.573425769805908
batch i:973
learning rate0.00011705532693187012, loss:2.5186104774475098
batch i:974
learning rate0.00011705532693187012, loss:2.481083393096924
batch i:975
learning rate0.00011705532693187012, loss:2.649261474609375
batch i:976
learning rate0.00011705532693187012, loss:2.505228042602539
batch i:977
learning rate0.00011705532693187012, loss:2.655334949493408
batch i:978
learning rate0.00011705532693187012, loss:2.569422483444214
batch i:979
learning rate0.00011705532693187012, loss:2.5677905082702637
batch i:980
learning rate0.00011705532693187012, loss:2.492952823638916
batch i:981
learning rate0.00011705532693187012, loss:2.6529080867767334
batch i:982
learning rate0.00011705532693187012, loss:2.5532402992248535
batch i:983
learning rate0.00011705532693187012, loss:2.5592944622039795
batch i:984
learning rate0.00011705532693187012, loss:2.6827340126037598
batch i:985
learning rate0.00011705532693187012, loss:2.4948787689208984
batch i:986
learning rate0.00011705532693187012, loss:2.5652613639831543
batch i:987
learning rate0.00011705532693187012, loss:2.4792428016662598
batch i:988
learning rate0.00011705532693187012, loss:2.5249547958374023
batch i:989
learning rate0.00011705532693187012, loss:2.4617855548858643
batch i:990
learning rate0.00011705532693187012, loss:2.4992740154266357
batch i:991
learning rate0.00011705532693187012, loss:2.543280839920044
batch i:992
learning rate0.00011705532693187012, loss:2.4692795276641846
batch i:993
learning rate0.00011705532693187012, loss:2.5442187786102295
batch i:994
learning rate0.00011705532693187012, loss:2.5597219467163086
batch i:995
learning rate0.00011705532693187012, loss:2.4716196060180664
batch i:996
learning rate0.00011705532693187012, loss:2.5706071853637695
batch i:997
learning rate0.00011705532693187012, loss:2.635690212249756
batch i:998
learning rate0.00011705532693187012, loss:2.6184492111206055
batch i:999
learning rate0.00011705532693187012, loss:2.477882146835327
batch i:1000
learning rate0.00011705532693187012, loss:2.573659896850586
current self-play batch: 1000
num_playouts:3000, win: 8, lose: 2, tie:0
average time: 710.056569480896
New best policy from pure MCTS
batch i:1001
learning rate0.00011705532693187012, loss:2.683709144592285
batch i:1002
learning rate0.00011705532693187012, loss:2.5308451652526855
batch i:1003
learning rate0.00011705532693187012, loss:2.6340157985687256
batch i:1004
learning rate0.00011705532693187012, loss:2.6487746238708496
batch i:1005
learning rate0.00011705532693187012, loss:2.56697678565979
batch i:1006
learning rate0.00011705532693187012, loss:2.645967483520508
batch i:1007
learning rate0.00011705532693187012, loss:2.584822654724121
batch i:1008
learning rate0.00011705532693187012, loss:2.616307258605957
batch i:1009
learning rate0.00011705532693187012, loss:2.6296439170837402
batch i:1010
learning rate0.00011705532693187012, loss:2.587794780731201
batch i:1011
learning rate0.00011705532693187012, loss:2.594550132751465
batch i:1012
learning rate0.00011705532693187012, loss:2.608234405517578
batch i:1013
learning rate0.00011705532693187012, loss:2.595005989074707
batch i:1014
learning rate0.00011705532693187012, loss:2.646361827850342
batch i:1015
learning rate0.00011705532693187012, loss:2.5500574111938477
batch i:1016
learning rate0.00011705532693187012, loss:2.6698312759399414
batch i:1017
learning rate0.00011705532693187012, loss:2.595287799835205
batch i:1018
learning rate0.00011705532693187012, loss:2.622307538986206
batch i:1019
learning rate0.00011705532693187012, loss:2.6690917015075684
batch i:1020
learning rate0.00011705532693187012, loss:2.672542095184326
batch i:1021
learning rate0.00011705532693187012, loss:2.650731086730957
batch i:1022
learning rate0.00011705532693187012, loss:2.588583469390869
batch i:1023
learning rate0.00011705532693187012, loss:2.6465091705322266
batch i:1024
learning rate0.00011705532693187012, loss:2.5325260162353516
batch i:1025
learning rate0.00011705532693187012, loss:2.592343330383301
batch i:1026
learning rate0.00011705532693187012, loss:2.55155348777771
batch i:1027
learning rate0.00011705532693187012, loss:2.6650772094726562
batch i:1028
learning rate0.00011705532693187012, loss:2.5828540325164795
batch i:1029
learning rate0.00011705532693187012, loss:2.6932313442230225
batch i:1030
learning rate0.00011705532693187012, loss:2.5687382221221924
batch i:1031
learning rate0.00011705532693187012, loss:2.617612600326538
batch i:1032
learning rate0.00011705532693187012, loss:2.6567087173461914
batch i:1033
learning rate0.00011705532693187012, loss:2.6816563606262207
batch i:1034
learning rate0.00011705532693187012, loss:2.603538751602173
batch i:1035
learning rate0.00011705532693187012, loss:2.69846510887146
batch i:1036
learning rate0.00011705532693187012, loss:2.6010396480560303
batch i:1037
learning rate0.00011705532693187012, loss:2.733886957168579
batch i:1038
learning rate0.00011705532693187012, loss:2.651989459991455
batch i:1039
learning rate0.00011705532693187012, loss:2.544710636138916
batch i:1040
learning rate0.00011705532693187012, loss:2.6296520233154297
batch i:1041
learning rate0.00011705532693187012, loss:2.6231346130371094
batch i:1042
learning rate0.00011705532693187012, loss:2.641672372817993
batch i:1043
learning rate0.00011705532693187012, loss:2.623771905899048
batch i:1044
learning rate0.00011705532693187012, loss:2.789393663406372
batch i:1045
learning rate0.00011705532693187012, loss:2.6111252307891846
batch i:1046
learning rate0.00011705532693187012, loss:2.596977710723877
batch i:1047
learning rate0.00011705532693187012, loss:2.6592001914978027
batch i:1048
learning rate0.00011705532693187012, loss:2.669708251953125
batch i:1049
learning rate0.00011705532693187012, loss:2.835907220840454
batch i:1050
learning rate0.00011705532693187012, loss:2.68356990814209
batch i:1051
learning rate0.00011705532693187012, loss:2.746387481689453
batch i:1052
learning rate0.00011705532693187012, loss:2.6617207527160645
batch i:1053
learning rate0.00011705532693187012, loss:2.7920687198638916
batch i:1054
learning rate0.00011705532693187012, loss:2.7227911949157715
batch i:1055
learning rate0.00011705532693187012, loss:2.7823071479797363
batch i:1056
learning rate0.00011705532693187012, loss:2.6657207012176514
batch i:1057
learning rate0.00011705532693187012, loss:2.7234182357788086
batch i:1058
learning rate0.00011705532693187012, loss:2.6956777572631836
batch i:1059
learning rate0.00011705532693187012, loss:2.82564115524292
batch i:1060
learning rate0.00011705532693187012, loss:2.764024257659912
batch i:1061
learning rate0.00011705532693187012, loss:2.5960533618927
batch i:1062
learning rate0.00011705532693187012, loss:2.777519702911377
batch i:1063
learning rate0.00011705532693187012, loss:2.705101251602173
batch i:1064
learning rate0.00011705532693187012, loss:2.7105231285095215
batch i:1065
learning rate0.00011705532693187012, loss:2.66534423828125
batch i:1066
learning rate0.00011705532693187012, loss:2.743670701980591
batch i:1067
learning rate0.00011705532693187012, loss:2.657097816467285
batch i:1068
learning rate0.00011705532693187012, loss:2.799285888671875
batch i:1069
learning rate0.00011705532693187012, loss:2.623166084289551
batch i:1070
learning rate0.00011705532693187012, loss:2.678922653198242
batch i:1071
learning rate0.00011705532693187012, loss:2.8477087020874023
batch i:1072
learning rate0.00011705532693187012, loss:2.770799160003662
batch i:1073
learning rate0.00011705532693187012, loss:2.7317066192626953
batch i:1074
learning rate0.00011705532693187012, loss:2.665616989135742
batch i:1075
learning rate0.00011705532693187012, loss:2.6730382442474365
batch i:1076
learning rate0.00011705532693187012, loss:2.639221429824829
batch i:1077
learning rate0.00011705532693187012, loss:2.8328969478607178
batch i:1078
learning rate0.00011705532693187012, loss:2.679704427719116
batch i:1079
learning rate0.00011705532693187012, loss:2.7103915214538574
batch i:1080
learning rate0.00011705532693187012, loss:2.658531904220581
batch i:1081
learning rate0.00011705532693187012, loss:2.8134851455688477
batch i:1082
learning rate0.00011705532693187012, loss:2.6821861267089844
batch i:1083
learning rate0.00011705532693187012, loss:2.7356388568878174
batch i:1084
learning rate0.00011705532693187012, loss:2.8379368782043457
batch i:1085
learning rate0.00011705532693187012, loss:2.7659482955932617
batch i:1086
learning rate0.00011705532693187012, loss:2.725288152694702
batch i:1087
learning rate0.00011705532693187012, loss:2.8569610118865967
batch i:1088
learning rate0.00011705532693187012, loss:2.776453971862793
batch i:1089
learning rate0.00011705532693187012, loss:2.757070541381836
batch i:1090
learning rate0.00011705532693187012, loss:2.8663554191589355
batch i:1091
learning rate0.00011705532693187012, loss:2.8116724491119385
batch i:1092
learning rate0.00011705532693187012, loss:2.711869716644287
batch i:1093
learning rate0.00011705532693187012, loss:2.8170547485351562
batch i:1094
learning rate0.00011705532693187012, loss:2.8027679920196533
batch i:1095
learning rate0.00011705532693187012, loss:2.8589465618133545
batch i:1096
learning rate0.00011705532693187012, loss:2.7158799171447754
batch i:1097
learning rate0.00011705532693187012, loss:2.8330514430999756
batch i:1098
learning rate0.00011705532693187012, loss:2.898836612701416
batch i:1099
learning rate0.00011705532693187012, loss:2.802579164505005
batch i:1100
learning rate0.00011705532693187012, loss:2.8273491859436035
batch i:1101
learning rate0.00011705532693187012, loss:2.7146658897399902
batch i:1102
learning rate0.00011705532693187012, loss:2.6953253746032715
batch i:1103
learning rate0.00011705532693187012, loss:2.7709014415740967
batch i:1104
learning rate0.00011705532693187012, loss:2.7550230026245117
batch i:1105
learning rate0.00011705532693187012, loss:2.8053786754608154
batch i:1106
learning rate0.00011705532693187012, loss:2.8082706928253174
batch i:1107
learning rate0.00011705532693187012, loss:2.897428512573242
batch i:1108
learning rate0.00011705532693187012, loss:2.761629581451416
batch i:1109
learning rate0.00011705532693187012, loss:2.817203998565674
batch i:1110
learning rate0.00011705532693187012, loss:2.823349714279175
batch i:1111
learning rate0.00011705532693187012, loss:2.831453800201416
batch i:1112
learning rate0.00011705532693187012, loss:2.857722759246826
batch i:1113
learning rate0.00011705532693187012, loss:2.8208553791046143
batch i:1114
learning rate0.00011705532693187012, loss:2.8464837074279785
batch i:1115
learning rate0.00011705532693187012, loss:2.8807685375213623
batch i:1116
learning rate0.00011705532693187012, loss:2.8208651542663574
batch i:1117
learning rate0.00011705532693187012, loss:2.8916804790496826
batch i:1118
learning rate0.00011705532693187012, loss:2.800048828125
batch i:1119
learning rate0.00011705532693187012, loss:2.855557441711426
batch i:1120
learning rate0.00011705532693187012, loss:2.852354049682617
batch i:1121
learning rate0.00011705532693187012, loss:2.7892870903015137
batch i:1122
learning rate0.00011705532693187012, loss:2.9400534629821777
batch i:1123
learning rate0.00011705532693187012, loss:2.8386945724487305
batch i:1124
learning rate0.00011705532693187012, loss:2.7638425827026367
batch i:1125
learning rate0.00011705532693187012, loss:2.8709218502044678
batch i:1126
learning rate0.00011705532693187012, loss:2.799603223800659
batch i:1127
learning rate0.00011705532693187012, loss:2.893599271774292
batch i:1128
learning rate0.00011705532693187012, loss:2.9458060264587402
batch i:1129
learning rate0.00011705532693187012, loss:2.9991705417633057
batch i:1130
learning rate0.00011705532693187012, loss:2.823317766189575
batch i:1131
learning rate0.00011705532693187012, loss:2.9007065296173096
batch i:1132
learning rate0.00011705532693187012, loss:2.973238229751587
batch i:1133
learning rate0.00011705532693187012, loss:2.9035115242004395
batch i:1134
learning rate0.00011705532693187012, loss:2.869934558868408
batch i:1135
learning rate0.00011705532693187012, loss:2.8858964443206787
batch i:1136
learning rate0.00011705532693187012, loss:2.8787026405334473
batch i:1137
learning rate0.00011705532693187012, loss:2.854172706604004
batch i:1138
learning rate0.00011705532693187012, loss:2.9267826080322266
batch i:1139
learning rate0.00011705532693187012, loss:2.8464651107788086
batch i:1140
learning rate0.00011705532693187012, loss:2.850813388824463
batch i:1141
learning rate0.00011705532693187012, loss:2.8087573051452637
batch i:1142
learning rate0.00011705532693187012, loss:2.746849536895752
batch i:1143
learning rate0.00011705532693187012, loss:2.8447327613830566
batch i:1144
learning rate0.00011705532693187012, loss:2.7902698516845703
batch i:1145
learning rate0.00011705532693187012, loss:2.824531078338623
batch i:1146
learning rate0.00011705532693187012, loss:2.79763126373291
batch i:1147
learning rate0.00011705532693187012, loss:2.779050350189209
batch i:1148
learning rate0.00011705532693187012, loss:2.8849079608917236
batch i:1149
learning rate0.00011705532693187012, loss:2.8626441955566406
batch i:1150
learning rate0.00011705532693187012, loss:2.898940086364746
batch i:1151
learning rate0.00011705532693187012, loss:2.9263691902160645
batch i:1152
learning rate0.00011705532693187012, loss:2.926002025604248
batch i:1153
learning rate0.00011705532693187012, loss:2.861159324645996
batch i:1154
learning rate0.00011705532693187012, loss:2.8752715587615967
batch i:1155
learning rate0.00011705532693187012, loss:2.8853068351745605
batch i:1156
learning rate0.00011705532693187012, loss:2.816498279571533
batch i:1157
learning rate0.00011705532693187012, loss:2.959441661834717
batch i:1158
learning rate0.00011705532693187012, loss:2.9081320762634277
batch i:1159
learning rate0.00011705532693187012, loss:2.8922765254974365
batch i:1160
learning rate0.00011705532693187012, loss:2.8388936519622803
batch i:1161
learning rate0.00011705532693187012, loss:2.8631279468536377
batch i:1162
learning rate0.00011705532693187012, loss:2.7397594451904297
batch i:1163
learning rate0.00011705532693187012, loss:2.862933874130249
batch i:1164
learning rate0.00011705532693187012, loss:2.7810707092285156
batch i:1165
learning rate0.00011705532693187012, loss:2.9398000240325928
batch i:1166
learning rate0.00011705532693187012, loss:2.784407615661621
batch i:1167
learning rate0.00011705532693187012, loss:2.7438957691192627
batch i:1168
learning rate0.00011705532693187012, loss:2.7609992027282715
batch i:1169
learning rate0.00011705532693187012, loss:2.8487863540649414
batch i:1170
learning rate0.00011705532693187012, loss:2.7900633811950684
batch i:1171
learning rate0.00011705532693187012, loss:2.7547574043273926
batch i:1172
learning rate0.00011705532693187012, loss:2.875849723815918
batch i:1173
learning rate0.00011705532693187012, loss:2.785032272338867
batch i:1174
learning rate0.00011705532693187012, loss:2.7713563442230225
batch i:1175
learning rate0.00011705532693187012, loss:2.8128209114074707
batch i:1176
learning rate0.00011705532693187012, loss:2.80881929397583
batch i:1177
learning rate0.00011705532693187012, loss:2.6944470405578613
batch i:1178
learning rate0.00011705532693187012, loss:2.608283042907715
batch i:1179
learning rate0.00011705532693187012, loss:2.6586594581604004
batch i:1180
learning rate0.00011705532693187012, loss:2.7630844116210938
batch i:1181
learning rate0.00011705532693187012, loss:2.805628776550293
batch i:1182
learning rate0.00011705532693187012, loss:2.6794066429138184
batch i:1183
learning rate0.00011705532693187012, loss:2.7720441818237305
batch i:1184
learning rate0.00011705532693187012, loss:2.7643332481384277
batch i:1185
learning rate0.00011705532693187012, loss:2.823807716369629
batch i:1186
learning rate0.00011705532693187012, loss:2.8285646438598633
batch i:1187
learning rate0.00011705532693187012, loss:2.7227895259857178
batch i:1188
learning rate0.00011705532693187012, loss:2.768110990524292
batch i:1189
learning rate0.00011705532693187012, loss:2.784912347793579
batch i:1190
learning rate0.00011705532693187012, loss:2.896681785583496
batch i:1191
learning rate0.00011705532693187012, loss:2.8779945373535156
batch i:1192
learning rate0.00011705532693187012, loss:2.7024924755096436
batch i:1193
learning rate0.00011705532693187012, loss:2.779914379119873
batch i:1194
learning rate0.00011705532693187012, loss:2.8346900939941406
batch i:1195
learning rate0.00011705532693187012, loss:2.7310259342193604
batch i:1196
learning rate0.00011705532693187012, loss:2.8535327911376953
batch i:1197
learning rate0.00011705532693187012, loss:2.8188705444335938
batch i:1198
learning rate0.00011705532693187012, loss:2.782954216003418
batch i:1199
learning rate0.00011705532693187012, loss:2.819181203842163
batch i:1200
learning rate0.00011705532693187012, loss:2.809224843978882
batch i:1201
learning rate0.00011705532693187012, loss:2.8351240158081055
batch i:1202
learning rate0.00011705532693187012, loss:2.6996102333068848
batch i:1203
learning rate0.00011705532693187012, loss:2.8012397289276123
batch i:1204
learning rate0.00011705532693187012, loss:2.772186756134033
batch i:1205
learning rate0.00011705532693187012, loss:2.7771711349487305
batch i:1206
learning rate0.00011705532693187012, loss:2.6908841133117676
batch i:1207
learning rate0.00011705532693187012, loss:2.735203742980957
batch i:1208
learning rate0.00011705532693187012, loss:2.7983193397521973
batch i:1209
learning rate0.00011705532693187012, loss:2.679342269897461
batch i:1210
learning rate0.0001755829903978052, loss:2.6766862869262695
batch i:1211
learning rate0.0001755829903978052, loss:2.7529990673065186
batch i:1212
learning rate0.0001755829903978052, loss:2.7228076457977295
batch i:1213
learning rate0.0001755829903978052, loss:2.629087448120117
batch i:1214
learning rate0.0001755829903978052, loss:2.651533365249634
batch i:1215
learning rate0.0001755829903978052, loss:2.7162575721740723
batch i:1216
learning rate0.0001755829903978052, loss:2.6671319007873535
batch i:1217
learning rate0.0001755829903978052, loss:2.7067737579345703
batch i:1218
learning rate0.0001755829903978052, loss:2.7500908374786377
batch i:1219
learning rate0.00011705532693187012, loss:2.793266773223877
batch i:1220
learning rate0.00011705532693187012, loss:2.6985836029052734
batch i:1221
learning rate0.00011705532693187012, loss:2.7488837242126465
batch i:1222
learning rate0.0001755829903978052, loss:2.7006824016571045
batch i:1223
learning rate0.0001755829903978052, loss:2.670956611633301
batch i:1224
learning rate0.0001755829903978052, loss:2.708434581756592
batch i:1225
learning rate0.0001755829903978052, loss:2.6915388107299805
batch i:1226
learning rate0.0001755829903978052, loss:2.7315921783447266
batch i:1227
learning rate0.0001755829903978052, loss:2.654470205307007
batch i:1228
learning rate0.0001755829903978052, loss:2.6259500980377197
batch i:1229
learning rate0.0001755829903978052, loss:2.617447853088379
batch i:1230
learning rate0.0001755829903978052, loss:2.5629754066467285
batch i:1231
learning rate0.0001755829903978052, loss:2.6559371948242188
batch i:1232
learning rate0.0001755829903978052, loss:2.57562255859375
batch i:1233
learning rate0.0001755829903978052, loss:2.6121504306793213
batch i:1234
learning rate0.0001755829903978052, loss:2.5973172187805176
batch i:1235
learning rate0.0001755829903978052, loss:2.709184169769287
batch i:1236
learning rate0.0001755829903978052, loss:2.5853183269500732
batch i:1237
learning rate0.0001755829903978052, loss:2.5967466831207275
batch i:1238
learning rate0.0001755829903978052, loss:2.609450340270996
batch i:1239
learning rate0.0001755829903978052, loss:2.6003103256225586
batch i:1240
learning rate0.0001755829903978052, loss:2.6484222412109375
batch i:1241
learning rate0.0001755829903978052, loss:2.6568214893341064
batch i:1242
learning rate0.0001755829903978052, loss:2.6979734897613525
batch i:1243
learning rate0.0001755829903978052, loss:2.5544521808624268
batch i:1244
learning rate0.0001755829903978052, loss:2.7140254974365234
batch i:1245
learning rate0.0001755829903978052, loss:2.655501365661621
batch i:1246
learning rate0.0001755829903978052, loss:2.61360502243042
batch i:1247
learning rate0.0001755829903978052, loss:2.58774471282959
batch i:1248
learning rate0.0001755829903978052, loss:2.672412395477295
batch i:1249
learning rate0.0001755829903978052, loss:2.6927285194396973
batch i:1250
learning rate0.0001755829903978052, loss:2.6702489852905273
current self-play batch: 1250
num_playouts:3000, win: 4, lose: 6, tie:0
average time: 788.6777632474899
batch i:1251
learning rate0.0001755829903978052, loss:2.7229723930358887
batch i:1252
learning rate0.0001755829903978052, loss:2.692183494567871
batch i:1253
learning rate0.0001755829903978052, loss:2.6396007537841797
batch i:1254
learning rate0.0001755829903978052, loss:2.6987552642822266
batch i:1255
learning rate0.0001755829903978052, loss:2.623802900314331
batch i:1256
learning rate0.0001755829903978052, loss:2.687985897064209
batch i:1257
learning rate0.0001755829903978052, loss:2.648531675338745
batch i:1258
learning rate0.0001755829903978052, loss:2.593170166015625
batch i:1259
learning rate0.0001755829903978052, loss:2.6393959522247314
batch i:1260
learning rate0.0001755829903978052, loss:2.618889808654785
batch i:1261
learning rate0.0001755829903978052, loss:2.664471387863159
batch i:1262
learning rate0.0001755829903978052, loss:2.7253541946411133
batch i:1263
learning rate0.0001755829903978052, loss:2.6796956062316895
batch i:1264
learning rate0.0001755829903978052, loss:2.7575440406799316
batch i:1265
learning rate0.0001755829903978052, loss:2.762531042098999
batch i:1266
learning rate0.0001755829903978052, loss:2.656381368637085
batch i:1267
learning rate0.0001755829903978052, loss:2.7107672691345215
batch i:1268
learning rate0.0001755829903978052, loss:2.72027587890625
batch i:1269
learning rate0.0001755829903978052, loss:2.7505550384521484
batch i:1270
learning rate0.0001755829903978052, loss:2.6544995307922363
batch i:1271
learning rate0.0001755829903978052, loss:2.737140417098999
batch i:1272
learning rate0.0001755829903978052, loss:2.7207114696502686
batch i:1273
learning rate0.0001755829903978052, loss:2.713853359222412
batch i:1274
learning rate0.0001755829903978052, loss:2.6414361000061035
batch i:1275
learning rate0.0001755829903978052, loss:2.83872127532959
batch i:1276
learning rate0.0001755829903978052, loss:2.7975401878356934
batch i:1277
learning rate0.0001755829903978052, loss:2.7166121006011963
batch i:1278
learning rate0.0001755829903978052, loss:2.6231558322906494
batch i:1279
learning rate0.0001755829903978052, loss:2.777656316757202
batch i:1280
learning rate0.0001755829903978052, loss:2.6998074054718018
batch i:1281
learning rate0.0001755829903978052, loss:2.6668224334716797
batch i:1282
learning rate0.0001755829903978052, loss:2.6306214332580566
batch i:1283
learning rate0.0001755829903978052, loss:2.708873987197876
batch i:1284
learning rate0.0001755829903978052, loss:2.6914725303649902
batch i:1285
learning rate0.0001755829903978052, loss:2.6933233737945557
batch i:1286
learning rate0.0001755829903978052, loss:2.6435387134552
batch i:1287
learning rate0.0001755829903978052, loss:2.721618413925171
batch i:1288
learning rate0.0001755829903978052, loss:2.7829325199127197
batch i:1289
learning rate0.0001755829903978052, loss:2.711603879928589
batch i:1290
learning rate0.0001755829903978052, loss:2.726754665374756
batch i:1291
learning rate0.0001755829903978052, loss:2.6169631481170654
batch i:1292
learning rate0.0001755829903978052, loss:2.560586929321289
batch i:1293
learning rate0.0001755829903978052, loss:2.698925018310547
batch i:1294
learning rate0.0001755829903978052, loss:2.6973719596862793
batch i:1295
learning rate0.0001755829903978052, loss:2.707408905029297
batch i:1296
learning rate0.0001755829903978052, loss:2.6769299507141113
batch i:1297
learning rate0.0001755829903978052, loss:2.7446703910827637
batch i:1298
learning rate0.0001755829903978052, loss:2.7323811054229736
batch i:1299
learning rate0.0001755829903978052, loss:2.801199436187744
batch i:1300
learning rate0.0001755829903978052, loss:2.7101125717163086
batch i:1301
learning rate0.0001755829903978052, loss:2.7386300563812256
batch i:1302
learning rate0.0001755829903978052, loss:2.735280752182007
batch i:1303
learning rate0.0001755829903978052, loss:2.8331141471862793
batch i:1304
learning rate0.0001755829903978052, loss:2.745173454284668
batch i:1305
learning rate0.0001755829903978052, loss:2.693936824798584
batch i:1306
learning rate0.0001755829903978052, loss:2.7799901962280273
batch i:1307
learning rate0.0001755829903978052, loss:2.7588534355163574
batch i:1308
learning rate0.0001755829903978052, loss:2.8308212757110596
batch i:1309
learning rate0.0001755829903978052, loss:2.6627309322357178
batch i:1310
learning rate0.0001755829903978052, loss:2.6431031227111816
batch i:1311
learning rate0.0001755829903978052, loss:2.6981937885284424
batch i:1312
learning rate0.0001755829903978052, loss:2.7720096111297607
batch i:1313
learning rate0.0001755829903978052, loss:2.678365468978882
batch i:1314
learning rate0.0001755829903978052, loss:2.7091546058654785
batch i:1315
learning rate0.0001755829903978052, loss:2.679537296295166
batch i:1316
learning rate0.0001755829903978052, loss:2.767618179321289
batch i:1317
learning rate0.0001755829903978052, loss:2.5836119651794434
batch i:1318
learning rate0.0001755829903978052, loss:2.734567403793335
batch i:1319
learning rate0.0001755829903978052, loss:2.7010209560394287
batch i:1320
learning rate0.0001755829903978052, loss:2.6909146308898926
batch i:1321
learning rate0.0001755829903978052, loss:2.6544268131256104
batch i:1322
learning rate0.0001755829903978052, loss:2.780388116836548
batch i:1323
learning rate0.0001755829903978052, loss:2.7662391662597656
batch i:1324
learning rate0.0001755829903978052, loss:2.7638931274414062
batch i:1325
learning rate0.0001755829903978052, loss:2.667212724685669
batch i:1326
learning rate0.0001755829903978052, loss:2.7015676498413086
batch i:1327
learning rate0.0001755829903978052, loss:2.761692762374878
batch i:1328
learning rate0.0001755829903978052, loss:2.7524404525756836
batch i:1329
learning rate0.0001755829903978052, loss:2.833131790161133
batch i:1330
learning rate0.0001755829903978052, loss:2.757539987564087
batch i:1331
learning rate0.0001755829903978052, loss:2.783810615539551
batch i:1332
learning rate0.0001755829903978052, loss:2.763972759246826
batch i:1333
learning rate0.0001755829903978052, loss:2.7569174766540527
batch i:1334
learning rate0.0001755829903978052, loss:2.6880178451538086
batch i:1335
learning rate0.0001755829903978052, loss:2.6977529525756836
batch i:1336
learning rate0.0001755829903978052, loss:2.720850706100464
batch i:1337
learning rate0.0001755829903978052, loss:2.7579264640808105
batch i:1338
learning rate0.0001755829903978052, loss:2.8356106281280518
batch i:1339
learning rate0.0001755829903978052, loss:2.81734037399292
batch i:1340
learning rate0.0001755829903978052, loss:2.6613903045654297
batch i:1341
learning rate0.0001755829903978052, loss:2.659290075302124
batch i:1342
learning rate0.0001755829903978052, loss:2.683411121368408
batch i:1343
learning rate0.0001755829903978052, loss:2.7388172149658203
batch i:1344
learning rate0.0001755829903978052, loss:2.741314649581909
batch i:1345
learning rate0.0001755829903978052, loss:2.7525277137756348
batch i:1346
learning rate0.0001755829903978052, loss:2.5754499435424805
batch i:1347
learning rate0.0001755829903978052, loss:2.584951400756836
batch i:1348
learning rate0.0001755829903978052, loss:2.70068097114563
batch i:1349
learning rate0.0001755829903978052, loss:2.682830572128296
batch i:1350
learning rate0.0001755829903978052, loss:2.7541685104370117
batch i:1351
learning rate0.0001755829903978052, loss:2.655500888824463
batch i:1352
learning rate0.0001755829903978052, loss:2.7373063564300537
batch i:1353
learning rate0.0001755829903978052, loss:2.6966452598571777
batch i:1354
learning rate0.0001755829903978052, loss:2.75521183013916
batch i:1355
learning rate0.0001755829903978052, loss:2.694967269897461
batch i:1356
learning rate0.0001755829903978052, loss:2.7008323669433594
batch i:1357
learning rate0.0001755829903978052, loss:2.684744119644165
batch i:1358
learning rate0.0001755829903978052, loss:2.6619880199432373
batch i:1359
learning rate0.0001755829903978052, loss:2.7413008213043213
batch i:1360
learning rate0.0001755829903978052, loss:2.68112850189209
batch i:1361
learning rate0.0001755829903978052, loss:2.6560702323913574
batch i:1362
learning rate0.0001755829903978052, loss:2.741687774658203
batch i:1363
learning rate0.0001755829903978052, loss:2.6831347942352295
batch i:1364
learning rate0.0001755829903978052, loss:2.7079062461853027
batch i:1365
learning rate0.0001755829903978052, loss:2.6996054649353027
batch i:1366
learning rate0.0001755829903978052, loss:2.782161235809326
batch i:1367
learning rate0.0001755829903978052, loss:2.6128461360931396
batch i:1368
learning rate0.0001755829903978052, loss:2.710023880004883
batch i:1369
learning rate0.0001755829903978052, loss:2.737496852874756
batch i:1370
learning rate0.0001755829903978052, loss:2.765084743499756
batch i:1371
learning rate0.0001755829903978052, loss:2.5630111694335938
batch i:1372
learning rate0.0001755829903978052, loss:2.680063247680664
batch i:1373
learning rate0.0001755829903978052, loss:2.684302568435669
batch i:1374
learning rate0.0001755829903978052, loss:2.6619489192962646
batch i:1375
learning rate0.0001755829903978052, loss:2.693493604660034
batch i:1376
learning rate0.0001755829903978052, loss:2.7282371520996094
batch i:1377
learning rate0.0001755829903978052, loss:2.6551637649536133
batch i:1378
learning rate0.0001755829903978052, loss:2.708523750305176
batch i:1379
learning rate0.0001755829903978052, loss:2.7673909664154053
batch i:1380
learning rate0.0001755829903978052, loss:2.6758406162261963
batch i:1381
learning rate0.0001755829903978052, loss:2.601369619369507
batch i:1382
learning rate0.0001755829903978052, loss:2.675431966781616
batch i:1383
learning rate0.0001755829903978052, loss:2.603342056274414
batch i:1384
learning rate0.0001755829903978052, loss:2.669785499572754
batch i:1385
learning rate0.0001755829903978052, loss:2.6188316345214844
batch i:1386
learning rate0.0001755829903978052, loss:2.676879644393921
batch i:1387
learning rate0.0001755829903978052, loss:2.5470998287200928
batch i:1388
learning rate0.0001755829903978052, loss:2.6046159267425537
batch i:1389
learning rate0.0001755829903978052, loss:2.722660541534424
batch i:1390
learning rate0.0001755829903978052, loss:2.7236459255218506
batch i:1391
learning rate0.0001755829903978052, loss:2.6306769847869873
batch i:1392
learning rate0.0001755829903978052, loss:2.6202383041381836
batch i:1393
learning rate0.0001755829903978052, loss:2.563939332962036
batch i:1394
learning rate0.0001755829903978052, loss:2.6549272537231445
batch i:1395
learning rate0.0001755829903978052, loss:2.7013351917266846
batch i:1396
learning rate0.0001755829903978052, loss:2.6689624786376953
batch i:1397
learning rate0.0001755829903978052, loss:2.7118990421295166
batch i:1398
learning rate0.0001755829903978052, loss:2.683049440383911
batch i:1399
learning rate0.0001755829903978052, loss:2.6748404502868652
batch i:1400
learning rate0.0001755829903978052, loss:2.693244695663452
batch i:1401
learning rate0.0001755829903978052, loss:2.735105037689209
batch i:1402
learning rate0.0001755829903978052, loss:2.692310094833374
batch i:1403
learning rate0.0001755829903978052, loss:2.6003623008728027
batch i:1404
learning rate0.0001755829903978052, loss:2.752192258834839
batch i:1405
learning rate0.0001755829903978052, loss:2.623016119003296
batch i:1406
learning rate0.0001755829903978052, loss:2.7109858989715576
batch i:1407
learning rate0.0001755829903978052, loss:2.712132692337036
batch i:1408
learning rate0.0001755829903978052, loss:2.73089599609375
batch i:1409
learning rate0.0001755829903978052, loss:2.652050018310547
batch i:1410
learning rate0.0001755829903978052, loss:2.6058709621429443
batch i:1411
learning rate0.0001755829903978052, loss:2.62546443939209
batch i:1412
learning rate0.0001755829903978052, loss:2.5195720195770264
batch i:1413
learning rate0.0001755829903978052, loss:2.7237343788146973
batch i:1414
learning rate0.0001755829903978052, loss:2.758950710296631
batch i:1415
learning rate0.0001755829903978052, loss:2.5750808715820312
batch i:1416
learning rate0.0001755829903978052, loss:2.6329264640808105
batch i:1417
learning rate0.0001755829903978052, loss:2.6186249256134033
batch i:1418
learning rate0.0001755829903978052, loss:2.645315170288086
batch i:1419
learning rate0.0001755829903978052, loss:2.6068968772888184
batch i:1420
learning rate0.0001755829903978052, loss:2.6046695709228516
batch i:1421
learning rate0.0001755829903978052, loss:2.623650074005127
batch i:1422
learning rate0.0001755829903978052, loss:2.643298864364624
batch i:1423
learning rate0.0001755829903978052, loss:2.611300468444824
batch i:1424
learning rate0.0001755829903978052, loss:2.539006233215332
batch i:1425
learning rate0.0001755829903978052, loss:2.6227447986602783
batch i:1426
learning rate0.0001755829903978052, loss:2.6450390815734863
batch i:1427
learning rate0.0001755829903978052, loss:2.606818675994873
batch i:1428
learning rate0.0001755829903978052, loss:2.51584529876709
batch i:1429
learning rate0.0001755829903978052, loss:2.6041996479034424
batch i:1430
learning rate0.0001755829903978052, loss:2.656144142150879
batch i:1431
learning rate0.0001755829903978052, loss:2.6904313564300537
batch i:1432
learning rate0.0001755829903978052, loss:2.728753089904785
batch i:1433
learning rate0.0001755829903978052, loss:2.639765501022339
batch i:1434
learning rate0.0001755829903978052, loss:2.616748332977295
batch i:1435
learning rate0.0001755829903978052, loss:2.6562395095825195
batch i:1436
learning rate0.0001755829903978052, loss:2.5756587982177734
batch i:1437
learning rate0.0001755829903978052, loss:2.7299435138702393
batch i:1438
learning rate0.0001755829903978052, loss:2.6201205253601074
batch i:1439
learning rate0.0001755829903978052, loss:2.67820405960083
batch i:1440
learning rate0.0001755829903978052, loss:2.644169807434082
batch i:1441
learning rate0.0001755829903978052, loss:2.595029354095459
batch i:1442
learning rate0.0001755829903978052, loss:2.5965700149536133
batch i:1443
learning rate0.0001755829903978052, loss:2.67263126373291
batch i:1444
learning rate0.0001755829903978052, loss:2.6122262477874756
batch i:1445
learning rate0.0001755829903978052, loss:2.682682514190674
batch i:1446
learning rate0.0001755829903978052, loss:2.6047112941741943
batch i:1447
learning rate0.0001755829903978052, loss:2.659325122833252
batch i:1448
learning rate0.0001755829903978052, loss:2.5746569633483887
batch i:1449
learning rate0.0001755829903978052, loss:2.657801628112793
batch i:1450
learning rate0.0001755829903978052, loss:2.6178057193756104
batch i:1451
learning rate0.0001755829903978052, loss:2.674609661102295
batch i:1452
learning rate0.0001755829903978052, loss:2.6816201210021973
batch i:1453
learning rate0.0001755829903978052, loss:2.5476183891296387
batch i:1454
learning rate0.0001755829903978052, loss:2.678614854812622
batch i:1455
learning rate0.0001755829903978052, loss:2.784883975982666
batch i:1456
learning rate0.0001755829903978052, loss:2.5932846069335938
batch i:1457
learning rate0.0001755829903978052, loss:2.652463436126709
batch i:1458
learning rate0.0001755829903978052, loss:2.627978801727295
batch i:1459
learning rate0.0001755829903978052, loss:2.661884307861328
batch i:1460
learning rate0.0001755829903978052, loss:2.7490172386169434
batch i:1461
learning rate0.0001755829903978052, loss:2.6475212574005127
batch i:1462
learning rate0.0001755829903978052, loss:2.62581205368042
batch i:1463
learning rate0.0001755829903978052, loss:2.747028112411499
batch i:1464
learning rate0.0001755829903978052, loss:2.688187837600708
batch i:1465
learning rate0.0001755829903978052, loss:2.745035171508789
batch i:1466
learning rate0.0001755829903978052, loss:2.6770031452178955
batch i:1467
learning rate0.0001755829903978052, loss:2.7068943977355957
batch i:1468
learning rate0.0001755829903978052, loss:2.707599639892578
batch i:1469
learning rate0.0001755829903978052, loss:2.6437935829162598
batch i:1470
learning rate0.0001755829903978052, loss:2.7526445388793945
batch i:1471
learning rate0.0001755829903978052, loss:2.677718162536621
batch i:1472
learning rate0.0001755829903978052, loss:2.6348378658294678
batch i:1473
learning rate0.0001755829903978052, loss:2.8563599586486816
batch i:1474
learning rate0.0001755829903978052, loss:2.7409067153930664
batch i:1475
learning rate0.0001755829903978052, loss:2.8480024337768555
batch i:1476
learning rate0.0001755829903978052, loss:2.762321949005127
batch i:1477
learning rate0.0001755829903978052, loss:2.852752208709717
batch i:1478
learning rate0.0001755829903978052, loss:2.7017693519592285
batch i:1479
learning rate0.0001755829903978052, loss:2.6867904663085938
batch i:1480
learning rate0.0001755829903978052, loss:2.7239909172058105
batch i:1481
learning rate0.0001755829903978052, loss:2.7249083518981934
batch i:1482
learning rate0.0001755829903978052, loss:2.665874481201172
batch i:1483
learning rate0.0001755829903978052, loss:2.8012943267822266
batch i:1484
learning rate0.0001755829903978052, loss:2.8285317420959473
batch i:1485
learning rate0.0001755829903978052, loss:2.717869758605957
batch i:1486
learning rate0.0001755829903978052, loss:2.779914140701294
batch i:1487
learning rate0.0001755829903978052, loss:2.790724277496338
batch i:1488
learning rate0.0001755829903978052, loss:2.759035587310791
batch i:1489
learning rate0.0001755829903978052, loss:2.7785420417785645
batch i:1490
learning rate0.0001755829903978052, loss:2.746608257293701
batch i:1491
learning rate0.0001755829903978052, loss:2.741128444671631
batch i:1492
learning rate0.0001755829903978052, loss:2.754375696182251
batch i:1493
learning rate0.0001755829903978052, loss:2.7200918197631836
batch i:1494
learning rate0.0001755829903978052, loss:2.7936220169067383
batch i:1495
learning rate0.0001755829903978052, loss:2.841477870941162
batch i:1496
learning rate0.0001755829903978052, loss:2.7747607231140137
batch i:1497
learning rate0.0001755829903978052, loss:2.8660223484039307
batch i:1498
learning rate0.0001755829903978052, loss:2.7235264778137207
batch i:1499
learning rate0.0001755829903978052, loss:2.6882481575012207
batch i:1500
learning rate0.0001755829903978052, loss:2.7564988136291504
current self-play batch: 1500
num_playouts:3000, win: 6, lose: 4, tie:0
average time: 681.6581390142441
