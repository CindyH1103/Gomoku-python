Start time: 2024-01-07 09:32:02.471471
priority replay buffer is in use
batch i:1
batch i:2
batch i:3
learning rate: 0.0013333333333333333, loss: 2.676262855529785
batch i:4
learning rate: 0.0008888888888888888, loss: 2.5829825401306152
batch i:5
learning rate: 0.0005925925925925926, loss: 2.6119065284729004
batch i:6
learning rate: 0.0003950617283950617, loss: 2.534193992614746
batch i:7
learning rate: 0.0002633744855967078, loss: 2.5915675163269043
batch i:8
learning rate: 0.0002633744855967078, loss: 2.5902953147888184
batch i:9
learning rate: 0.0002633744855967078, loss: 2.6450600624084473
batch i:10
learning rate: 0.0002633744855967078, loss: 2.751849889755249
current self-play batch: 10
num_playouts:1000, win: 10, lose: 0, tie:0
average time: 224.60268149375915
New best policy from pure MCTS
batch i:11
learning rate: 0.0002633744855967078, loss: 2.1740660667419434
batch i:12
learning rate: 0.0002633744855967078, loss: 2.4069604873657227
batch i:13
learning rate: 0.0002633744855967078, loss: 2.359844446182251
batch i:14
/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/autograd/__init__.py:173: UserWarning: Error detected in ConvolutionBackward0. Traceback of forward call that caused the error:
  File "/mnt/nas/home/huangyixin/AI/train.py", line 402, in <module>
    training_pipeline.train(game_batch_num)
  File "/mnt/nas/home/huangyixin/AI/train.py", line 304, in train
    self.policy_update()
  File "/mnt/nas/home/huangyixin/AI/train.py", line 271, in policy_update
    loss = self.train_step(state_batch,
  File "/mnt/nas/home/huangyixin/AI/train.py", line 344, in train_step
    policy_logits, value_logits, value = self.net.eval_state(state_batch)
  File "/mnt/nas/home/huangyixin/AI/model.py", line 296, in eval_state
    policy_logits, value_logits, value = self.net(state_batch)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/nas/home/huangyixin/AI/model.py", line 110, in forward
    policy_logits = self.policy_head(x)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 457, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 453, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484775609/work/torch/csrc/autograd/python_anomaly_mode.cpp:104.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Traceback (most recent call last):
  File "/mnt/nas/home/huangyixin/AI/train.py", line 402, in <module>
    training_pipeline.train(game_batch_num)
  File "/mnt/nas/home/huangyixin/AI/train.py", line 304, in train
    self.policy_update()
  File "/mnt/nas/home/huangyixin/AI/train.py", line 271, in policy_update
    loss = self.train_step(state_batch,
  File "/mnt/nas/home/huangyixin/AI/train.py", line 371, in train_step
    loss.backward()
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 10.76 GiB total capacity; 9.35 GiB already allocated; 35.44 MiB free; 9.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
