Start time: 2023-12-23 00:25:03.764482
batch i:1
batch i:2
batch i:3
batch i:4
batch i:5
batch i:6
learning rate0.0013333333333333333, loss:4.54918098449707
batch i:7
learning rate0.0008888888888888888, loss:4.595069885253906
batch i:8
learning rate0.0005925925925925926, loss:4.72577428817749
batch i:9
learning rate0.0003950617283950617, loss:4.2854156494140625
batch i:10
learning rate0.0002633744855967078, loss:4.109724998474121
batch i:11
learning rate0.0001755829903978052, loss:3.8287270069122314
batch i:12
learning rate0.0001755829903978052, loss:3.7244582176208496
batch i:13
learning rate0.0001755829903978052, loss:3.616684913635254
batch i:14
learning rate0.0001755829903978052, loss:3.5801098346710205
batch i:15
learning rate0.0001755829903978052, loss:3.5737991333007812
batch i:16
learning rate0.0001755829903978052, loss:3.5016732215881348
batch i:17
learning rate0.0001755829903978052, loss:3.4404349327087402
batch i:18
learning rate0.0001755829903978052, loss:3.4840049743652344
batch i:19
learning rate0.0002633744855967078, loss:3.405580997467041
batch i:20
learning rate0.0002633744855967078, loss:3.352917432785034
batch i:21
learning rate0.0002633744855967078, loss:3.3452465534210205
batch i:22
learning rate0.0002633744855967078, loss:3.411309242248535
batch i:23
learning rate0.0002633744855967078, loss:3.4022057056427
batch i:24
learning rate0.0002633744855967078, loss:3.317627429962158
batch i:25
learning rate0.0002633744855967078, loss:3.3089985847473145
batch i:26
learning rate0.0002633744855967078, loss:3.3445749282836914
batch i:27
learning rate0.0002633744855967078, loss:3.2441580295562744
batch i:28
learning rate0.0002633744855967078, loss:3.3557422161102295
batch i:29
learning rate0.0002633744855967078, loss:3.268156051635742
batch i:30
learning rate0.0002633744855967078, loss:3.2991931438446045
batch i:31
learning rate0.0002633744855967078, loss:3.294050693511963
batch i:32
learning rate0.0002633744855967078, loss:3.214570999145508
batch i:33
learning rate0.0002633744855967078, loss:3.2023534774780273
batch i:34
learning rate0.0002633744855967078, loss:3.2144718170166016
batch i:35
learning rate0.0002633744855967078, loss:3.199981212615967
batch i:36
learning rate0.0002633744855967078, loss:3.260899066925049
batch i:37
learning rate0.0002633744855967078, loss:3.178030490875244
batch i:38
learning rate0.0002633744855967078, loss:3.2371344566345215
batch i:39
learning rate0.0002633744855967078, loss:3.128272533416748
batch i:40
learning rate0.0002633744855967078, loss:3.2732415199279785
batch i:41
learning rate0.0002633744855967078, loss:3.182302951812744
batch i:42
learning rate0.0002633744855967078, loss:3.2110626697540283
batch i:43
learning rate0.0002633744855967078, loss:3.252516269683838
batch i:44
learning rate0.0002633744855967078, loss:3.1824193000793457
batch i:45
learning rate0.0002633744855967078, loss:3.199787139892578
batch i:46
learning rate0.0002633744855967078, loss:3.211331844329834
batch i:47
learning rate0.0002633744855967078, loss:3.157088041305542
batch i:48
learning rate0.0002633744855967078, loss:3.218954086303711
batch i:49
learning rate0.0002633744855967078, loss:3.1981561183929443
batch i:50
learning rate0.0002633744855967078, loss:3.2042551040649414
current self-play batch: 50
num_playouts:1000, win: 8, lose: 2, tie:0
average time: 347.8215072154999
New best policy from pure MCTS
batch i:51
learning rate0.0002633744855967078, loss:3.1911044120788574
batch i:52
learning rate0.0002633744855967078, loss:3.1551268100738525
batch i:53
learning rate0.00039506172839506165, loss:3.209625244140625
batch i:54
learning rate0.00039506172839506165, loss:3.147862434387207
batch i:55
learning rate0.00039506172839506165, loss:3.1843714714050293
batch i:56
learning rate0.00039506172839506165, loss:3.1643424034118652
batch i:57
learning rate0.00039506172839506165, loss:3.1705121994018555
batch i:58
learning rate0.00039506172839506165, loss:3.169114351272583
batch i:59
learning rate0.00039506172839506165, loss:3.0832855701446533
batch i:60
learning rate0.00039506172839506165, loss:3.106205701828003
batch i:61
learning rate0.00039506172839506165, loss:3.149143695831299
batch i:62
learning rate0.00039506172839506165, loss:3.2011475563049316
batch i:63
learning rate0.00039506172839506165, loss:3.1859235763549805
batch i:64
learning rate0.00039506172839506165, loss:3.1357767581939697
batch i:65
learning rate0.00039506172839506165, loss:3.11540150642395
batch i:66
learning rate0.00039506172839506165, loss:3.085697650909424
batch i:67
learning rate0.00039506172839506165, loss:3.1790335178375244
batch i:68
learning rate0.00039506172839506165, loss:3.1820828914642334
batch i:69
learning rate0.00039506172839506165, loss:3.122535228729248
batch i:70
learning rate0.00039506172839506165, loss:3.108936309814453
batch i:71
learning rate0.00039506172839506165, loss:3.0642476081848145
batch i:72
learning rate0.00039506172839506165, loss:3.07869291305542
batch i:73
learning rate0.00039506172839506165, loss:3.045616865158081
batch i:74
learning rate0.00039506172839506165, loss:3.027276039123535
batch i:75
learning rate0.00039506172839506165, loss:3.062865734100342
batch i:76
learning rate0.00039506172839506165, loss:3.104358673095703
batch i:77
learning rate0.00039506172839506165, loss:3.027109384536743
batch i:78
learning rate0.0002633744855967078, loss:3.106410503387451
batch i:79
learning rate0.0002633744855967078, loss:3.053192138671875
batch i:80
learning rate0.0002633744855967078, loss:3.1012165546417236
batch i:81
learning rate0.0002633744855967078, loss:3.1261558532714844
batch i:82
learning rate0.0002633744855967078, loss:3.055727243423462
batch i:83
learning rate0.0002633744855967078, loss:3.1043457984924316
batch i:84
learning rate0.0002633744855967078, loss:3.0718626976013184
batch i:85
learning rate0.0002633744855967078, loss:3.0459671020507812
batch i:86
learning rate0.0002633744855967078, loss:3.045508861541748
batch i:87
learning rate0.0002633744855967078, loss:2.991497278213501
batch i:88
learning rate0.0002633744855967078, loss:3.1129486560821533
batch i:89
learning rate0.0002633744855967078, loss:3.1486566066741943
batch i:90
learning rate0.0002633744855967078, loss:3.1203887462615967
batch i:91
learning rate0.0002633744855967078, loss:3.065739631652832
batch i:92
learning rate0.0002633744855967078, loss:3.0354669094085693
batch i:93
learning rate0.0002633744855967078, loss:3.0340628623962402
batch i:94
learning rate0.0002633744855967078, loss:3.0839431285858154
batch i:95
learning rate0.0002633744855967078, loss:3.156623601913452
batch i:96
learning rate0.0002633744855967078, loss:3.075345993041992
batch i:97
learning rate0.0002633744855967078, loss:3.1061642169952393
batch i:98
learning rate0.0002633744855967078, loss:3.214517831802368
batch i:99
learning rate0.0002633744855967078, loss:3.0835113525390625
batch i:100
learning rate0.0002633744855967078, loss:3.0630877017974854
current self-play batch: 100
num_playouts:1000, win: 9, lose: 1, tie:0
average time: 240.12770020961761
New best policy from pure MCTS
batch i:101
learning rate0.0002633744855967078, loss:3.0726070404052734
batch i:102
learning rate0.0002633744855967078, loss:3.0471882820129395
batch i:103
learning rate0.0002633744855967078, loss:3.1315412521362305
batch i:104
learning rate0.0002633744855967078, loss:3.1324853897094727
batch i:105
learning rate0.0002633744855967078, loss:3.129608392715454
batch i:106
learning rate0.0002633744855967078, loss:3.1715049743652344
batch i:107
learning rate0.0002633744855967078, loss:3.115593910217285
batch i:108
learning rate0.0002633744855967078, loss:3.049617052078247
batch i:109
learning rate0.0002633744855967078, loss:3.063756227493286
batch i:110
learning rate0.0002633744855967078, loss:3.117734909057617
batch i:111
learning rate0.0002633744855967078, loss:3.067486047744751
batch i:112
learning rate0.0002633744855967078, loss:3.0809664726257324
batch i:113
learning rate0.0002633744855967078, loss:3.083073616027832
batch i:114
learning rate0.0002633744855967078, loss:3.02988862991333
batch i:115
learning rate0.0002633744855967078, loss:3.0451083183288574
batch i:116
learning rate0.0002633744855967078, loss:3.0284409523010254
batch i:117
learning rate0.0002633744855967078, loss:3.0649919509887695
batch i:118
learning rate0.0002633744855967078, loss:3.006784200668335
batch i:119
learning rate0.0002633744855967078, loss:3.1128225326538086
batch i:120
learning rate0.0002633744855967078, loss:3.082448959350586
batch i:121
learning rate0.0002633744855967078, loss:2.9985251426696777
batch i:122
learning rate0.0002633744855967078, loss:3.054581642150879
batch i:123
learning rate0.0002633744855967078, loss:3.0682077407836914
batch i:124
learning rate0.0002633744855967078, loss:3.094632625579834
batch i:125
learning rate0.0002633744855967078, loss:3.051975727081299
batch i:126
learning rate0.0002633744855967078, loss:3.0588173866271973
batch i:127
learning rate0.0002633744855967078, loss:3.098324775695801
batch i:128
learning rate0.0002633744855967078, loss:3.0614542961120605
batch i:129
learning rate0.0002633744855967078, loss:2.9803507328033447
batch i:130
learning rate0.0002633744855967078, loss:2.990104913711548
batch i:131
learning rate0.0002633744855967078, loss:3.058283805847168
batch i:132
learning rate0.0002633744855967078, loss:2.9800541400909424
batch i:133
learning rate0.0002633744855967078, loss:3.060865640640259
batch i:134
learning rate0.0002633744855967078, loss:2.9684324264526367
batch i:135
learning rate0.0002633744855967078, loss:3.01627516746521
batch i:136
learning rate0.0002633744855967078, loss:3.0393190383911133
batch i:137
learning rate0.0002633744855967078, loss:3.036442518234253
batch i:138
learning rate0.0002633744855967078, loss:3.0053248405456543
batch i:139
learning rate0.0002633744855967078, loss:3.0143001079559326
batch i:140
learning rate0.0002633744855967078, loss:3.040536642074585
batch i:141
learning rate0.0002633744855967078, loss:3.028245449066162
batch i:142
learning rate0.0002633744855967078, loss:3.0441689491271973
batch i:143
learning rate0.0002633744855967078, loss:2.9349122047424316
batch i:144
learning rate0.0002633744855967078, loss:2.9706177711486816
batch i:145
learning rate0.0002633744855967078, loss:3.1083288192749023
batch i:146
learning rate0.0002633744855967078, loss:2.9755735397338867
batch i:147
learning rate0.0002633744855967078, loss:2.9642550945281982
batch i:148
learning rate0.0002633744855967078, loss:2.949925422668457
batch i:149
learning rate0.0002633744855967078, loss:3.0089688301086426
batch i:150
learning rate0.0002633744855967078, loss:2.9243712425231934
current self-play batch: 150
num_playouts:1000, win: 9, lose: 1, tie:0
average time: 246.6358521938324
batch i:151
learning rate0.0002633744855967078, loss:3.0011277198791504
batch i:152
learning rate0.0002633744855967078, loss:3.053370952606201
batch i:153
learning rate0.0002633744855967078, loss:2.9812660217285156
batch i:154
learning rate0.0002633744855967078, loss:3.020037889480591
batch i:155
learning rate0.0002633744855967078, loss:2.9469025135040283
batch i:156
learning rate0.0002633744855967078, loss:2.976370096206665
batch i:157
learning rate0.0002633744855967078, loss:2.9933485984802246
batch i:158
learning rate0.0002633744855967078, loss:3.048283815383911
batch i:159
learning rate0.0002633744855967078, loss:3.097111701965332
batch i:160
learning rate0.0002633744855967078, loss:2.949047565460205
batch i:161
learning rate0.0002633744855967078, loss:3.0554733276367188
batch i:162
learning rate0.0002633744855967078, loss:2.9729437828063965
batch i:163
learning rate0.0002633744855967078, loss:3.0700345039367676
batch i:164
learning rate0.0002633744855967078, loss:2.985321521759033
batch i:165
learning rate0.0002633744855967078, loss:2.9804506301879883
batch i:166
learning rate0.0002633744855967078, loss:3.0947866439819336
batch i:167
learning rate0.0002633744855967078, loss:3.0029094219207764
batch i:168
learning rate0.0002633744855967078, loss:3.0376226902008057
batch i:169
learning rate0.0002633744855967078, loss:3.030277729034424
batch i:170
learning rate0.0002633744855967078, loss:3.0108063220977783
batch i:171
learning rate0.0002633744855967078, loss:2.9833104610443115
batch i:172
learning rate0.0002633744855967078, loss:2.994378089904785
batch i:173
learning rate0.0002633744855967078, loss:3.0014090538024902
batch i:174
learning rate0.0002633744855967078, loss:3.064262866973877
batch i:175
learning rate0.0002633744855967078, loss:2.984267234802246
batch i:176
learning rate0.0002633744855967078, loss:2.935976266860962
batch i:177
learning rate0.0002633744855967078, loss:2.965902805328369
batch i:178
learning rate0.0002633744855967078, loss:2.9721803665161133
batch i:179
learning rate0.0002633744855967078, loss:2.9859023094177246
batch i:180
learning rate0.0002633744855967078, loss:2.9612247943878174
batch i:181
learning rate0.0002633744855967078, loss:3.010903835296631
batch i:182
learning rate0.0002633744855967078, loss:3.032607078552246
batch i:183
learning rate0.0002633744855967078, loss:2.9373512268066406
batch i:184
learning rate0.0002633744855967078, loss:2.9294612407684326
batch i:185
learning rate0.0002633744855967078, loss:2.9405040740966797
batch i:186
learning rate0.0002633744855967078, loss:2.994163751602173
batch i:187
learning rate0.0002633744855967078, loss:2.982849597930908
batch i:188
learning rate0.0002633744855967078, loss:2.920457363128662
batch i:189
learning rate0.0002633744855967078, loss:2.9397785663604736
batch i:190
learning rate0.0002633744855967078, loss:2.973299503326416
batch i:191
learning rate0.0002633744855967078, loss:2.9702720642089844
batch i:192
learning rate0.0002633744855967078, loss:2.9133291244506836
batch i:193
learning rate0.0002633744855967078, loss:2.9568982124328613
batch i:194
learning rate0.0002633744855967078, loss:2.9597764015197754
batch i:195
learning rate0.0002633744855967078, loss:2.953824043273926
batch i:196
learning rate0.0002633744855967078, loss:2.9871625900268555
batch i:197
learning rate0.0002633744855967078, loss:2.999229669570923
batch i:198
learning rate0.0002633744855967078, loss:2.8957982063293457
batch i:199
learning rate0.0002633744855967078, loss:2.9555110931396484
batch i:200
learning rate0.0002633744855967078, loss:2.9842326641082764
current self-play batch: 200
Traceback (most recent call last):
  File "/mnt/nas/home/huangyixin/AI/train.py", line 378, in <module>
    training_pipeline.train(game_batch_num)
  File "/mnt/nas/home/huangyixin/AI/train.py", line 303, in train
    win_ratio = self.game.policy_compete(filename, filename_best,
  File "/mnt/nas/home/huangyixin/AI/game.py", line 448, in policy_compete
    is_shown=0)
  File "/mnt/nas/home/huangyixin/AI/game.py", line 343, in start_play
    self.graphic(self.board, player1.player, player2.player)
  File "/mnt/nas/home/huangyixin/AI/game.py", line 73, in move_to_location
    h = move // self.width
TypeError: unsupported operand type(s) for //: 'tuple' and 'int'
