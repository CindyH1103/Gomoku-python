Start time: 2024-01-06 13:49:07.156638
priority replay buffer is in use
batch i:1
batch i:2
batch i:3
batch i:4
learning rate: 0.0013333333333333333, loss: 2.1760616302490234
batch i:5
learning rate: 0.0008888888888888888, loss: 1.6459596157073975
batch i:6
learning rate: 0.0005925925925925926, loss: 1.757411003112793
batch i:7
learning rate: 0.0003950617283950617, loss: 1.7392873764038086
batch i:8
learning rate: 0.0002633744855967078, loss: 1.5838876962661743
batch i:9
learning rate: 0.0002633744855967078, loss: 1.8164927959442139
batch i:10
learning rate: 0.0002633744855967078, loss: 1.8596625328063965
batch i:11
learning rate: 0.0002633744855967078, loss: 1.8497874736785889
batch i:12
learning rate: 0.0002633744855967078, loss: 1.9770804643630981
batch i:13
learning rate: 0.0002633744855967078, loss: 1.780074119567871
batch i:14
learning rate: 0.0002633744855967078, loss: 1.7313790321350098
batch i:15
learning rate: 0.0002633744855967078, loss: 1.9061225652694702
batch i:16
learning rate: 0.0002633744855967078, loss: 1.619448184967041
batch i:17
Traceback (most recent call last):
  File "/mnt/nas/home/huangyixin/AI/train.py", line 402, in <module>
    training_pipeline.train(game_batch_num)
  File "/mnt/nas/home/huangyixin/AI/train.py", line 304, in train
    self.policy_update()
  File "/mnt/nas/home/huangyixin/AI/train.py", line 268, in policy_update
    old_probs, _, _ = self.net.eval_state(state_batch)
  File "/mnt/nas/home/huangyixin/AI/model.py", line 296, in eval_state
    policy_logits, value_logits, value = self.net(state_batch)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/nas/home/huangyixin/AI/model.py", line 109, in forward
    x = self.res_blocks(x)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/nas/home/huangyixin/AI/model.py", line 58, in forward
    ret = self.conv_block_relu(x)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/activation.py", line 98, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/functional.py", line 1457, in relu
    result = torch.relu(input)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 10.76 GiB total capacity; 9.31 GiB already allocated; 63.44 MiB free; 9.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
