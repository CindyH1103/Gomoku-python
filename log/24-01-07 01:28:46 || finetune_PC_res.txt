Start time: 2024-01-07 01:28:46.454993
priority replay buffer is in use
batch i:1
batch i:2
batch i:3
learning rate: 0.0013333333333333333, loss: 2.576181650161743
batch i:4
learning rate: 0.0008888888888888888, loss: 2.5359420776367188
batch i:5
learning rate: 0.0005925925925925926, loss: 2.5610930919647217
batch i:6
learning rate: 0.0003950617283950617, loss: 2.4856090545654297
batch i:7
learning rate: 0.0002633744855967078, loss: 2.6077322959899902
batch i:8
learning rate: 0.0002633744855967078, loss: 2.396066665649414
batch i:9
learning rate: 0.0002633744855967078, loss: 2.3438327312469482
batch i:10
learning rate: 0.0002633744855967078, loss: 2.2673773765563965
current self-play batch: 10
num_playouts:1000, win: 10, lose: 0, tie:0
average time: 305.4471043109894
New best policy from pure MCTS
batch i:11
learning rate: 0.00039506172839506165, loss: 2.3193531036376953
batch i:12
learning rate: 0.00039506172839506165, loss: 2.254117012023926
batch i:13
learning rate: 0.00039506172839506165, loss: 2.302248477935791
batch i:14
learning rate: 0.00039506172839506165, loss: 2.229079008102417
batch i:15
learning rate: 0.00039506172839506165, loss: 2.318916082382202
batch i:16
learning rate: 0.00039506172839506165, loss: 2.3500900268554688
batch i:17
learning rate: 0.00039506172839506165, loss: 2.150346040725708
batch i:18
learning rate: 0.00039506172839506165, loss: 2.3157849311828613
batch i:19
Traceback (most recent call last):
  File "/mnt/nas/home/huangyixin/AI/train.py", line 402, in <module>
    training_pipeline.train(game_batch_num)
  File "/mnt/nas/home/huangyixin/AI/train.py", line 304, in train
    self.policy_update()
  File "/mnt/nas/home/huangyixin/AI/train.py", line 268, in policy_update
    old_probs, _, _ = self.net.eval_state(state_batch)
  File "/mnt/nas/home/huangyixin/AI/model.py", line 296, in eval_state
    policy_logits, value_logits, value = self.net(state_batch)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/nas/home/huangyixin/AI/model.py", line 109, in forward
    x = self.res_blocks(x)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/nas/home/huangyixin/AI/model.py", line 58, in forward
    ret = self.conv_block_relu(x)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/activation.py", line 98, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/functional.py", line 1457, in relu
    result = torch.relu(input)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 10.76 GiB total capacity; 9.20 GiB already allocated; 85.44 MiB free; 9.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
