Start time: 2023-12-22 20:40:32.129315
batch i:1
batch i:2
batch i:3
batch i:4
batch i:5
learning rate0.0013333333333333333, loss:3.7563157081604004
batch i:6
learning rate0.0013333333333333333, loss:3.6081857681274414
batch i:7
learning rate0.0013333333333333333, loss:3.5065689086914062
batch i:8
learning rate0.0013333333333333333, loss:3.4319393634796143
batch i:9
learning rate0.0013333333333333333, loss:3.2601146697998047
batch i:10
learning rate0.0013333333333333333, loss:3.1387648582458496
batch i:11
learning rate0.0013333333333333333, loss:3.2740402221679688
batch i:12
learning rate0.0013333333333333333, loss:3.193504810333252
batch i:13
learning rate0.0013333333333333333, loss:3.144371509552002
batch i:14
learning rate0.0013333333333333333, loss:3.190946578979492
batch i:15
learning rate0.0013333333333333333, loss:3.102501392364502
batch i:16
learning rate0.0013333333333333333, loss:3.1490020751953125
batch i:17
learning rate0.0013333333333333333, loss:3.130805730819702
batch i:18
learning rate0.0013333333333333333, loss:3.135754108428955
batch i:19
learning rate0.0013333333333333333, loss:3.0529751777648926
batch i:20
learning rate0.0013333333333333333, loss:3.1274337768554688
batch i:21
learning rate0.0013333333333333333, loss:3.0496675968170166
batch i:22
learning rate0.0013333333333333333, loss:3.1096463203430176
batch i:23
learning rate0.0013333333333333333, loss:3.2075562477111816
batch i:24
learning rate0.0013333333333333333, loss:3.115241050720215
batch i:25
learning rate0.0013333333333333333, loss:3.1395351886749268
batch i:26
learning rate0.0013333333333333333, loss:3.119382381439209
batch i:27
learning rate0.0013333333333333333, loss:3.045327663421631
batch i:28
learning rate0.0013333333333333333, loss:3.1100101470947266
batch i:29
learning rate0.0013333333333333333, loss:3.084718942642212
batch i:30
learning rate0.0013333333333333333, loss:3.1239006519317627
batch i:31
learning rate0.0013333333333333333, loss:3.123602867126465
batch i:32
learning rate0.0013333333333333333, loss:3.1632895469665527
batch i:33
learning rate0.0013333333333333333, loss:3.063204288482666
batch i:34
learning rate0.0013333333333333333, loss:3.125568389892578
batch i:35
learning rate0.0013333333333333333, loss:3.1041817665100098
batch i:36
learning rate0.0013333333333333333, loss:3.1706933975219727
batch i:37
learning rate0.0013333333333333333, loss:3.044706344604492
batch i:38
learning rate0.0013333333333333333, loss:3.0704753398895264
batch i:39
learning rate0.0013333333333333333, loss:3.0655505657196045
batch i:40
learning rate0.0013333333333333333, loss:3.069406509399414
batch i:41
learning rate0.0013333333333333333, loss:3.07623028755188
batch i:42
learning rate0.0013333333333333333, loss:3.092001438140869
batch i:43
learning rate0.0013333333333333333, loss:3.0299835205078125
batch i:44
learning rate0.0013333333333333333, loss:3.0968804359436035
batch i:45
learning rate0.0013333333333333333, loss:3.0640859603881836
batch i:46
learning rate0.0013333333333333333, loss:3.096595287322998
batch i:47
learning rate0.0013333333333333333, loss:3.101503849029541
batch i:48
learning rate0.0013333333333333333, loss:3.1187644004821777
batch i:49
learning rate0.0013333333333333333, loss:3.03546142578125
batch i:50
learning rate0.0013333333333333333, loss:3.0585618019104004
current self-play batch: 50
num_playouts:1000, win: 9, lose: 1, tie:0
average time: 186.57119529247285
Traceback (most recent call last):
  File "/mnt/nas/home/huangyixin/AI/train.py", line 378, in <module>
    training_pipeline.train(game_batch_num)
  File "/mnt/nas/home/huangyixin/AI/train.py", line 294, in train
    if win_ratio > self.best_win_ratio:
TypeError: '>' not supported between instances of 'tuple' and 'float'
