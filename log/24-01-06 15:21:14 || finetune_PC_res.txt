Start time: 2024-01-06 15:21:14.240066
priority replay buffer is in use
batch i:1
batch i:2
batch i:3
batch i:4
learning rate: 0.0013333333333333333, loss: 2.1760616302490234
batch i:5
learning rate: 0.0008888888888888888, loss: 1.6459596157073975
batch i:6
learning rate: 0.0005925925925925926, loss: 1.757411003112793
batch i:7
learning rate: 0.0003950617283950617, loss: 1.739434838294983
batch i:8
learning rate: 0.0002633744855967078, loss: 1.8794158697128296
batch i:9
learning rate: 0.0002633744855967078, loss: 1.7068513631820679
batch i:10
learning rate: 0.0002633744855967078, loss: 1.7195276021957397
batch i:11
learning rate: 0.0002633744855967078, loss: 1.4709118604660034
batch i:12
learning rate: 0.0002633744855967078, loss: 1.7826426029205322
batch i:13
learning rate: 0.0002633744855967078, loss: 1.8186447620391846
batch i:14
learning rate: 0.0002633744855967078, loss: 1.9140456914901733
batch i:15
learning rate: 0.0002633744855967078, loss: 1.7944563627243042
batch i:16
learning rate: 0.0002633744855967078, loss: 1.7821913957595825
batch i:17
learning rate: 0.0002633744855967078, loss: 1.6958624124526978
batch i:18
learning rate: 0.0002633744855967078, loss: 1.5457067489624023
batch i:19
learning rate: 0.0002633744855967078, loss: 1.712644338607788
batch i:20
learning rate: 0.00039506172839506165, loss: 1.5491626262664795
current self-play batch: 20
num_playouts:1000, win: 10, lose: 0, tie:0
average time: 278.0762529373169
New best policy from pure MCTS
batch i:21
Traceback (most recent call last):
  File "/mnt/nas/home/huangyixin/AI/train.py", line 402, in <module>
    training_pipeline.train(game_batch_num)
  File "/mnt/nas/home/huangyixin/AI/train.py", line 304, in train
    self.policy_update()
  File "/mnt/nas/home/huangyixin/AI/train.py", line 268, in policy_update
    old_probs, _, _ = self.net.eval_state(state_batch)
  File "/mnt/nas/home/huangyixin/AI/model.py", line 296, in eval_state
    policy_logits, value_logits, value = self.net(state_batch)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/nas/home/huangyixin/AI/model.py", line 109, in forward
    x = self.res_blocks(x)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/nas/home/huangyixin/AI/model.py", line 58, in forward
    ret = self.conv_block_relu(x)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/modules/activation.py", line 98, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/huangyixin/anaconda3/envs/gomoku/lib/python3.9/site-packages/torch/nn/functional.py", line 1457, in relu
    result = torch.relu(input)
RuntimeError: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 10.76 GiB total capacity; 9.40 GiB already allocated; 51.44 MiB free; 9.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
