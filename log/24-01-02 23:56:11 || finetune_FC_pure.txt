Start time: 2024-01-02 23:56:11.099971
batch i:1
batch i:2
batch i:3
batch i:4
batch i:5
batch i:6
batch i:7
batch i:8
batch i:9
batch i:10
learning rate0.0013333333333333333, loss:3.266066312789917
batch i:11
learning rate0.0008888888888888888, loss:3.1767899990081787
batch i:12
learning rate0.0008888888888888888, loss:2.981429100036621
batch i:13
learning rate0.0008888888888888888, loss:3.0662145614624023
batch i:14
learning rate0.0005925925925925926, loss:3.01725435256958
batch i:15
learning rate0.0005925925925925926, loss:2.801795482635498
batch i:16
learning rate0.0005925925925925926, loss:2.7669894695281982
batch i:17
learning rate0.0005925925925925926, loss:2.69191837310791
batch i:18
learning rate0.0008888888888888889, loss:2.664581298828125
batch i:19
learning rate0.0008888888888888889, loss:2.624150276184082
batch i:20
learning rate0.0008888888888888889, loss:2.589794874191284
batch i:21
learning rate0.0008888888888888889, loss:2.636884927749634
batch i:22
learning rate0.0008888888888888889, loss:2.633600950241089
batch i:23
learning rate0.0008888888888888889, loss:2.58634614944458
batch i:24
learning rate0.0008888888888888889, loss:2.566716194152832
batch i:25
learning rate0.0008888888888888889, loss:2.6113152503967285
batch i:26
learning rate0.0013333333333333335, loss:2.6236703395843506
batch i:27
learning rate0.0013333333333333335, loss:2.596099853515625
batch i:28
learning rate0.0013333333333333335, loss:2.6164238452911377
batch i:29
learning rate0.0013333333333333335, loss:2.594148635864258
batch i:30
learning rate0.0013333333333333335, loss:2.423100709915161
batch i:31
learning rate0.0013333333333333335, loss:2.4920926094055176
batch i:32
learning rate0.0013333333333333335, loss:2.5944762229919434
batch i:33
learning rate0.0013333333333333335, loss:2.4048686027526855
batch i:34
learning rate0.0013333333333333335, loss:2.4141926765441895
batch i:35
learning rate0.0013333333333333335, loss:2.454500675201416
batch i:36
learning rate0.0013333333333333335, loss:2.481400489807129
batch i:37
learning rate0.0013333333333333335, loss:2.408557653427124
batch i:38
learning rate0.0013333333333333335, loss:2.5605788230895996
batch i:39
learning rate0.0013333333333333335, loss:2.4156341552734375
batch i:40
learning rate0.0013333333333333335, loss:2.391360282897949
batch i:41
learning rate0.0013333333333333335, loss:2.3793444633483887
batch i:42
learning rate0.0013333333333333335, loss:2.379833221435547
batch i:43
learning rate0.0013333333333333335, loss:2.428981304168701
batch i:44
learning rate0.0013333333333333335, loss:2.4726340770721436
batch i:45
learning rate0.0013333333333333335, loss:2.4820261001586914
batch i:46
learning rate0.0013333333333333335, loss:2.431089162826538
batch i:47
learning rate0.0013333333333333335, loss:2.329622745513916
batch i:48
learning rate0.0013333333333333335, loss:2.380476951599121
batch i:49
learning rate0.0013333333333333335, loss:2.34157133102417
batch i:50
learning rate0.0013333333333333335, loss:2.4375905990600586
batch i:51
learning rate0.0013333333333333335, loss:2.416456699371338
batch i:52
learning rate0.0013333333333333335, loss:2.480698823928833
batch i:53
learning rate0.0013333333333333335, loss:2.486311197280884
batch i:54
learning rate0.0013333333333333335, loss:2.5201029777526855
batch i:55
learning rate0.0013333333333333335, loss:2.399043560028076
batch i:56
learning rate0.0013333333333333335, loss:2.396548271179199
batch i:57
learning rate0.0013333333333333335, loss:2.464298725128174
batch i:58
learning rate0.0013333333333333335, loss:2.4494693279266357
batch i:59
learning rate0.0013333333333333335, loss:2.4689793586730957
batch i:60
learning rate0.0013333333333333335, loss:2.4154138565063477
batch i:61
learning rate0.0013333333333333335, loss:2.428945779800415
batch i:62
learning rate0.0013333333333333335, loss:2.4120447635650635
batch i:63
learning rate0.0013333333333333335, loss:2.4748237133026123
batch i:64
learning rate0.0013333333333333335, loss:2.4217512607574463
batch i:65
learning rate0.0013333333333333335, loss:2.4610631465911865
batch i:66
learning rate0.0013333333333333335, loss:2.4691097736358643
batch i:67
learning rate0.0013333333333333335, loss:2.5724234580993652
batch i:68
learning rate0.0013333333333333335, loss:2.4319777488708496
batch i:69
learning rate0.0013333333333333335, loss:2.498065948486328
batch i:70
learning rate0.0013333333333333335, loss:2.5079190731048584
batch i:71
learning rate0.0013333333333333335, loss:2.5521860122680664
batch i:72
learning rate0.0013333333333333335, loss:2.5488462448120117
batch i:73
learning rate0.0013333333333333335, loss:2.484766960144043
batch i:74
learning rate0.0013333333333333335, loss:2.559239625930786
batch i:75
learning rate0.0013333333333333335, loss:2.534691333770752
batch i:76
learning rate0.0013333333333333335, loss:2.5070717334747314
batch i:77
learning rate0.0013333333333333335, loss:2.510509967803955
batch i:78
learning rate0.0013333333333333335, loss:2.484762191772461
batch i:79
learning rate0.0013333333333333335, loss:2.484783172607422
batch i:80
learning rate0.0013333333333333335, loss:2.5145561695098877
batch i:81
learning rate0.0013333333333333335, loss:2.4904472827911377
batch i:82
learning rate0.0013333333333333335, loss:2.518679141998291
batch i:83
learning rate0.0013333333333333335, loss:2.5228006839752197
batch i:84
learning rate0.0013333333333333335, loss:2.513795852661133
batch i:85
learning rate0.0013333333333333335, loss:2.4089651107788086
batch i:86
learning rate0.0013333333333333335, loss:2.6255290508270264
batch i:87
learning rate0.0013333333333333335, loss:2.571619987487793
batch i:88
learning rate0.0013333333333333335, loss:2.443486213684082
batch i:89
learning rate0.0013333333333333335, loss:2.528968095779419
batch i:90
learning rate0.0013333333333333335, loss:2.4608116149902344
batch i:91
learning rate0.0013333333333333335, loss:2.562582492828369
batch i:92
learning rate0.0013333333333333335, loss:2.557640552520752
batch i:93
learning rate0.0013333333333333335, loss:2.538435220718384
batch i:94
learning rate0.0013333333333333335, loss:2.5632925033569336
batch i:95
learning rate0.0013333333333333335, loss:2.5939507484436035
batch i:96
learning rate0.0013333333333333335, loss:2.5479822158813477
batch i:97
learning rate0.0013333333333333335, loss:2.5476152896881104
batch i:98
learning rate0.0013333333333333335, loss:2.505324602127075
batch i:99
learning rate0.0013333333333333335, loss:2.5855047702789307
batch i:100
learning rate0.0013333333333333335, loss:2.6383285522460938
batch i:101
learning rate0.0013333333333333335, loss:2.4769136905670166
batch i:102
learning rate0.0013333333333333335, loss:2.5500264167785645
batch i:103
learning rate0.0013333333333333335, loss:2.5934367179870605
batch i:104
learning rate0.0013333333333333335, loss:2.665343761444092
batch i:105
learning rate0.0013333333333333335, loss:2.7168803215026855
batch i:106
learning rate0.0013333333333333335, loss:2.692502975463867
batch i:107
learning rate0.0013333333333333335, loss:2.6476547718048096
batch i:108
learning rate0.0013333333333333335, loss:2.485801935195923
batch i:109
learning rate0.0013333333333333335, loss:2.6165757179260254
batch i:110
learning rate0.0013333333333333335, loss:2.583029270172119
batch i:111
learning rate0.0013333333333333335, loss:2.680482864379883
batch i:112
learning rate0.0013333333333333335, loss:2.705934524536133
batch i:113
learning rate0.0013333333333333335, loss:2.6721718311309814
batch i:114
learning rate0.0013333333333333335, loss:2.6270627975463867
batch i:115
learning rate0.0013333333333333335, loss:2.6600728034973145
batch i:116
learning rate0.0013333333333333335, loss:2.7226014137268066
batch i:117
learning rate0.0013333333333333335, loss:2.7160468101501465
batch i:118
learning rate0.0013333333333333335, loss:2.7219882011413574
batch i:119
learning rate0.0013333333333333335, loss:2.6112098693847656
batch i:120
learning rate0.0013333333333333335, loss:2.612055540084839
batch i:121
learning rate0.0013333333333333335, loss:2.681847333908081
batch i:122
learning rate0.0013333333333333335, loss:2.7129878997802734
batch i:123
learning rate0.0013333333333333335, loss:2.7185354232788086
batch i:124
learning rate0.0013333333333333335, loss:2.6893880367279053
batch i:125
learning rate0.0013333333333333335, loss:2.661425828933716
batch i:126
learning rate0.0013333333333333335, loss:2.757962465286255
batch i:127
learning rate0.0013333333333333335, loss:2.622494697570801
batch i:128
learning rate0.0013333333333333335, loss:2.6393184661865234
batch i:129
learning rate0.0013333333333333335, loss:2.762369155883789
batch i:130
learning rate0.0013333333333333335, loss:2.7753353118896484
batch i:131
learning rate0.0013333333333333335, loss:2.7511677742004395
batch i:132
learning rate0.0013333333333333335, loss:2.6651501655578613
batch i:133
learning rate0.0013333333333333335, loss:2.635145664215088
batch i:134
learning rate0.0013333333333333335, loss:2.6643824577331543
batch i:135
learning rate0.0013333333333333335, loss:2.7373335361480713
batch i:136
learning rate0.0013333333333333335, loss:2.6778953075408936
batch i:137
learning rate0.0013333333333333335, loss:2.7507224082946777
batch i:138
learning rate0.0013333333333333335, loss:2.7673563957214355
batch i:139
learning rate0.0013333333333333335, loss:2.67340350151062
batch i:140
learning rate0.0013333333333333335, loss:2.636213779449463
batch i:141
learning rate0.0013333333333333335, loss:2.6808032989501953
batch i:142
learning rate0.0013333333333333335, loss:2.743439197540283
batch i:143
learning rate0.0013333333333333335, loss:2.6247925758361816
batch i:144
learning rate0.0013333333333333335, loss:2.5913219451904297
batch i:145
learning rate0.0013333333333333335, loss:2.5707907676696777
batch i:146
learning rate0.0013333333333333335, loss:2.6460790634155273
batch i:147
learning rate0.0013333333333333335, loss:2.6746678352355957
batch i:148
learning rate0.0013333333333333335, loss:2.647646903991699
batch i:149
learning rate0.0013333333333333335, loss:2.6444029808044434
batch i:150
learning rate0.0013333333333333335, loss:2.5641870498657227
batch i:151
learning rate0.0013333333333333335, loss:2.6816048622131348
batch i:152
learning rate0.0013333333333333335, loss:2.608121871948242
batch i:153
learning rate0.0013333333333333335, loss:2.6869778633117676
batch i:154
learning rate0.0013333333333333335, loss:2.6493468284606934
batch i:155
learning rate0.0013333333333333335, loss:2.6348090171813965
batch i:156
learning rate0.0013333333333333335, loss:2.6556179523468018
batch i:157
learning rate0.0013333333333333335, loss:2.7264411449432373
batch i:158
learning rate0.0013333333333333335, loss:2.7557358741760254
batch i:159
learning rate0.0013333333333333335, loss:2.792689561843872
batch i:160
learning rate0.0013333333333333335, loss:2.6982502937316895
batch i:161
learning rate0.0013333333333333335, loss:2.661968231201172
batch i:162
learning rate0.0013333333333333335, loss:2.6418135166168213
batch i:163
learning rate0.0013333333333333335, loss:2.728517532348633
batch i:164
learning rate0.0013333333333333335, loss:2.568936347961426
batch i:165
learning rate0.0013333333333333335, loss:2.6461894512176514
batch i:166
learning rate0.0013333333333333335, loss:2.6398534774780273
batch i:167
learning rate0.0013333333333333335, loss:2.619248867034912
batch i:168
learning rate0.0013333333333333335, loss:2.550088882446289
batch i:169
learning rate0.0013333333333333335, loss:2.770751476287842
batch i:170
learning rate0.0013333333333333335, loss:2.6437644958496094
batch i:171
learning rate0.0013333333333333335, loss:2.6796746253967285
batch i:172
learning rate0.0013333333333333335, loss:2.7236480712890625
batch i:173
learning rate0.0013333333333333335, loss:2.6542959213256836
batch i:174
learning rate0.0013333333333333335, loss:2.641829490661621
batch i:175
learning rate0.0013333333333333335, loss:2.647764205932617
batch i:176
learning rate0.0013333333333333335, loss:2.6494297981262207
batch i:177
learning rate0.0013333333333333335, loss:2.5958805084228516
batch i:178
learning rate0.0013333333333333335, loss:2.5791525840759277
batch i:179
learning rate0.0013333333333333335, loss:2.6777102947235107
batch i:180
learning rate0.0013333333333333335, loss:2.6763148307800293
batch i:181
learning rate0.0013333333333333335, loss:2.6138529777526855
batch i:182
learning rate0.0013333333333333335, loss:2.5986289978027344
batch i:183
learning rate0.0013333333333333335, loss:2.660038948059082
batch i:184
learning rate0.0013333333333333335, loss:2.665225028991699
batch i:185
learning rate0.0013333333333333335, loss:2.7294933795928955
batch i:186
learning rate0.0013333333333333335, loss:2.6017117500305176
batch i:187
learning rate0.0013333333333333335, loss:2.6554646492004395
batch i:188
learning rate0.0013333333333333335, loss:2.6333069801330566
batch i:189
learning rate0.0013333333333333335, loss:2.6769862174987793
batch i:190
learning rate0.0013333333333333335, loss:2.6995363235473633
batch i:191
learning rate0.0013333333333333335, loss:2.7303428649902344
batch i:192
learning rate0.0013333333333333335, loss:2.6841561794281006
batch i:193
learning rate0.0013333333333333335, loss:2.7723548412323
batch i:194
learning rate0.0013333333333333335, loss:2.711162567138672
batch i:195
learning rate0.0013333333333333335, loss:2.6343040466308594
batch i:196
learning rate0.0013333333333333335, loss:2.6859354972839355
batch i:197
learning rate0.0013333333333333335, loss:2.6754133701324463
batch i:198
learning rate0.0013333333333333335, loss:2.5765295028686523
batch i:199
learning rate0.0013333333333333335, loss:2.7421281337738037
batch i:200
learning rate0.0013333333333333335, loss:2.6853225231170654
batch i:201
learning rate0.0013333333333333335, loss:2.6139140129089355
batch i:202
learning rate0.0013333333333333335, loss:2.7046031951904297
batch i:203
learning rate0.0013333333333333335, loss:2.7506496906280518
batch i:204
learning rate0.0013333333333333335, loss:2.730498790740967
batch i:205
learning rate0.0013333333333333335, loss:2.646833896636963
batch i:206
learning rate0.0013333333333333335, loss:2.6964733600616455
batch i:207
learning rate0.0013333333333333335, loss:2.657322883605957
batch i:208
learning rate0.0013333333333333335, loss:2.746474266052246
batch i:209
learning rate0.0013333333333333335, loss:2.6842899322509766
batch i:210
learning rate0.0013333333333333335, loss:2.7758727073669434
batch i:211
learning rate0.0013333333333333335, loss:2.7068703174591064
batch i:212
learning rate0.0013333333333333335, loss:2.677706480026245
batch i:213
learning rate0.0013333333333333335, loss:2.678293466567993
batch i:214
learning rate0.0013333333333333335, loss:2.6702194213867188
batch i:215
learning rate0.0013333333333333335, loss:2.6458377838134766
batch i:216
learning rate0.0013333333333333335, loss:2.7010746002197266
batch i:217
learning rate0.0013333333333333335, loss:2.854154586791992
batch i:218
learning rate0.0013333333333333335, loss:2.753715991973877
batch i:219
learning rate0.0013333333333333335, loss:2.710814952850342
batch i:220
learning rate0.0013333333333333335, loss:2.765848159790039
batch i:221
learning rate0.0013333333333333335, loss:2.671903610229492
batch i:222
learning rate0.0013333333333333335, loss:2.7038588523864746
batch i:223
learning rate0.0013333333333333335, loss:2.7381811141967773
batch i:224
learning rate0.0013333333333333335, loss:2.7008216381073
batch i:225
learning rate0.0013333333333333335, loss:2.6734821796417236
batch i:226
learning rate0.0013333333333333335, loss:2.6801605224609375
batch i:227
learning rate0.0013333333333333335, loss:2.6444449424743652
batch i:228
learning rate0.0013333333333333335, loss:2.70680570602417
batch i:229
learning rate0.0013333333333333335, loss:2.7068963050842285
batch i:230
learning rate0.0013333333333333335, loss:2.7584376335144043
batch i:231
learning rate0.0013333333333333335, loss:2.752469301223755
batch i:232
learning rate0.0013333333333333335, loss:2.762381076812744
batch i:233
learning rate0.0013333333333333335, loss:2.755513906478882
batch i:234
learning rate0.0013333333333333335, loss:2.7604968547821045
batch i:235
learning rate0.0013333333333333335, loss:2.841348648071289
batch i:236
learning rate0.0013333333333333335, loss:2.803030014038086
batch i:237
learning rate0.0013333333333333335, loss:2.8562097549438477
batch i:238
learning rate0.0013333333333333335, loss:2.804786205291748
batch i:239
learning rate0.0013333333333333335, loss:2.7794604301452637
batch i:240
learning rate0.0013333333333333335, loss:2.822523593902588
batch i:241
learning rate0.0013333333333333335, loss:2.813257932662964
batch i:242
learning rate0.0013333333333333335, loss:2.768916606903076
batch i:243
learning rate0.0013333333333333335, loss:2.7469496726989746
batch i:244
learning rate0.0013333333333333335, loss:2.7806015014648438
batch i:245
learning rate0.0013333333333333335, loss:2.8078699111938477
batch i:246
learning rate0.0013333333333333335, loss:2.653554916381836
batch i:247
learning rate0.0013333333333333335, loss:2.778945207595825
batch i:248
learning rate0.0013333333333333335, loss:2.7994158267974854
batch i:249
learning rate0.0013333333333333335, loss:2.8397319316864014
batch i:250
learning rate0.0013333333333333335, loss:2.7539637088775635
current self-play batch: 250
num_playouts:1000, win: 9, lose: 1, tie:0
average time: 149.36197905540467
New best policy from pure MCTS
batch i:251
learning rate0.0013333333333333335, loss:2.8346362113952637
batch i:252
learning rate0.0013333333333333335, loss:2.793300151824951
batch i:253
learning rate0.0013333333333333335, loss:2.8303842544555664
batch i:254
learning rate0.0013333333333333335, loss:2.8448712825775146
batch i:255
learning rate0.0013333333333333335, loss:2.7909748554229736
batch i:256
learning rate0.0013333333333333335, loss:2.8332672119140625
batch i:257
learning rate0.0013333333333333335, loss:2.700725555419922
batch i:258
learning rate0.0013333333333333335, loss:2.686479330062866
batch i:259
learning rate0.0013333333333333335, loss:2.7678909301757812
batch i:260
learning rate0.0013333333333333335, loss:2.7478911876678467
batch i:261
learning rate0.0013333333333333335, loss:2.830777645111084
batch i:262
learning rate0.0013333333333333335, loss:2.6893391609191895
batch i:263
learning rate0.0013333333333333335, loss:2.8094725608825684
batch i:264
learning rate0.0013333333333333335, loss:2.833951950073242
batch i:265
learning rate0.0013333333333333335, loss:2.85434627532959
batch i:266
learning rate0.0013333333333333335, loss:2.804704189300537
batch i:267
learning rate0.0013333333333333335, loss:2.792733669281006
batch i:268
learning rate0.0013333333333333335, loss:2.789978265762329
batch i:269
learning rate0.0013333333333333335, loss:2.870195150375366
batch i:270
learning rate0.0013333333333333335, loss:2.859034299850464
batch i:271
learning rate0.0013333333333333335, loss:2.9254238605499268
batch i:272
learning rate0.0013333333333333335, loss:2.792757034301758
batch i:273
learning rate0.0013333333333333335, loss:2.8843352794647217
batch i:274
learning rate0.0013333333333333335, loss:2.8010315895080566
batch i:275
learning rate0.0013333333333333335, loss:2.765493392944336
batch i:276
learning rate0.0013333333333333335, loss:2.8043508529663086
batch i:277
learning rate0.0013333333333333335, loss:2.912163496017456
batch i:278
learning rate0.0013333333333333335, loss:2.813732385635376
batch i:279
learning rate0.0013333333333333335, loss:2.9339118003845215
batch i:280
learning rate0.0013333333333333335, loss:2.8241019248962402
batch i:281
learning rate0.0013333333333333335, loss:2.8007149696350098
batch i:282
learning rate0.0013333333333333335, loss:2.883821487426758
batch i:283
learning rate0.0013333333333333335, loss:2.884979009628296
batch i:284
learning rate0.0013333333333333335, loss:2.8256995677948
batch i:285
learning rate0.0013333333333333335, loss:2.829658031463623
batch i:286
learning rate0.0013333333333333335, loss:2.867398738861084
batch i:287
learning rate0.0013333333333333335, loss:2.9120473861694336
batch i:288
learning rate0.0013333333333333335, loss:2.899045944213867
batch i:289
learning rate0.0013333333333333335, loss:2.8743603229522705
batch i:290
learning rate0.0013333333333333335, loss:2.8537983894348145
batch i:291
learning rate0.0013333333333333335, loss:3.043246030807495
batch i:292
learning rate0.0013333333333333335, loss:2.884571075439453
batch i:293
learning rate0.0013333333333333335, loss:2.976552963256836
batch i:294
learning rate0.0013333333333333335, loss:2.8962936401367188
batch i:295
learning rate0.0013333333333333335, loss:2.870291233062744
batch i:296
learning rate0.0013333333333333335, loss:2.9649438858032227
batch i:297
learning rate0.0013333333333333335, loss:2.838499069213867
batch i:298
learning rate0.0013333333333333335, loss:2.9303653240203857
batch i:299
learning rate0.0013333333333333335, loss:2.930969476699829
batch i:300
learning rate0.0013333333333333335, loss:2.9886393547058105
batch i:301
learning rate0.0013333333333333335, loss:2.927523136138916
batch i:302
learning rate0.0013333333333333335, loss:3.03220796585083
batch i:303
learning rate0.0013333333333333335, loss:2.8998191356658936
batch i:304
learning rate0.0013333333333333335, loss:2.9253158569335938
batch i:305
learning rate0.0013333333333333335, loss:2.8952879905700684
batch i:306
learning rate0.0013333333333333335, loss:2.908623456954956
batch i:307
learning rate0.0013333333333333335, loss:3.0328528881073
batch i:308
learning rate0.0013333333333333335, loss:2.936452865600586
batch i:309
learning rate0.0013333333333333335, loss:2.865912675857544
batch i:310
learning rate0.0013333333333333335, loss:2.881875991821289
batch i:311
learning rate0.0013333333333333335, loss:2.8914201259613037
batch i:312
learning rate0.0013333333333333335, loss:2.93682599067688
batch i:313
learning rate0.0013333333333333335, loss:2.8844919204711914
batch i:314
learning rate0.0013333333333333335, loss:2.8682658672332764
batch i:315
learning rate0.0013333333333333335, loss:2.9048500061035156
batch i:316
learning rate0.0013333333333333335, loss:2.9489457607269287
batch i:317
learning rate0.0013333333333333335, loss:2.9164700508117676
batch i:318
learning rate0.0013333333333333335, loss:2.8834011554718018
batch i:319
learning rate0.0013333333333333335, loss:2.863034248352051
batch i:320
learning rate0.0013333333333333335, loss:2.9258792400360107
batch i:321
learning rate0.0013333333333333335, loss:2.907676935195923
batch i:322
learning rate0.0013333333333333335, loss:2.9047839641571045
batch i:323
learning rate0.0013333333333333335, loss:2.9520809650421143
batch i:324
learning rate0.0013333333333333335, loss:2.8665947914123535
batch i:325
learning rate0.0013333333333333335, loss:2.8189196586608887
batch i:326
learning rate0.0013333333333333335, loss:2.885331869125366
batch i:327
learning rate0.0013333333333333335, loss:2.9580020904541016
batch i:328
learning rate0.0013333333333333335, loss:2.8641905784606934
batch i:329
learning rate0.0013333333333333335, loss:2.959094524383545
batch i:330
learning rate0.0013333333333333335, loss:2.9028098583221436
batch i:331
learning rate0.0013333333333333335, loss:2.8452939987182617
batch i:332
learning rate0.0013333333333333335, loss:2.836793899536133
batch i:333
learning rate0.0013333333333333335, loss:2.875338077545166
batch i:334
learning rate0.0013333333333333335, loss:2.961362838745117
batch i:335
learning rate0.0013333333333333335, loss:2.91463041305542
batch i:336
learning rate0.0013333333333333335, loss:3.030540704727173
batch i:337
learning rate0.0013333333333333335, loss:2.648655414581299
batch i:338
learning rate0.0013333333333333335, loss:2.9170875549316406
batch i:339
learning rate0.0013333333333333335, loss:2.793726682662964
batch i:340
learning rate0.0013333333333333335, loss:2.7755918502807617
batch i:341
learning rate0.0013333333333333335, loss:2.847813129425049
batch i:342
learning rate0.0013333333333333335, loss:2.783080577850342
batch i:343
learning rate0.0013333333333333335, loss:2.815200090408325
batch i:344
learning rate0.0013333333333333335, loss:2.8220396041870117
batch i:345
learning rate0.0013333333333333335, loss:2.779613971710205
batch i:346
learning rate0.0013333333333333335, loss:2.8663253784179688
batch i:347
learning rate0.0013333333333333335, loss:2.876295804977417
batch i:348
learning rate0.0013333333333333335, loss:2.875838279724121
batch i:349
learning rate0.0013333333333333335, loss:2.8386025428771973
batch i:350
learning rate0.0013333333333333335, loss:2.912766933441162
batch i:351
learning rate0.0013333333333333335, loss:2.8026223182678223
batch i:352
learning rate0.0013333333333333335, loss:2.87809681892395
batch i:353
learning rate0.0013333333333333335, loss:2.7929012775421143
batch i:354
learning rate0.0013333333333333335, loss:2.8707525730133057
batch i:355
learning rate0.0013333333333333335, loss:2.8177645206451416
batch i:356
learning rate0.0013333333333333335, loss:2.9268572330474854
batch i:357
learning rate0.0013333333333333335, loss:2.8445513248443604
batch i:358
learning rate0.0013333333333333335, loss:2.8443799018859863
batch i:359
learning rate0.0013333333333333335, loss:2.7765398025512695
batch i:360
learning rate0.0013333333333333335, loss:2.8685896396636963
batch i:361
learning rate0.0013333333333333335, loss:2.888655185699463
batch i:362
learning rate0.0013333333333333335, loss:2.9332804679870605
batch i:363
learning rate0.0013333333333333335, loss:2.928703784942627
batch i:364
learning rate0.0013333333333333335, loss:2.9108622074127197
batch i:365
learning rate0.0013333333333333335, loss:2.935734272003174
batch i:366
learning rate0.0013333333333333335, loss:2.8827033042907715
batch i:367
learning rate0.0013333333333333335, loss:2.9155478477478027
batch i:368
learning rate0.0013333333333333335, loss:2.9314146041870117
batch i:369
learning rate0.0013333333333333335, loss:2.84026837348938
batch i:370
learning rate0.0013333333333333335, loss:2.818470001220703
batch i:371
learning rate0.0013333333333333335, loss:2.8642683029174805
batch i:372
learning rate0.0013333333333333335, loss:2.917116641998291
batch i:373
learning rate0.0013333333333333335, loss:2.869032382965088
batch i:374
learning rate0.0013333333333333335, loss:2.8305444717407227
batch i:375
learning rate0.0013333333333333335, loss:2.878203868865967
batch i:376
learning rate0.0013333333333333335, loss:2.8796560764312744
batch i:377
learning rate0.0013333333333333335, loss:2.8996121883392334
batch i:378
learning rate0.0013333333333333335, loss:2.8718035221099854
batch i:379
learning rate0.0013333333333333335, loss:2.9732472896575928
batch i:380
learning rate0.0013333333333333335, loss:2.9028711318969727
batch i:381
learning rate0.0013333333333333335, loss:2.826737403869629
batch i:382
learning rate0.0013333333333333335, loss:2.8651034832000732
batch i:383
learning rate0.0013333333333333335, loss:2.7719407081604004
batch i:384
learning rate0.0013333333333333335, loss:2.899746894836426
batch i:385
learning rate0.0013333333333333335, loss:2.788722515106201
batch i:386
learning rate0.0013333333333333335, loss:2.780365467071533
batch i:387
learning rate0.0013333333333333335, loss:2.8755059242248535
batch i:388
learning rate0.0013333333333333335, loss:2.82863187789917
batch i:389
learning rate0.0013333333333333335, loss:2.8453726768493652
batch i:390
learning rate0.0013333333333333335, loss:2.995354413986206
batch i:391
learning rate0.0013333333333333335, loss:2.906479835510254
batch i:392
learning rate0.0013333333333333335, loss:2.8096299171447754
batch i:393
learning rate0.0013333333333333335, loss:2.98358154296875
batch i:394
learning rate0.0013333333333333335, loss:2.791841745376587
batch i:395
learning rate0.0013333333333333335, loss:2.8636906147003174
batch i:396
learning rate0.0013333333333333335, loss:2.8736236095428467
batch i:397
learning rate0.0013333333333333335, loss:2.7990410327911377
batch i:398
learning rate0.0013333333333333335, loss:2.901553153991699
batch i:399
learning rate0.0013333333333333335, loss:2.8618245124816895
batch i:400
learning rate0.0013333333333333335, loss:2.7778263092041016
batch i:401
learning rate0.0013333333333333335, loss:2.7764673233032227
batch i:402
learning rate0.0013333333333333335, loss:2.7613015174865723
batch i:403
learning rate0.0013333333333333335, loss:2.8056249618530273
batch i:404
learning rate0.0013333333333333335, loss:2.8636224269866943
batch i:405
learning rate0.0013333333333333335, loss:2.842637062072754
batch i:406
learning rate0.0013333333333333335, loss:2.84206223487854
batch i:407
learning rate0.0013333333333333335, loss:2.777097702026367
batch i:408
learning rate0.0013333333333333335, loss:2.796853542327881
batch i:409
learning rate0.0013333333333333335, loss:2.8278541564941406
batch i:410
learning rate0.0013333333333333335, loss:2.8115172386169434
batch i:411
learning rate0.0013333333333333335, loss:2.7613179683685303
batch i:412
learning rate0.0013333333333333335, loss:2.708491325378418
batch i:413
learning rate0.0013333333333333335, loss:2.8695690631866455
batch i:414
learning rate0.0013333333333333335, loss:2.774182081222534
batch i:415
learning rate0.0013333333333333335, loss:2.7412755489349365
batch i:416
learning rate0.0013333333333333335, loss:2.818251848220825
batch i:417
learning rate0.0013333333333333335, loss:2.806272506713867
batch i:418
learning rate0.0013333333333333335, loss:2.8188443183898926
batch i:419
learning rate0.0013333333333333335, loss:2.7615840435028076
batch i:420
learning rate0.0013333333333333335, loss:2.7577497959136963
batch i:421
learning rate0.0013333333333333335, loss:2.786825180053711
batch i:422
learning rate0.0013333333333333335, loss:2.7885727882385254
batch i:423
learning rate0.0013333333333333335, loss:2.839627265930176
batch i:424
learning rate0.0013333333333333335, loss:2.7799837589263916
batch i:425
learning rate0.0013333333333333335, loss:2.7174346446990967
batch i:426
learning rate0.0013333333333333335, loss:2.7518954277038574
batch i:427
learning rate0.0013333333333333335, loss:2.770825147628784
batch i:428
learning rate0.0013333333333333335, loss:2.697648763656616
batch i:429
learning rate0.0013333333333333335, loss:2.7125046253204346
batch i:430
learning rate0.0013333333333333335, loss:2.7250537872314453
batch i:431
learning rate0.0013333333333333335, loss:2.7697339057922363
batch i:432
learning rate0.0013333333333333335, loss:2.639596462249756
batch i:433
learning rate0.0013333333333333335, loss:2.7434070110321045
batch i:434
learning rate0.0013333333333333335, loss:2.683009147644043
batch i:435
learning rate0.0013333333333333335, loss:2.7037694454193115
batch i:436
learning rate0.0013333333333333335, loss:2.696162223815918
batch i:437
learning rate0.0013333333333333335, loss:2.728011131286621
batch i:438
learning rate0.0013333333333333335, loss:2.7875542640686035
batch i:439
learning rate0.0013333333333333335, loss:2.759253978729248
batch i:440
learning rate0.0013333333333333335, loss:2.7161598205566406
batch i:441
learning rate0.0013333333333333335, loss:2.8581438064575195
batch i:442
learning rate0.0013333333333333335, loss:2.8173866271972656
batch i:443
learning rate0.0013333333333333335, loss:2.8002829551696777
batch i:444
learning rate0.0013333333333333335, loss:2.80731201171875
batch i:445
learning rate0.0013333333333333335, loss:2.747379779815674
batch i:446
learning rate0.0013333333333333335, loss:2.724748134613037
batch i:447
learning rate0.0013333333333333335, loss:2.7939400672912598
batch i:448
learning rate0.0013333333333333335, loss:2.725584030151367
batch i:449
learning rate0.0013333333333333335, loss:2.7144012451171875
batch i:450
learning rate0.0013333333333333335, loss:2.7076416015625
batch i:451
learning rate0.0013333333333333335, loss:2.7100634574890137
batch i:452
learning rate0.0013333333333333335, loss:2.7671775817871094
batch i:453
learning rate0.0013333333333333335, loss:2.7313194274902344
batch i:454
learning rate0.0013333333333333335, loss:2.720900535583496
batch i:455
learning rate0.0013333333333333335, loss:2.7464518547058105
batch i:456
learning rate0.0013333333333333335, loss:2.694871425628662
batch i:457
learning rate0.0013333333333333335, loss:2.7856202125549316
batch i:458
learning rate0.0013333333333333335, loss:2.6898012161254883
batch i:459
learning rate0.0013333333333333335, loss:2.713993549346924
batch i:460
learning rate0.0013333333333333335, loss:2.732632637023926
batch i:461
learning rate0.0013333333333333335, loss:2.731785774230957
batch i:462
learning rate0.0013333333333333335, loss:2.7249488830566406
batch i:463
learning rate0.0013333333333333335, loss:2.7368931770324707
batch i:464
learning rate0.0013333333333333335, loss:2.6414802074432373
batch i:465
learning rate0.0013333333333333335, loss:2.682957410812378
batch i:466
learning rate0.0013333333333333335, loss:2.6836342811584473
batch i:467
learning rate0.0013333333333333335, loss:2.7755327224731445
batch i:468
learning rate0.0013333333333333335, loss:2.7540504932403564
batch i:469
learning rate0.0013333333333333335, loss:2.678821563720703
batch i:470
learning rate0.0013333333333333335, loss:2.684946298599243
batch i:471
learning rate0.0013333333333333335, loss:2.765902042388916
batch i:472
learning rate0.0013333333333333335, loss:2.704930543899536
batch i:473
learning rate0.0013333333333333335, loss:2.7452378273010254
batch i:474
learning rate0.0013333333333333335, loss:2.661728858947754
batch i:475
learning rate0.0013333333333333335, loss:2.6838431358337402
batch i:476
learning rate0.0013333333333333335, loss:2.6257264614105225
batch i:477
learning rate0.0013333333333333335, loss:2.6984949111938477
batch i:478
learning rate0.0013333333333333335, loss:2.7713963985443115
batch i:479
learning rate0.0013333333333333335, loss:2.647348403930664
batch i:480
learning rate0.0013333333333333335, loss:2.6366913318634033
batch i:481
learning rate0.0013333333333333335, loss:2.8294434547424316
batch i:482
learning rate0.0013333333333333335, loss:2.7591381072998047
batch i:483
learning rate0.0013333333333333335, loss:2.6709237098693848
batch i:484
learning rate0.0013333333333333335, loss:2.6570382118225098
batch i:485
learning rate0.0013333333333333335, loss:2.705784797668457
batch i:486
learning rate0.0013333333333333335, loss:2.6653172969818115
batch i:487
learning rate0.0013333333333333335, loss:2.737152576446533
batch i:488
learning rate0.0013333333333333335, loss:2.6654300689697266
batch i:489
learning rate0.0013333333333333335, loss:2.7013893127441406
batch i:490
learning rate0.0013333333333333335, loss:2.74000883102417
batch i:491
learning rate0.0013333333333333335, loss:2.7109570503234863
batch i:492
learning rate0.0013333333333333335, loss:2.7103655338287354
batch i:493
learning rate0.0013333333333333335, loss:2.6534433364868164
batch i:494
learning rate0.0013333333333333335, loss:2.6441140174865723
batch i:495
learning rate0.0013333333333333335, loss:2.627978801727295
batch i:496
learning rate0.0013333333333333335, loss:2.538896083831787
batch i:497
learning rate0.0013333333333333335, loss:2.72885799407959
batch i:498
learning rate0.0013333333333333335, loss:2.6656665802001953
batch i:499
learning rate0.0013333333333333335, loss:2.819653034210205
batch i:500
learning rate0.0013333333333333335, loss:2.6718783378601074
current self-play batch: 500
num_playouts:1000, win: 9, lose: 1, tie:0
average time: 167.0178171634674
batch i:501
learning rate0.0013333333333333335, loss:2.6643216609954834
batch i:502
learning rate0.0013333333333333335, loss:2.711620569229126
batch i:503
learning rate0.0013333333333333335, loss:2.531144857406616
batch i:504
learning rate0.0013333333333333335, loss:2.7273778915405273
batch i:505
learning rate0.0013333333333333335, loss:2.7237019538879395
batch i:506
learning rate0.0013333333333333335, loss:2.730848789215088
batch i:507
learning rate0.0013333333333333335, loss:2.6700353622436523
batch i:508
learning rate0.0013333333333333335, loss:2.655540943145752
batch i:509
learning rate0.0013333333333333335, loss:2.7865066528320312
batch i:510
learning rate0.0013333333333333335, loss:2.738278865814209
batch i:511
learning rate0.0013333333333333335, loss:2.68967342376709
batch i:512
learning rate0.0013333333333333335, loss:2.617434024810791
batch i:513
learning rate0.0013333333333333335, loss:2.7587156295776367
batch i:514
learning rate0.0013333333333333335, loss:2.7018256187438965
batch i:515
learning rate0.0013333333333333335, loss:2.679989814758301
batch i:516
learning rate0.0013333333333333335, loss:2.6426639556884766
batch i:517
learning rate0.0013333333333333335, loss:2.5804481506347656
batch i:518
learning rate0.0013333333333333335, loss:2.680414915084839
batch i:519
learning rate0.0013333333333333335, loss:2.683032512664795
batch i:520
learning rate0.0013333333333333335, loss:2.7091712951660156
batch i:521
learning rate0.0013333333333333335, loss:2.694037437438965
batch i:522
learning rate0.0013333333333333335, loss:2.7287700176239014
batch i:523
learning rate0.0013333333333333335, loss:2.749912977218628
batch i:524
learning rate0.0013333333333333335, loss:2.856316566467285
batch i:525
learning rate0.0013333333333333335, loss:2.7771313190460205
batch i:526
learning rate0.0013333333333333335, loss:2.911360740661621
batch i:527
learning rate0.0013333333333333335, loss:2.894946813583374
batch i:528
learning rate0.0013333333333333335, loss:2.812760353088379
batch i:529
learning rate0.0013333333333333335, loss:2.834547519683838
batch i:530
learning rate0.0013333333333333335, loss:2.7202062606811523
batch i:531
learning rate0.0013333333333333335, loss:2.7271885871887207
batch i:532
learning rate0.0013333333333333335, loss:2.6979527473449707
batch i:533
learning rate0.0013333333333333335, loss:2.690086841583252
batch i:534
learning rate0.0013333333333333335, loss:2.758779764175415
batch i:535
learning rate0.0013333333333333335, loss:2.821885585784912
batch i:536
learning rate0.0013333333333333335, loss:2.699449062347412
batch i:537
learning rate0.0013333333333333335, loss:2.7017946243286133
batch i:538
learning rate0.0013333333333333335, loss:2.724668502807617
batch i:539
learning rate0.0013333333333333335, loss:2.7528038024902344
batch i:540
learning rate0.0013333333333333335, loss:2.7600481510162354
batch i:541
learning rate0.0013333333333333335, loss:2.687168836593628
batch i:542
learning rate0.0013333333333333335, loss:2.7160937786102295
batch i:543
learning rate0.0013333333333333335, loss:2.7739076614379883
batch i:544
learning rate0.0013333333333333335, loss:2.74928617477417
batch i:545
learning rate0.0013333333333333335, loss:2.746401309967041
batch i:546
learning rate0.0013333333333333335, loss:2.708416223526001
batch i:547
learning rate0.0013333333333333335, loss:2.7839231491088867
batch i:548
learning rate0.0013333333333333335, loss:2.818203926086426
batch i:549
learning rate0.0013333333333333335, loss:2.7746825218200684
batch i:550
learning rate0.0013333333333333335, loss:2.7484192848205566
batch i:551
learning rate0.0013333333333333335, loss:2.7895069122314453
batch i:552
learning rate0.0013333333333333335, loss:2.78641414642334
batch i:553
learning rate0.0013333333333333335, loss:2.774257183074951
batch i:554
learning rate0.0013333333333333335, loss:2.8581504821777344
batch i:555
learning rate0.0013333333333333335, loss:2.7752699851989746
batch i:556
learning rate0.0013333333333333335, loss:2.758521795272827
batch i:557
learning rate0.0013333333333333335, loss:2.7662129402160645
batch i:558
learning rate0.0013333333333333335, loss:2.805300712585449
batch i:559
learning rate0.0013333333333333335, loss:2.704378128051758
batch i:560
learning rate0.0013333333333333335, loss:2.8458914756774902
batch i:561
learning rate0.0013333333333333335, loss:2.8036553859710693
batch i:562
learning rate0.0013333333333333335, loss:2.6689982414245605
batch i:563
learning rate0.0013333333333333335, loss:2.766709804534912
batch i:564
learning rate0.0013333333333333335, loss:2.8900136947631836
batch i:565
learning rate0.0013333333333333335, loss:2.788081169128418
batch i:566
learning rate0.0013333333333333335, loss:2.7744977474212646
batch i:567
learning rate0.0013333333333333335, loss:2.7507739067077637
batch i:568
learning rate0.0013333333333333335, loss:2.7328405380249023
batch i:569
learning rate0.0013333333333333335, loss:2.736159563064575
batch i:570
learning rate0.0013333333333333335, loss:2.719013214111328
batch i:571
learning rate0.0013333333333333335, loss:2.690098524093628
batch i:572
learning rate0.0013333333333333335, loss:2.7259092330932617
batch i:573
learning rate0.0013333333333333335, loss:2.7476487159729004
batch i:574
learning rate0.0013333333333333335, loss:2.750317096710205
batch i:575
learning rate0.0013333333333333335, loss:2.7796406745910645
batch i:576
learning rate0.0013333333333333335, loss:2.767526149749756
batch i:577
learning rate0.0013333333333333335, loss:2.7573089599609375
batch i:578
learning rate0.0013333333333333335, loss:2.846250057220459
batch i:579
learning rate0.0013333333333333335, loss:2.7741925716400146
batch i:580
learning rate0.0013333333333333335, loss:2.7582638263702393
batch i:581
learning rate0.0013333333333333335, loss:2.8091111183166504
batch i:582
learning rate0.0013333333333333335, loss:2.7132883071899414
batch i:583
learning rate0.0013333333333333335, loss:2.7854785919189453
batch i:584
learning rate0.0013333333333333335, loss:2.8360977172851562
batch i:585
learning rate0.0013333333333333335, loss:2.8128981590270996
batch i:586
learning rate0.0013333333333333335, loss:2.7461843490600586
batch i:587
learning rate0.0013333333333333335, loss:2.6944260597229004
batch i:588
learning rate0.0013333333333333335, loss:2.791985511779785
batch i:589
learning rate0.0013333333333333335, loss:2.805915117263794
batch i:590
learning rate0.0013333333333333335, loss:2.7952141761779785
batch i:591
learning rate0.0013333333333333335, loss:2.843031406402588
batch i:592
learning rate0.0013333333333333335, loss:2.7768850326538086
batch i:593
learning rate0.0013333333333333335, loss:2.7677836418151855
batch i:594
learning rate0.0013333333333333335, loss:2.842733144760132
batch i:595
learning rate0.0013333333333333335, loss:2.8650026321411133
batch i:596
learning rate0.0013333333333333335, loss:2.8275680541992188
batch i:597
learning rate0.0013333333333333335, loss:2.7984273433685303
batch i:598
learning rate0.0013333333333333335, loss:2.8154401779174805
batch i:599
learning rate0.0013333333333333335, loss:2.845832109451294
batch i:600
learning rate0.0013333333333333335, loss:2.8138298988342285
batch i:601
learning rate0.0013333333333333335, loss:2.8938982486724854
batch i:602
learning rate0.0013333333333333335, loss:2.933357000350952
batch i:603
learning rate0.0013333333333333335, loss:2.9071643352508545
batch i:604
learning rate0.0013333333333333335, loss:2.8292500972747803
batch i:605
learning rate0.0013333333333333335, loss:2.7836461067199707
batch i:606
learning rate0.0013333333333333335, loss:2.819925308227539
batch i:607
learning rate0.0013333333333333335, loss:2.779141664505005
batch i:608
learning rate0.0013333333333333335, loss:2.7986226081848145
batch i:609
learning rate0.0013333333333333335, loss:2.814450979232788
batch i:610
learning rate0.0013333333333333335, loss:2.910287857055664
batch i:611
learning rate0.0013333333333333335, loss:2.840444564819336
batch i:612
learning rate0.0013333333333333335, loss:2.997325897216797
batch i:613
learning rate0.0013333333333333335, loss:2.894782543182373
batch i:614
learning rate0.0013333333333333335, loss:2.898963451385498
batch i:615
learning rate0.0013333333333333335, loss:2.989408016204834
batch i:616
learning rate0.0013333333333333335, loss:2.972676992416382
batch i:617
learning rate0.0013333333333333335, loss:2.960606098175049
batch i:618
learning rate0.0013333333333333335, loss:2.988892078399658
batch i:619
learning rate0.0013333333333333335, loss:2.9130802154541016
batch i:620
learning rate0.0013333333333333335, loss:3.005122661590576
batch i:621
learning rate0.0013333333333333335, loss:2.9849681854248047
batch i:622
learning rate0.0013333333333333335, loss:2.9284791946411133
batch i:623
learning rate0.0013333333333333335, loss:2.942282199859619
batch i:624
learning rate0.0013333333333333335, loss:2.823331356048584
batch i:625
learning rate0.0013333333333333335, loss:2.8895699977874756
batch i:626
learning rate0.0013333333333333335, loss:2.866452217102051
batch i:627
learning rate0.0013333333333333335, loss:2.8717026710510254
batch i:628
learning rate0.0013333333333333335, loss:2.90541934967041
batch i:629
learning rate0.0013333333333333335, loss:3.009718418121338
batch i:630
learning rate0.0013333333333333335, loss:2.953577995300293
batch i:631
learning rate0.0013333333333333335, loss:2.9623708724975586
batch i:632
learning rate0.0013333333333333335, loss:2.915436267852783
batch i:633
learning rate0.0013333333333333335, loss:2.9176025390625
batch i:634
learning rate0.0013333333333333335, loss:2.836001396179199
batch i:635
learning rate0.0013333333333333335, loss:2.8969600200653076
batch i:636
learning rate0.0013333333333333335, loss:2.883491039276123
batch i:637
learning rate0.0013333333333333335, loss:2.9177889823913574
batch i:638
learning rate0.0013333333333333335, loss:2.8647077083587646
batch i:639
learning rate0.0013333333333333335, loss:2.904911994934082
batch i:640
learning rate0.0013333333333333335, loss:2.896247625350952
batch i:641
learning rate0.0013333333333333335, loss:2.918313503265381
batch i:642
learning rate0.0013333333333333335, loss:2.816225051879883
batch i:643
learning rate0.0013333333333333335, loss:2.9273979663848877
batch i:644
learning rate0.0013333333333333335, loss:2.9371848106384277
batch i:645
learning rate0.0013333333333333335, loss:2.9614250659942627
batch i:646
learning rate0.0013333333333333335, loss:2.9965615272521973
batch i:647
learning rate0.0013333333333333335, loss:3.012342691421509
batch i:648
learning rate0.0013333333333333335, loss:2.869680643081665
batch i:649
learning rate0.0013333333333333335, loss:2.956925868988037
batch i:650
learning rate0.0013333333333333335, loss:2.9545154571533203
batch i:651
learning rate0.0013333333333333335, loss:2.933068037033081
batch i:652
learning rate0.0013333333333333335, loss:2.9790735244750977
batch i:653
learning rate0.0013333333333333335, loss:2.9760985374450684
batch i:654
learning rate0.0013333333333333335, loss:2.8897838592529297
batch i:655
learning rate0.0013333333333333335, loss:2.8598294258117676
batch i:656
learning rate0.0013333333333333335, loss:2.8989648818969727
batch i:657
learning rate0.0013333333333333335, loss:2.984487533569336
batch i:658
learning rate0.0013333333333333335, loss:2.9383809566497803
batch i:659
learning rate0.0013333333333333335, loss:2.8883934020996094
batch i:660
learning rate0.0013333333333333335, loss:2.8911080360412598
batch i:661
learning rate0.0013333333333333335, loss:2.9301419258117676
batch i:662
learning rate0.0013333333333333335, loss:2.936366081237793
batch i:663
learning rate0.0013333333333333335, loss:2.9535951614379883
batch i:664
learning rate0.0013333333333333335, loss:2.879378080368042
batch i:665
learning rate0.0013333333333333335, loss:2.970999240875244
batch i:666
learning rate0.0013333333333333335, loss:2.9021878242492676
batch i:667
learning rate0.0013333333333333335, loss:2.909970283508301
batch i:668
learning rate0.0013333333333333335, loss:2.9162678718566895
batch i:669
learning rate0.0013333333333333335, loss:2.869032621383667
batch i:670
learning rate0.0013333333333333335, loss:2.8498830795288086
batch i:671
learning rate0.0013333333333333335, loss:2.8545777797698975
batch i:672
learning rate0.0013333333333333335, loss:2.83290433883667
batch i:673
learning rate0.0013333333333333335, loss:2.9315879344940186
batch i:674
learning rate0.0013333333333333335, loss:2.9097352027893066
batch i:675
learning rate0.0013333333333333335, loss:2.894073486328125
batch i:676
learning rate0.0013333333333333335, loss:2.8918604850769043
batch i:677
learning rate0.0013333333333333335, loss:2.8744449615478516
batch i:678
learning rate0.0013333333333333335, loss:2.878232955932617
batch i:679
learning rate0.0013333333333333335, loss:2.8872225284576416
batch i:680
learning rate0.0013333333333333335, loss:2.8568341732025146
batch i:681
learning rate0.0013333333333333335, loss:2.8374786376953125
batch i:682
learning rate0.0013333333333333335, loss:2.7717578411102295
batch i:683
learning rate0.0013333333333333335, loss:2.85522198677063
batch i:684
learning rate0.0013333333333333335, loss:2.8866477012634277
batch i:685
learning rate0.0013333333333333335, loss:2.81142520904541
batch i:686
learning rate0.0013333333333333335, loss:2.7891693115234375
batch i:687
learning rate0.0013333333333333335, loss:2.9346108436584473
batch i:688
learning rate0.0013333333333333335, loss:2.8679699897766113
batch i:689
learning rate0.0013333333333333335, loss:2.7434990406036377
batch i:690
learning rate0.0013333333333333335, loss:2.748446464538574
batch i:691
learning rate0.0013333333333333335, loss:2.7186226844787598
batch i:692
learning rate0.0013333333333333335, loss:2.849766731262207
batch i:693
learning rate0.0013333333333333335, loss:2.7504076957702637
batch i:694
learning rate0.0013333333333333335, loss:2.8139705657958984
batch i:695
learning rate0.0013333333333333335, loss:2.924898147583008
batch i:696
learning rate0.0013333333333333335, loss:2.795819044113159
batch i:697
learning rate0.0013333333333333335, loss:2.8965811729431152
batch i:698
learning rate0.0013333333333333335, loss:2.821117401123047
batch i:699
learning rate0.0013333333333333335, loss:2.875417947769165
batch i:700
learning rate0.0013333333333333335, loss:2.789825916290283
batch i:701
learning rate0.0013333333333333335, loss:2.7861666679382324
batch i:702
learning rate0.0013333333333333335, loss:2.8836169242858887
batch i:703
learning rate0.0013333333333333335, loss:2.7341389656066895
batch i:704
learning rate0.0013333333333333335, loss:2.795624256134033
batch i:705
learning rate0.0013333333333333335, loss:2.8691444396972656
batch i:706
learning rate0.0013333333333333335, loss:2.7936043739318848
batch i:707
learning rate0.0013333333333333335, loss:2.801849842071533
batch i:708
learning rate0.0013333333333333335, loss:2.8039088249206543
batch i:709
learning rate0.0013333333333333335, loss:2.713313579559326
batch i:710
learning rate0.0013333333333333335, loss:2.8381471633911133
batch i:711
learning rate0.0013333333333333335, loss:2.817446708679199
batch i:712
learning rate0.0013333333333333335, loss:2.857015609741211
batch i:713
learning rate0.0013333333333333335, loss:2.8565337657928467
batch i:714
learning rate0.0013333333333333335, loss:2.9280283451080322
batch i:715
learning rate0.0013333333333333335, loss:2.848576545715332
batch i:716
learning rate0.0013333333333333335, loss:2.8927388191223145
batch i:717
learning rate0.0013333333333333335, loss:2.9218108654022217
batch i:718
learning rate0.0013333333333333335, loss:2.842050075531006
batch i:719
learning rate0.0013333333333333335, loss:2.8738481998443604
batch i:720
learning rate0.0013333333333333335, loss:2.889453411102295
batch i:721
learning rate0.0013333333333333335, loss:2.7534823417663574
batch i:722
learning rate0.0013333333333333335, loss:2.87162446975708
batch i:723
learning rate0.0013333333333333335, loss:2.7984464168548584
batch i:724
learning rate0.0013333333333333335, loss:2.81929612159729
batch i:725
learning rate0.0013333333333333335, loss:2.914447784423828
batch i:726
learning rate0.0013333333333333335, loss:2.8197591304779053
batch i:727
learning rate0.0013333333333333335, loss:2.762448787689209
batch i:728
learning rate0.0013333333333333335, loss:2.7312092781066895
batch i:729
learning rate0.0013333333333333335, loss:2.743473529815674
batch i:730
learning rate0.0013333333333333335, loss:2.9430766105651855
batch i:731
learning rate0.0013333333333333335, loss:2.941896677017212
batch i:732
learning rate0.0013333333333333335, loss:2.8415489196777344
batch i:733
learning rate0.0013333333333333335, loss:2.8075854778289795
batch i:734
learning rate0.0013333333333333335, loss:2.835057020187378
batch i:735
learning rate0.0013333333333333335, loss:2.8902080059051514
batch i:736
learning rate0.0013333333333333335, loss:2.7684950828552246
batch i:737
learning rate0.0013333333333333335, loss:2.8907928466796875
batch i:738
learning rate0.0013333333333333335, loss:2.831408977508545
batch i:739
learning rate0.0013333333333333335, loss:2.8176684379577637
batch i:740
learning rate0.0013333333333333335, loss:2.807060480117798
batch i:741
learning rate0.0013333333333333335, loss:2.8647658824920654
batch i:742
learning rate0.0013333333333333335, loss:2.843928813934326
batch i:743
learning rate0.0013333333333333335, loss:2.7166614532470703
batch i:744
learning rate0.0013333333333333335, loss:2.802239179611206
batch i:745
learning rate0.0013333333333333335, loss:2.8426761627197266
batch i:746
learning rate0.0013333333333333335, loss:2.8400912284851074
batch i:747
learning rate0.0013333333333333335, loss:2.915215492248535
batch i:748
learning rate0.0013333333333333335, loss:2.7945094108581543
batch i:749
learning rate0.0013333333333333335, loss:2.840174913406372
batch i:750
learning rate0.0013333333333333335, loss:2.781506061553955
current self-play batch: 750
num_playouts:1000, win: 10, lose: 0, tie:0
average time: 154.35957803726197
New best policy from pure MCTS
batch i:751
learning rate0.0013333333333333335, loss:2.913299798965454
batch i:752
learning rate0.0013333333333333335, loss:2.7456107139587402
batch i:753
learning rate0.0013333333333333335, loss:2.7765254974365234
batch i:754
learning rate0.0013333333333333335, loss:2.8430910110473633
batch i:755
learning rate0.0013333333333333335, loss:2.7335853576660156
batch i:756
learning rate0.0013333333333333335, loss:2.8125433921813965
batch i:757
learning rate0.0013333333333333335, loss:2.816054344177246
batch i:758
learning rate0.0013333333333333335, loss:2.7966413497924805
batch i:759
learning rate0.0013333333333333335, loss:2.8543505668640137
batch i:760
learning rate0.0013333333333333335, loss:2.79622745513916
batch i:761
learning rate0.0013333333333333335, loss:2.836953639984131
batch i:762
learning rate0.0013333333333333335, loss:2.8689095973968506
batch i:763
learning rate0.0013333333333333335, loss:2.8454678058624268
batch i:764
learning rate0.0013333333333333335, loss:2.970454216003418
batch i:765
learning rate0.0013333333333333335, loss:2.855309009552002
batch i:766
learning rate0.0013333333333333335, loss:2.830657482147217
batch i:767
learning rate0.0013333333333333335, loss:2.835775852203369
batch i:768
learning rate0.0013333333333333335, loss:2.9475603103637695
batch i:769
learning rate0.0013333333333333335, loss:2.824991464614868
batch i:770
learning rate0.0013333333333333335, loss:2.8250386714935303
batch i:771
learning rate0.0013333333333333335, loss:2.9067986011505127
batch i:772
learning rate0.0013333333333333335, loss:2.908144235610962
batch i:773
learning rate0.0013333333333333335, loss:2.859499454498291
batch i:774
learning rate0.0013333333333333335, loss:2.8223750591278076
batch i:775
learning rate0.0013333333333333335, loss:3.018876552581787
batch i:776
learning rate0.0013333333333333335, loss:2.8572661876678467
batch i:777
learning rate0.0013333333333333335, loss:2.8421316146850586
batch i:778
learning rate0.0013333333333333335, loss:2.9451377391815186
batch i:779
learning rate0.0013333333333333335, loss:2.8762600421905518
batch i:780
learning rate0.0013333333333333335, loss:2.8692939281463623
batch i:781
learning rate0.0013333333333333335, loss:2.899301052093506
batch i:782
learning rate0.0013333333333333335, loss:2.9880452156066895
batch i:783
learning rate0.0013333333333333335, loss:2.8626961708068848
batch i:784
learning rate0.0013333333333333335, loss:2.915639877319336
batch i:785
learning rate0.0013333333333333335, loss:2.8945159912109375
batch i:786
learning rate0.0013333333333333335, loss:2.8682761192321777
batch i:787
learning rate0.0013333333333333335, loss:2.9131200313568115
batch i:788
learning rate0.0013333333333333335, loss:2.847553253173828
batch i:789
learning rate0.0013333333333333335, loss:2.967055559158325
batch i:790
learning rate0.0013333333333333335, loss:2.848228931427002
batch i:791
learning rate0.0013333333333333335, loss:2.8183577060699463
batch i:792
learning rate0.0013333333333333335, loss:2.8672008514404297
batch i:793
learning rate0.0013333333333333335, loss:2.959054470062256
batch i:794
learning rate0.0013333333333333335, loss:2.9391469955444336
batch i:795
learning rate0.0013333333333333335, loss:2.906252384185791
batch i:796
learning rate0.0013333333333333335, loss:2.944791078567505
batch i:797
learning rate0.0013333333333333335, loss:2.963301658630371
batch i:798
learning rate0.0013333333333333335, loss:2.917574882507324
batch i:799
learning rate0.0013333333333333335, loss:2.916724681854248
batch i:800
learning rate0.0013333333333333335, loss:3.0459771156311035
batch i:801
learning rate0.0013333333333333335, loss:2.966970920562744
batch i:802
learning rate0.0013333333333333335, loss:2.9349677562713623
batch i:803
learning rate0.0013333333333333335, loss:3.148108959197998
batch i:804
learning rate0.0013333333333333335, loss:3.039608955383301
batch i:805
learning rate0.0013333333333333335, loss:3.058502197265625
batch i:806
learning rate0.0013333333333333335, loss:3.107002019882202
batch i:807
learning rate0.0013333333333333335, loss:2.9679114818573
batch i:808
learning rate0.0013333333333333335, loss:2.928795576095581
batch i:809
learning rate0.0013333333333333335, loss:3.0527005195617676
batch i:810
learning rate0.0013333333333333335, loss:3.035242795944214
batch i:811
learning rate0.0013333333333333335, loss:3.021923542022705
batch i:812
learning rate0.0013333333333333335, loss:2.830136299133301
batch i:813
learning rate0.0013333333333333335, loss:2.955282688140869
batch i:814
learning rate0.0013333333333333335, loss:2.957947254180908
batch i:815
learning rate0.0013333333333333335, loss:3.0810484886169434
batch i:816
learning rate0.0013333333333333335, loss:3.0104777812957764
batch i:817
learning rate0.0013333333333333335, loss:2.9832046031951904
batch i:818
learning rate0.0013333333333333335, loss:2.982743978500366
batch i:819
learning rate0.0013333333333333335, loss:3.096982955932617
batch i:820
learning rate0.0013333333333333335, loss:3.0326709747314453
batch i:821
learning rate0.0013333333333333335, loss:3.0680670738220215
batch i:822
learning rate0.0013333333333333335, loss:3.0859365463256836
batch i:823
learning rate0.0013333333333333335, loss:3.0296385288238525
batch i:824
learning rate0.0013333333333333335, loss:2.953979730606079
batch i:825
learning rate0.0013333333333333335, loss:3.057478666305542
batch i:826
learning rate0.0013333333333333335, loss:3.0110692977905273
batch i:827
learning rate0.0013333333333333335, loss:3.0520339012145996
batch i:828
learning rate0.0013333333333333335, loss:3.0421953201293945
batch i:829
learning rate0.0013333333333333335, loss:2.968698263168335
batch i:830
learning rate0.0013333333333333335, loss:3.037548542022705
batch i:831
learning rate0.0013333333333333335, loss:3.0464372634887695
batch i:832
learning rate0.0013333333333333335, loss:2.994702100753784
batch i:833
learning rate0.0013333333333333335, loss:3.1320858001708984
batch i:834
learning rate0.0013333333333333335, loss:3.1521244049072266
batch i:835
learning rate0.0013333333333333335, loss:3.104229211807251
batch i:836
learning rate0.0013333333333333335, loss:3.045518159866333
batch i:837
learning rate0.0013333333333333335, loss:3.098508358001709
batch i:838
learning rate0.0013333333333333335, loss:3.0744903087615967
batch i:839
learning rate0.0013333333333333335, loss:3.092283248901367
batch i:840
learning rate0.0013333333333333335, loss:3.0688657760620117
batch i:841
learning rate0.0013333333333333335, loss:3.0908312797546387
batch i:842
learning rate0.0013333333333333335, loss:3.0621042251586914
batch i:843
learning rate0.0013333333333333335, loss:3.0866599082946777
batch i:844
learning rate0.0013333333333333335, loss:3.1465322971343994
batch i:845
learning rate0.0013333333333333335, loss:3.102835178375244
batch i:846
learning rate0.0013333333333333335, loss:3.1382193565368652
batch i:847
learning rate0.0013333333333333335, loss:3.1697115898132324
batch i:848
learning rate0.0013333333333333335, loss:3.1553797721862793
batch i:849
learning rate0.0013333333333333335, loss:3.1414618492126465
batch i:850
learning rate0.0013333333333333335, loss:3.1255362033843994
batch i:851
learning rate0.0013333333333333335, loss:3.002962350845337
batch i:852
learning rate0.0013333333333333335, loss:3.094804525375366
batch i:853
learning rate0.0013333333333333335, loss:3.1127123832702637
batch i:854
learning rate0.0013333333333333335, loss:3.155247926712036
batch i:855
learning rate0.0013333333333333335, loss:3.076075553894043
batch i:856
learning rate0.0013333333333333335, loss:3.163109302520752
batch i:857
learning rate0.0013333333333333335, loss:3.132026433944702
batch i:858
learning rate0.0013333333333333335, loss:3.047041416168213
batch i:859
learning rate0.0013333333333333335, loss:3.0822834968566895
batch i:860
learning rate0.0013333333333333335, loss:3.076310873031616
batch i:861
learning rate0.0013333333333333335, loss:2.973228931427002
batch i:862
learning rate0.0013333333333333335, loss:3.150515556335449
batch i:863
learning rate0.0013333333333333335, loss:3.1017353534698486
batch i:864
learning rate0.0013333333333333335, loss:3.128610134124756
batch i:865
learning rate0.0013333333333333335, loss:3.079044818878174
batch i:866
learning rate0.0013333333333333335, loss:3.032586097717285
batch i:867
learning rate0.0013333333333333335, loss:3.0426297187805176
batch i:868
learning rate0.0013333333333333335, loss:3.0432000160217285
batch i:869
learning rate0.0013333333333333335, loss:3.1541693210601807
batch i:870
learning rate0.0013333333333333335, loss:3.1012487411499023
batch i:871
learning rate0.0013333333333333335, loss:3.0754570960998535
batch i:872
learning rate0.0013333333333333335, loss:3.129912853240967
batch i:873
learning rate0.0013333333333333335, loss:3.1113176345825195
batch i:874
learning rate0.0013333333333333335, loss:3.1100172996520996
batch i:875
learning rate0.0013333333333333335, loss:3.0978188514709473
batch i:876
learning rate0.0013333333333333335, loss:3.1364684104919434
batch i:877
learning rate0.0013333333333333335, loss:3.1831541061401367
batch i:878
learning rate0.0013333333333333335, loss:3.0798864364624023
batch i:879
learning rate0.0013333333333333335, loss:3.102367401123047
batch i:880
learning rate0.0013333333333333335, loss:3.036479949951172
batch i:881
learning rate0.0013333333333333335, loss:3.1377182006835938
batch i:882
learning rate0.0013333333333333335, loss:3.1160945892333984
batch i:883
learning rate0.0013333333333333335, loss:3.083151340484619
batch i:884
learning rate0.0013333333333333335, loss:3.1106643676757812
batch i:885
learning rate0.0013333333333333335, loss:3.110649585723877
batch i:886
learning rate0.0013333333333333335, loss:3.0701422691345215
batch i:887
learning rate0.0013333333333333335, loss:3.1049935817718506
batch i:888
learning rate0.0013333333333333335, loss:3.0969181060791016
batch i:889
learning rate0.0013333333333333335, loss:3.003688335418701
batch i:890
learning rate0.0013333333333333335, loss:3.032050132751465
batch i:891
learning rate0.0013333333333333335, loss:3.146430492401123
batch i:892
learning rate0.0013333333333333335, loss:3.195225238800049
batch i:893
learning rate0.0013333333333333335, loss:3.1325011253356934
batch i:894
learning rate0.0013333333333333335, loss:3.070438861846924
batch i:895
learning rate0.0013333333333333335, loss:3.096285581588745
batch i:896
learning rate0.0013333333333333335, loss:3.1600441932678223
batch i:897
learning rate0.0013333333333333335, loss:3.038282871246338
batch i:898
learning rate0.0013333333333333335, loss:3.0840840339660645
batch i:899
learning rate0.0013333333333333335, loss:3.0386691093444824
batch i:900
learning rate0.0013333333333333335, loss:3.1469614505767822
batch i:901
learning rate0.0013333333333333335, loss:3.142606735229492
batch i:902
learning rate0.0013333333333333335, loss:3.2048752307891846
batch i:903
learning rate0.0013333333333333335, loss:3.191532850265503
batch i:904
learning rate0.0013333333333333335, loss:3.062307357788086
batch i:905
learning rate0.0013333333333333335, loss:3.2107067108154297
batch i:906
learning rate0.0013333333333333335, loss:2.99596905708313
batch i:907
learning rate0.0013333333333333335, loss:3.0484089851379395
batch i:908
learning rate0.0013333333333333335, loss:3.108424425125122
batch i:909
learning rate0.0013333333333333335, loss:3.071530818939209
batch i:910
learning rate0.0013333333333333335, loss:3.101742744445801
batch i:911
learning rate0.0013333333333333335, loss:3.0856051445007324
batch i:912
learning rate0.0013333333333333335, loss:2.9882471561431885
batch i:913
learning rate0.0013333333333333335, loss:3.058487892150879
batch i:914
learning rate0.0013333333333333335, loss:3.119250774383545
batch i:915
learning rate0.0013333333333333335, loss:3.1180052757263184
batch i:916
learning rate0.0013333333333333335, loss:3.095118522644043
batch i:917
learning rate0.0013333333333333335, loss:2.99985671043396
batch i:918
learning rate0.0013333333333333335, loss:3.1995346546173096
batch i:919
learning rate0.0013333333333333335, loss:3.0481228828430176
batch i:920
learning rate0.0013333333333333335, loss:3.015373706817627
batch i:921
learning rate0.0013333333333333335, loss:3.02724027633667
batch i:922
learning rate0.0013333333333333335, loss:3.090681552886963
batch i:923
learning rate0.0013333333333333335, loss:3.0941638946533203
batch i:924
learning rate0.0013333333333333335, loss:3.0343244075775146
batch i:925
learning rate0.0013333333333333335, loss:3.051666736602783
batch i:926
learning rate0.0013333333333333335, loss:3.086487054824829
batch i:927
learning rate0.0013333333333333335, loss:3.0771493911743164
batch i:928
learning rate0.0013333333333333335, loss:3.046020269393921
batch i:929
learning rate0.0013333333333333335, loss:3.056584596633911
batch i:930
learning rate0.0013333333333333335, loss:3.133985996246338
batch i:931
learning rate0.0013333333333333335, loss:3.135169506072998
batch i:932
learning rate0.0013333333333333335, loss:3.0358996391296387
batch i:933
learning rate0.0013333333333333335, loss:3.064457893371582
batch i:934
learning rate0.0013333333333333335, loss:3.132004737854004
batch i:935
learning rate0.0013333333333333335, loss:2.9289207458496094
batch i:936
learning rate0.0013333333333333335, loss:2.992640972137451
batch i:937
learning rate0.0013333333333333335, loss:3.0271711349487305
batch i:938
learning rate0.0013333333333333335, loss:3.0319902896881104
batch i:939
learning rate0.0013333333333333335, loss:3.0131139755249023
batch i:940
learning rate0.0013333333333333335, loss:3.0471081733703613
batch i:941
learning rate0.0013333333333333335, loss:2.9943695068359375
batch i:942
learning rate0.0013333333333333335, loss:3.0393776893615723
batch i:943
learning rate0.0013333333333333335, loss:3.0095536708831787
batch i:944
learning rate0.0013333333333333335, loss:3.0509724617004395
batch i:945
learning rate0.0013333333333333335, loss:3.000162363052368
batch i:946
learning rate0.0013333333333333335, loss:2.954322576522827
batch i:947
learning rate0.0013333333333333335, loss:3.134366035461426
batch i:948
learning rate0.0013333333333333335, loss:3.1182799339294434
batch i:949
learning rate0.0013333333333333335, loss:3.05172061920166
batch i:950
learning rate0.0013333333333333335, loss:2.984166145324707
batch i:951
learning rate0.0013333333333333335, loss:3.037665843963623
batch i:952
learning rate0.0013333333333333335, loss:3.034637928009033
batch i:953
learning rate0.0013333333333333335, loss:2.9887561798095703
batch i:954
learning rate0.0013333333333333335, loss:3.027316093444824
batch i:955
learning rate0.0013333333333333335, loss:3.069397211074829
batch i:956
learning rate0.0013333333333333335, loss:2.9780359268188477
batch i:957
learning rate0.0013333333333333335, loss:3.087800979614258
batch i:958
learning rate0.0013333333333333335, loss:2.939640522003174
batch i:959
learning rate0.0013333333333333335, loss:3.114349842071533
batch i:960
learning rate0.0013333333333333335, loss:3.0318069458007812
batch i:961
learning rate0.0013333333333333335, loss:3.0829429626464844
batch i:962
learning rate0.0013333333333333335, loss:2.944700241088867
batch i:963
learning rate0.0013333333333333335, loss:3.0924072265625
batch i:964
learning rate0.0013333333333333335, loss:3.123702049255371
batch i:965
learning rate0.0013333333333333335, loss:3.0029091835021973
batch i:966
learning rate0.0013333333333333335, loss:3.1919069290161133
batch i:967
learning rate0.0013333333333333335, loss:3.066941022872925
batch i:968
learning rate0.0013333333333333335, loss:3.0522279739379883
batch i:969
learning rate0.0013333333333333335, loss:3.0755116939544678
batch i:970
learning rate0.0013333333333333335, loss:2.9834868907928467
batch i:971
learning rate0.0013333333333333335, loss:3.075456380844116
batch i:972
learning rate0.0013333333333333335, loss:3.1176576614379883
batch i:973
learning rate0.0013333333333333335, loss:3.24353289604187
batch i:974
learning rate0.0013333333333333335, loss:3.1445393562316895
batch i:975
learning rate0.0013333333333333335, loss:3.189692974090576
batch i:976
learning rate0.0013333333333333335, loss:3.0400054454803467
batch i:977
learning rate0.0013333333333333335, loss:3.0862717628479004
batch i:978
learning rate0.0013333333333333335, loss:3.105632781982422
batch i:979
learning rate0.0013333333333333335, loss:3.1052470207214355
batch i:980
learning rate0.0013333333333333335, loss:3.015523910522461
batch i:981
learning rate0.0013333333333333335, loss:3.0936360359191895
batch i:982
learning rate0.0013333333333333335, loss:3.0445456504821777
batch i:983
learning rate0.0013333333333333335, loss:3.0530145168304443
batch i:984
learning rate0.0013333333333333335, loss:3.133054256439209
batch i:985
learning rate0.0013333333333333335, loss:2.9579062461853027
batch i:986
learning rate0.0013333333333333335, loss:3.072984218597412
batch i:987
learning rate0.0013333333333333335, loss:3.119776964187622
batch i:988
learning rate0.0013333333333333335, loss:2.998243570327759
batch i:989
learning rate0.0013333333333333335, loss:2.987436294555664
batch i:990
learning rate0.0013333333333333335, loss:2.981238842010498
batch i:991
learning rate0.0013333333333333335, loss:3.0220704078674316
batch i:992
learning rate0.0013333333333333335, loss:2.963369846343994
batch i:993
learning rate0.0013333333333333335, loss:3.0149223804473877
batch i:994
learning rate0.0013333333333333335, loss:3.087939739227295
batch i:995
learning rate0.0013333333333333335, loss:3.0036637783050537
batch i:996
learning rate0.0013333333333333335, loss:3.039811134338379
batch i:997
learning rate0.0013333333333333335, loss:2.9574828147888184
batch i:998
learning rate0.0013333333333333335, loss:2.993992567062378
batch i:999
learning rate0.0013333333333333335, loss:3.0145883560180664
batch i:1000
learning rate0.0013333333333333335, loss:2.939321994781494
current self-play batch: 1000
num_playouts:2000, win: 5, lose: 5, tie:0
average time: 391.9212914228439
New best policy from pure MCTS
batch i:1001
learning rate0.0013333333333333335, loss:2.946824789047241
batch i:1002
learning rate0.0013333333333333335, loss:3.0577707290649414
batch i:1003
learning rate0.0013333333333333335, loss:2.9638051986694336
batch i:1004
learning rate0.0013333333333333335, loss:2.913736343383789
batch i:1005
learning rate0.0013333333333333335, loss:2.970414638519287
batch i:1006
learning rate0.0013333333333333335, loss:2.955641746520996
batch i:1007
learning rate0.0013333333333333335, loss:2.913815498352051
batch i:1008
learning rate0.0013333333333333335, loss:2.937321186065674
batch i:1009
learning rate0.0013333333333333335, loss:2.8905837535858154
batch i:1010
learning rate0.0013333333333333335, loss:3.0451197624206543
batch i:1011
learning rate0.0013333333333333335, loss:3.02046537399292
batch i:1012
learning rate0.0013333333333333335, loss:2.959880828857422
batch i:1013
learning rate0.0013333333333333335, loss:3.068466901779175
batch i:1014
learning rate0.0013333333333333335, loss:3.128474712371826
batch i:1015
learning rate0.0013333333333333335, loss:3.046506881713867
batch i:1016
learning rate0.0013333333333333335, loss:3.025587558746338
batch i:1017
learning rate0.0013333333333333335, loss:3.0913548469543457
batch i:1018
learning rate0.0013333333333333335, loss:3.07080078125
batch i:1019
learning rate0.0013333333333333335, loss:2.987427234649658
batch i:1020
learning rate0.0013333333333333335, loss:3.0012691020965576
batch i:1021
learning rate0.0013333333333333335, loss:3.0223371982574463
batch i:1022
learning rate0.0013333333333333335, loss:3.0891671180725098
batch i:1023
learning rate0.0013333333333333335, loss:3.101630687713623
batch i:1024
learning rate0.0013333333333333335, loss:3.035466432571411
batch i:1025
learning rate0.0013333333333333335, loss:3.003345012664795
batch i:1026
learning rate0.0013333333333333335, loss:3.021576404571533
batch i:1027
learning rate0.0013333333333333335, loss:2.9753031730651855
batch i:1028
learning rate0.0013333333333333335, loss:3.013719081878662
batch i:1029
learning rate0.0013333333333333335, loss:3.025043487548828
batch i:1030
learning rate0.0013333333333333335, loss:3.128751277923584
batch i:1031
learning rate0.0013333333333333335, loss:3.1092143058776855
batch i:1032
learning rate0.0013333333333333335, loss:3.0735058784484863
batch i:1033
learning rate0.0013333333333333335, loss:3.096165180206299
batch i:1034
learning rate0.0013333333333333335, loss:3.0942652225494385
batch i:1035
learning rate0.0013333333333333335, loss:2.9680118560791016
batch i:1036
learning rate0.0013333333333333335, loss:3.070784568786621
batch i:1037
learning rate0.0013333333333333335, loss:3.1096339225769043
batch i:1038
learning rate0.0013333333333333335, loss:3.0416407585144043
batch i:1039
learning rate0.0013333333333333335, loss:2.9426283836364746
batch i:1040
learning rate0.0013333333333333335, loss:3.028681755065918
batch i:1041
learning rate0.0013333333333333335, loss:2.9656312465667725
batch i:1042
learning rate0.0013333333333333335, loss:3.015673875808716
batch i:1043
learning rate0.0013333333333333335, loss:2.9025049209594727
batch i:1044
learning rate0.0013333333333333335, loss:2.9702305793762207
batch i:1045
learning rate0.0013333333333333335, loss:2.922009229660034
batch i:1046
learning rate0.0013333333333333335, loss:2.9495749473571777
batch i:1047
learning rate0.0013333333333333335, loss:2.9969122409820557
batch i:1048
learning rate0.0013333333333333335, loss:3.018629550933838
batch i:1049
learning rate0.0013333333333333335, loss:3.1185431480407715
batch i:1050
learning rate0.0013333333333333335, loss:3.060591220855713
batch i:1051
learning rate0.0013333333333333335, loss:3.087374210357666
batch i:1052
learning rate0.0013333333333333335, loss:2.9932665824890137
batch i:1053
learning rate0.0013333333333333335, loss:3.095820903778076
batch i:1054
learning rate0.0013333333333333335, loss:3.022169589996338
batch i:1055
learning rate0.0013333333333333335, loss:3.099154233932495
batch i:1056
learning rate0.0013333333333333335, loss:3.056053400039673
batch i:1057
learning rate0.0013333333333333335, loss:3.0322182178497314
batch i:1058
learning rate0.0013333333333333335, loss:3.1143102645874023
batch i:1059
learning rate0.0013333333333333335, loss:3.011103391647339
batch i:1060
learning rate0.0013333333333333335, loss:3.1042675971984863
batch i:1061
learning rate0.0013333333333333335, loss:3.065688133239746
batch i:1062
learning rate0.0013333333333333335, loss:2.9905920028686523
batch i:1063
learning rate0.0013333333333333335, loss:3.024780750274658
batch i:1064
learning rate0.0013333333333333335, loss:3.0486183166503906
batch i:1065
learning rate0.0013333333333333335, loss:2.9991555213928223
batch i:1066
learning rate0.0013333333333333335, loss:3.0473968982696533
batch i:1067
learning rate0.0013333333333333335, loss:2.981119155883789
batch i:1068
learning rate0.0013333333333333335, loss:2.9958109855651855
batch i:1069
learning rate0.0013333333333333335, loss:2.9863719940185547
batch i:1070
learning rate0.0013333333333333335, loss:3.071800708770752
batch i:1071
learning rate0.0013333333333333335, loss:3.0779571533203125
batch i:1072
learning rate0.0013333333333333335, loss:3.0465612411499023
batch i:1073
learning rate0.0013333333333333335, loss:2.9628939628601074
batch i:1074
learning rate0.0013333333333333335, loss:3.048389434814453
batch i:1075
learning rate0.0013333333333333335, loss:2.997426748275757
batch i:1076
learning rate0.0013333333333333335, loss:3.0475635528564453
batch i:1077
learning rate0.0013333333333333335, loss:3.0308945178985596
batch i:1078
learning rate0.0013333333333333335, loss:3.0018467903137207
batch i:1079
learning rate0.0013333333333333335, loss:3.033869981765747
batch i:1080
learning rate0.0013333333333333335, loss:3.0263614654541016
batch i:1081
learning rate0.0013333333333333335, loss:3.0748839378356934
batch i:1082
learning rate0.0013333333333333335, loss:2.9620096683502197
batch i:1083
learning rate0.0013333333333333335, loss:2.944286346435547
batch i:1084
learning rate0.0013333333333333335, loss:2.9886367321014404
batch i:1085
learning rate0.0013333333333333335, loss:3.0075912475585938
batch i:1086
learning rate0.0013333333333333335, loss:2.9292073249816895
batch i:1087
learning rate0.0013333333333333335, loss:2.9624338150024414
batch i:1088
learning rate0.0013333333333333335, loss:2.987435817718506
batch i:1089
learning rate0.0013333333333333335, loss:3.0348610877990723
batch i:1090
learning rate0.0013333333333333335, loss:2.9788014888763428
batch i:1091
learning rate0.0013333333333333335, loss:3.0188517570495605
batch i:1092
learning rate0.0013333333333333335, loss:3.0493528842926025
batch i:1093
learning rate0.0013333333333333335, loss:3.085629940032959
batch i:1094
learning rate0.0013333333333333335, loss:3.0593910217285156
batch i:1095
learning rate0.0013333333333333335, loss:2.993968963623047
batch i:1096
learning rate0.0013333333333333335, loss:2.9622671604156494
batch i:1097
learning rate0.0013333333333333335, loss:2.937554359436035
batch i:1098
learning rate0.0013333333333333335, loss:2.9520931243896484
batch i:1099
learning rate0.0013333333333333335, loss:2.979130506515503
batch i:1100
learning rate0.0013333333333333335, loss:2.925583839416504
batch i:1101
learning rate0.0013333333333333335, loss:3.009810447692871
batch i:1102
learning rate0.0013333333333333335, loss:3.0708861351013184
batch i:1103
learning rate0.0013333333333333335, loss:2.9404256343841553
batch i:1104
learning rate0.0013333333333333335, loss:3.0698585510253906
batch i:1105
learning rate0.0013333333333333335, loss:3.0985047817230225
batch i:1106
learning rate0.0013333333333333335, loss:3.106661319732666
batch i:1107
learning rate0.0013333333333333335, loss:3.0087966918945312
batch i:1108
learning rate0.0013333333333333335, loss:3.01269793510437
batch i:1109
learning rate0.0013333333333333335, loss:3.053616523742676
batch i:1110
learning rate0.0013333333333333335, loss:3.0364089012145996
batch i:1111
learning rate0.0013333333333333335, loss:3.0542821884155273
batch i:1112
learning rate0.0013333333333333335, loss:3.0149002075195312
batch i:1113
learning rate0.0013333333333333335, loss:3.075181007385254
batch i:1114
learning rate0.0013333333333333335, loss:3.0062992572784424
batch i:1115
learning rate0.0013333333333333335, loss:3.042766571044922
batch i:1116
learning rate0.0013333333333333335, loss:3.111222982406616
batch i:1117
learning rate0.0013333333333333335, loss:3.0134668350219727
batch i:1118
learning rate0.0013333333333333335, loss:3.0562543869018555
batch i:1119
learning rate0.0013333333333333335, loss:2.9981565475463867
batch i:1120
learning rate0.0013333333333333335, loss:3.0942389965057373
batch i:1121
learning rate0.0013333333333333335, loss:2.951324462890625
batch i:1122
learning rate0.0013333333333333335, loss:2.961529493331909
batch i:1123
learning rate0.0013333333333333335, loss:3.013859272003174
batch i:1124
learning rate0.0013333333333333335, loss:2.954604148864746
batch i:1125
learning rate0.0013333333333333335, loss:2.9744277000427246
batch i:1126
learning rate0.0013333333333333335, loss:2.9524784088134766
batch i:1127
learning rate0.0013333333333333335, loss:2.9401302337646484
batch i:1128
learning rate0.0013333333333333335, loss:3.089876174926758
batch i:1129
learning rate0.0013333333333333335, loss:2.9283695220947266
batch i:1130
learning rate0.0013333333333333335, loss:3.0081210136413574
batch i:1131
learning rate0.0013333333333333335, loss:2.9870197772979736
batch i:1132
learning rate0.0013333333333333335, loss:3.0270514488220215
batch i:1133
learning rate0.0013333333333333335, loss:2.9925312995910645
batch i:1134
learning rate0.0013333333333333335, loss:2.963270425796509
batch i:1135
learning rate0.0013333333333333335, loss:3.0723977088928223
batch i:1136
learning rate0.0013333333333333335, loss:3.0050933361053467
batch i:1137
learning rate0.0013333333333333335, loss:3.0005838871002197
batch i:1138
learning rate0.0013333333333333335, loss:3.013976573944092
batch i:1139
learning rate0.0013333333333333335, loss:3.028304100036621
batch i:1140
learning rate0.0013333333333333335, loss:2.961343288421631
batch i:1141
learning rate0.0013333333333333335, loss:2.977170467376709
batch i:1142
learning rate0.0013333333333333335, loss:2.9368112087249756
batch i:1143
learning rate0.0013333333333333335, loss:2.996854782104492
batch i:1144
learning rate0.0013333333333333335, loss:2.922077178955078
batch i:1145
learning rate0.0013333333333333335, loss:2.923292636871338
batch i:1146
learning rate0.0013333333333333335, loss:2.969589948654175
batch i:1147
learning rate0.0013333333333333335, loss:3.010916233062744
batch i:1148
learning rate0.0013333333333333335, loss:2.993830442428589
batch i:1149
learning rate0.0013333333333333335, loss:3.021228075027466
batch i:1150
learning rate0.0013333333333333335, loss:2.9751195907592773
batch i:1151
learning rate0.0013333333333333335, loss:3.0979819297790527
batch i:1152
learning rate0.0013333333333333335, loss:2.9795093536376953
batch i:1153
learning rate0.0013333333333333335, loss:3.0480992794036865
batch i:1154
learning rate0.0013333333333333335, loss:3.014458179473877
batch i:1155
learning rate0.0013333333333333335, loss:3.1108016967773438
batch i:1156
learning rate0.0013333333333333335, loss:3.056391954421997
batch i:1157
learning rate0.0013333333333333335, loss:3.139291763305664
batch i:1158
learning rate0.0013333333333333335, loss:3.025019884109497
batch i:1159
learning rate0.0013333333333333335, loss:2.980830669403076
batch i:1160
learning rate0.0013333333333333335, loss:3.1021244525909424
batch i:1161
learning rate0.0013333333333333335, loss:3.0369577407836914
batch i:1162
learning rate0.0013333333333333335, loss:3.02406644821167
batch i:1163
learning rate0.0013333333333333335, loss:3.1285648345947266
batch i:1164
learning rate0.0013333333333333335, loss:3.082505941390991
batch i:1165
learning rate0.0013333333333333335, loss:3.115758180618286
batch i:1166
learning rate0.0013333333333333335, loss:3.140289783477783
batch i:1167
learning rate0.0013333333333333335, loss:3.0226776599884033
batch i:1168
learning rate0.0013333333333333335, loss:3.0874390602111816
batch i:1169
learning rate0.0013333333333333335, loss:2.9770588874816895
batch i:1170
learning rate0.0013333333333333335, loss:3.0540242195129395
batch i:1171
learning rate0.0013333333333333335, loss:3.069369077682495
batch i:1172
learning rate0.0013333333333333335, loss:3.1553115844726562
batch i:1173
learning rate0.0013333333333333335, loss:3.114401340484619
batch i:1174
learning rate0.0013333333333333335, loss:3.057354211807251
batch i:1175
learning rate0.0013333333333333335, loss:3.136578321456909
batch i:1176
learning rate0.0013333333333333335, loss:3.1004176139831543
batch i:1177
learning rate0.0013333333333333335, loss:3.058687210083008
batch i:1178
learning rate0.0013333333333333335, loss:3.0670576095581055
batch i:1179
learning rate0.0013333333333333335, loss:3.115778923034668
batch i:1180
learning rate0.0013333333333333335, loss:3.051772117614746
batch i:1181
learning rate0.0013333333333333335, loss:3.0266194343566895
batch i:1182
learning rate0.0013333333333333335, loss:3.0450448989868164
batch i:1183
learning rate0.0013333333333333335, loss:2.9675467014312744
batch i:1184
learning rate0.0013333333333333335, loss:2.9869704246520996
batch i:1185
learning rate0.0013333333333333335, loss:2.996997594833374
batch i:1186
learning rate0.0013333333333333335, loss:3.0666255950927734
batch i:1187
learning rate0.0013333333333333335, loss:3.1199116706848145
batch i:1188
learning rate0.0013333333333333335, loss:2.9445815086364746
batch i:1189
learning rate0.0013333333333333335, loss:2.963819980621338
batch i:1190
learning rate0.0013333333333333335, loss:2.9389796257019043
batch i:1191
learning rate0.0013333333333333335, loss:2.988430976867676
batch i:1192
learning rate0.0013333333333333335, loss:3.0195541381835938
batch i:1193
learning rate0.0013333333333333335, loss:3.033015012741089
batch i:1194
learning rate0.0013333333333333335, loss:2.995032787322998
batch i:1195
learning rate0.0013333333333333335, loss:3.0189568996429443
batch i:1196
learning rate0.0013333333333333335, loss:3.0252532958984375
batch i:1197
learning rate0.0013333333333333335, loss:3.0265414714813232
batch i:1198
learning rate0.0013333333333333335, loss:2.9550466537475586
batch i:1199
learning rate0.0013333333333333335, loss:3.013629913330078
batch i:1200
learning rate0.0013333333333333335, loss:2.931042194366455
batch i:1201
learning rate0.0013333333333333335, loss:2.9483089447021484
batch i:1202
learning rate0.0013333333333333335, loss:3.0692596435546875
batch i:1203
learning rate0.0013333333333333335, loss:3.056147575378418
batch i:1204
learning rate0.0013333333333333335, loss:2.973970413208008
batch i:1205
learning rate0.0013333333333333335, loss:2.975985050201416
batch i:1206
learning rate0.0013333333333333335, loss:3.0951039791107178
batch i:1207
learning rate0.0013333333333333335, loss:2.9150729179382324
batch i:1208
learning rate0.0013333333333333335, loss:3.0952892303466797
batch i:1209
learning rate0.0013333333333333335, loss:3.14650821685791
batch i:1210
learning rate0.0013333333333333335, loss:3.025695323944092
batch i:1211
learning rate0.0013333333333333335, loss:3.00102162361145
batch i:1212
learning rate0.0013333333333333335, loss:3.0639073848724365
batch i:1213
learning rate0.0013333333333333335, loss:3.1212751865386963
batch i:1214
learning rate0.0013333333333333335, loss:3.15044903755188
batch i:1215
learning rate0.0013333333333333335, loss:3.0433835983276367
batch i:1216
learning rate0.0013333333333333335, loss:3.0824849605560303
batch i:1217
learning rate0.0013333333333333335, loss:3.0640454292297363
batch i:1218
learning rate0.0013333333333333335, loss:3.081453800201416
batch i:1219
learning rate0.0013333333333333335, loss:3.1398000717163086
batch i:1220
learning rate0.0013333333333333335, loss:3.137044906616211
batch i:1221
learning rate0.0013333333333333335, loss:3.136838912963867
batch i:1222
learning rate0.0013333333333333335, loss:3.1143929958343506
batch i:1223
learning rate0.0013333333333333335, loss:3.0913023948669434
batch i:1224
learning rate0.0013333333333333335, loss:3.0259416103363037
batch i:1225
learning rate0.0013333333333333335, loss:3.148651361465454
batch i:1226
learning rate0.0013333333333333335, loss:3.107206344604492
batch i:1227
learning rate0.0013333333333333335, loss:3.1058993339538574
batch i:1228
learning rate0.0013333333333333335, loss:3.0608253479003906
batch i:1229
learning rate0.0013333333333333335, loss:3.0768537521362305
batch i:1230
learning rate0.0013333333333333335, loss:3.120978593826294
batch i:1231
learning rate0.0013333333333333335, loss:3.150796890258789
batch i:1232
learning rate0.0013333333333333335, loss:3.0586373805999756
batch i:1233
learning rate0.0013333333333333335, loss:3.0720949172973633
batch i:1234
learning rate0.0013333333333333335, loss:3.100717067718506
batch i:1235
learning rate0.0013333333333333335, loss:3.076878070831299
batch i:1236
learning rate0.0013333333333333335, loss:3.070793390274048
batch i:1237
learning rate0.0013333333333333335, loss:3.0453009605407715
batch i:1238
learning rate0.0013333333333333335, loss:3.041572093963623
batch i:1239
learning rate0.0013333333333333335, loss:3.117624282836914
batch i:1240
learning rate0.0013333333333333335, loss:3.110931634902954
batch i:1241
learning rate0.0013333333333333335, loss:3.14709734916687
batch i:1242
learning rate0.0013333333333333335, loss:3.080815553665161
batch i:1243
learning rate0.0013333333333333335, loss:3.1137099266052246
batch i:1244
learning rate0.0013333333333333335, loss:3.0722131729125977
batch i:1245
learning rate0.0013333333333333335, loss:3.0014100074768066
batch i:1246
learning rate0.0013333333333333335, loss:3.1529877185821533
batch i:1247
learning rate0.0013333333333333335, loss:3.101771116256714
batch i:1248
learning rate0.0013333333333333335, loss:3.0466513633728027
batch i:1249
learning rate0.0013333333333333335, loss:2.98929500579834
batch i:1250
learning rate0.0013333333333333335, loss:3.0849263668060303
current self-play batch: 1250
num_playouts:2000, win: 7, lose: 3, tie:0
average time: 354.08064975738523
New best policy from pure MCTS
batch i:1251
learning rate0.0013333333333333335, loss:3.023169994354248
batch i:1252
learning rate0.0013333333333333335, loss:3.0293946266174316
batch i:1253
learning rate0.0013333333333333335, loss:3.0950889587402344
batch i:1254
learning rate0.0013333333333333335, loss:3.0637500286102295
batch i:1255
learning rate0.0013333333333333335, loss:3.0744991302490234
batch i:1256
learning rate0.0013333333333333335, loss:2.9966564178466797
batch i:1257
learning rate0.0013333333333333335, loss:3.167786121368408
batch i:1258
learning rate0.0013333333333333335, loss:2.9858148097991943
batch i:1259
learning rate0.0013333333333333335, loss:3.0477561950683594
batch i:1260
learning rate0.0013333333333333335, loss:3.1337709426879883
batch i:1261
learning rate0.0013333333333333335, loss:3.1017212867736816
batch i:1262
learning rate0.0013333333333333335, loss:3.0461108684539795
batch i:1263
learning rate0.0013333333333333335, loss:3.1128175258636475
batch i:1264
learning rate0.0013333333333333335, loss:3.0510683059692383
batch i:1265
learning rate0.0013333333333333335, loss:3.02001953125
batch i:1266
learning rate0.0013333333333333335, loss:3.0040245056152344
batch i:1267
learning rate0.0013333333333333335, loss:3.0075292587280273
batch i:1268
learning rate0.0013333333333333335, loss:2.950920820236206
batch i:1269
learning rate0.0013333333333333335, loss:3.031897783279419
batch i:1270
learning rate0.0013333333333333335, loss:3.20597243309021
batch i:1271
learning rate0.0013333333333333335, loss:3.1038784980773926
batch i:1272
learning rate0.0013333333333333335, loss:3.081925630569458
batch i:1273
learning rate0.0013333333333333335, loss:3.174818515777588
batch i:1274
learning rate0.0013333333333333335, loss:3.0066680908203125
batch i:1275
learning rate0.0013333333333333335, loss:3.2113356590270996
batch i:1276
learning rate0.0013333333333333335, loss:3.061185836791992
batch i:1277
learning rate0.0013333333333333335, loss:3.1028900146484375
batch i:1278
learning rate0.0013333333333333335, loss:3.0947766304016113
batch i:1279
learning rate0.0013333333333333335, loss:2.9922003746032715
batch i:1280
learning rate0.0013333333333333335, loss:3.0571837425231934
batch i:1281
learning rate0.0013333333333333335, loss:3.020644187927246
batch i:1282
learning rate0.0013333333333333335, loss:3.0554189682006836
batch i:1283
learning rate0.0013333333333333335, loss:2.909966468811035
batch i:1284
learning rate0.0013333333333333335, loss:2.9803385734558105
batch i:1285
learning rate0.0013333333333333335, loss:2.991126298904419
batch i:1286
learning rate0.0013333333333333335, loss:2.948737144470215
batch i:1287
learning rate0.0013333333333333335, loss:2.9481353759765625
batch i:1288
learning rate0.0013333333333333335, loss:3.0712499618530273
batch i:1289
learning rate0.0013333333333333335, loss:2.9126410484313965
batch i:1290
learning rate0.0013333333333333335, loss:2.9274563789367676
batch i:1291
learning rate0.0013333333333333335, loss:3.0477840900421143
batch i:1292
learning rate0.0013333333333333335, loss:3.0002474784851074
batch i:1293
learning rate0.0013333333333333335, loss:2.977325439453125
batch i:1294
learning rate0.0013333333333333335, loss:3.0413200855255127
batch i:1295
learning rate0.0013333333333333335, loss:3.006697177886963
batch i:1296
learning rate0.0013333333333333335, loss:3.0669989585876465
batch i:1297
learning rate0.0013333333333333335, loss:2.914917469024658
batch i:1298
learning rate0.0013333333333333335, loss:2.979038715362549
batch i:1299
learning rate0.0013333333333333335, loss:3.011383533477783
batch i:1300
learning rate0.0013333333333333335, loss:3.017681360244751
batch i:1301
learning rate0.0013333333333333335, loss:3.083249807357788
batch i:1302
learning rate0.0013333333333333335, loss:2.953094482421875
batch i:1303
learning rate0.0013333333333333335, loss:2.99429988861084
batch i:1304
learning rate0.0013333333333333335, loss:2.978386402130127
batch i:1305
learning rate0.0013333333333333335, loss:3.0227999687194824
batch i:1306
learning rate0.0013333333333333335, loss:3.1018614768981934
batch i:1307
learning rate0.0013333333333333335, loss:3.0327234268188477
batch i:1308
learning rate0.0013333333333333335, loss:2.993227958679199
batch i:1309
learning rate0.0013333333333333335, loss:2.913969039916992
batch i:1310
learning rate0.0013333333333333335, loss:3.0621700286865234
batch i:1311
learning rate0.0013333333333333335, loss:3.1055727005004883
batch i:1312
learning rate0.0013333333333333335, loss:3.045909881591797
batch i:1313
learning rate0.0013333333333333335, loss:3.0264058113098145
batch i:1314
learning rate0.0013333333333333335, loss:3.019944667816162
batch i:1315
learning rate0.0013333333333333335, loss:3.0634665489196777
batch i:1316
learning rate0.0013333333333333335, loss:3.106161594390869
batch i:1317
learning rate0.0013333333333333335, loss:3.0109543800354004
batch i:1318
learning rate0.0013333333333333335, loss:2.9717893600463867
batch i:1319
learning rate0.0013333333333333335, loss:3.0510144233703613
batch i:1320
learning rate0.0013333333333333335, loss:2.8992629051208496
batch i:1321
learning rate0.0013333333333333335, loss:3.0632712841033936
batch i:1322
learning rate0.0013333333333333335, loss:3.047511100769043
batch i:1323
learning rate0.0013333333333333335, loss:3.029949188232422
batch i:1324
learning rate0.0013333333333333335, loss:3.0372157096862793
batch i:1325
learning rate0.0013333333333333335, loss:3.0198023319244385
batch i:1326
learning rate0.0013333333333333335, loss:3.1166532039642334
batch i:1327
learning rate0.0013333333333333335, loss:3.109647512435913
batch i:1328
learning rate0.0013333333333333335, loss:3.138701915740967
batch i:1329
learning rate0.0013333333333333335, loss:3.1277952194213867
batch i:1330
learning rate0.0013333333333333335, loss:3.073820114135742
batch i:1331
learning rate0.0013333333333333335, loss:3.0201869010925293
batch i:1332
learning rate0.0013333333333333335, loss:2.9997518062591553
batch i:1333
learning rate0.0013333333333333335, loss:3.173619270324707
batch i:1334
learning rate0.0013333333333333335, loss:2.992495059967041
batch i:1335
learning rate0.0013333333333333335, loss:2.996469736099243
batch i:1336
learning rate0.0013333333333333335, loss:3.0339255332946777
batch i:1337
learning rate0.0013333333333333335, loss:3.0384042263031006
batch i:1338
learning rate0.0013333333333333335, loss:3.0055689811706543
batch i:1339
learning rate0.0013333333333333335, loss:2.975698471069336
batch i:1340
learning rate0.0013333333333333335, loss:3.0342516899108887
batch i:1341
learning rate0.0013333333333333335, loss:2.9589767456054688
batch i:1342
learning rate0.0013333333333333335, loss:3.0548276901245117
batch i:1343
learning rate0.0013333333333333335, loss:2.969101905822754
batch i:1344
learning rate0.0013333333333333335, loss:2.9405627250671387
batch i:1345
learning rate0.0013333333333333335, loss:3.030135154724121
batch i:1346
learning rate0.0013333333333333335, loss:3.0024232864379883
batch i:1347
learning rate0.0013333333333333335, loss:2.9722368717193604
batch i:1348
learning rate0.0013333333333333335, loss:2.9176535606384277
batch i:1349
learning rate0.0013333333333333335, loss:3.0292835235595703
batch i:1350
learning rate0.0013333333333333335, loss:2.9529290199279785
batch i:1351
learning rate0.0013333333333333335, loss:2.9979186058044434
batch i:1352
learning rate0.0013333333333333335, loss:3.0683672428131104
batch i:1353
learning rate0.0013333333333333335, loss:3.0884666442871094
batch i:1354
learning rate0.0013333333333333335, loss:2.9988348484039307
batch i:1355
learning rate0.0013333333333333335, loss:3.050177574157715
batch i:1356
learning rate0.0013333333333333335, loss:3.0406508445739746
batch i:1357
learning rate0.0013333333333333335, loss:2.9981179237365723
batch i:1358
learning rate0.0013333333333333335, loss:3.059831142425537
batch i:1359
learning rate0.0013333333333333335, loss:3.0608322620391846
batch i:1360
learning rate0.0013333333333333335, loss:3.0238404273986816
batch i:1361
learning rate0.0013333333333333335, loss:3.064596652984619
batch i:1362
learning rate0.0013333333333333335, loss:3.0388526916503906
batch i:1363
learning rate0.0013333333333333335, loss:2.9622626304626465
batch i:1364
learning rate0.0013333333333333335, loss:2.926168441772461
batch i:1365
learning rate0.0013333333333333335, loss:2.9267897605895996
batch i:1366
learning rate0.0013333333333333335, loss:2.986624240875244
batch i:1367
learning rate0.0013333333333333335, loss:2.920452117919922
batch i:1368
learning rate0.0013333333333333335, loss:2.9930689334869385
batch i:1369
learning rate0.0013333333333333335, loss:2.972947597503662
batch i:1370
learning rate0.0013333333333333335, loss:2.9146857261657715
batch i:1371
learning rate0.0013333333333333335, loss:2.916088104248047
batch i:1372
learning rate0.0013333333333333335, loss:3.041848659515381
batch i:1373
learning rate0.0013333333333333335, loss:2.892601251602173
batch i:1374
learning rate0.0013333333333333335, loss:2.9394702911376953
batch i:1375
learning rate0.0013333333333333335, loss:2.9660749435424805
batch i:1376
learning rate0.0013333333333333335, loss:2.999464988708496
batch i:1377
learning rate0.0013333333333333335, loss:2.8369569778442383
batch i:1378
learning rate0.0013333333333333335, loss:2.8452324867248535
batch i:1379
learning rate0.0013333333333333335, loss:2.9768495559692383
batch i:1380
learning rate0.0013333333333333335, loss:2.911649703979492
batch i:1381
learning rate0.0013333333333333335, loss:2.9125823974609375
batch i:1382
learning rate0.0013333333333333335, loss:2.855706214904785
batch i:1383
learning rate0.0013333333333333335, loss:2.9878573417663574
batch i:1384
learning rate0.0013333333333333335, loss:2.893472194671631
batch i:1385
learning rate0.0013333333333333335, loss:2.9896721839904785
batch i:1386
learning rate0.0013333333333333335, loss:2.9557573795318604
batch i:1387
learning rate0.0013333333333333335, loss:2.904771566390991
batch i:1388
learning rate0.0013333333333333335, loss:3.0057106018066406
batch i:1389
learning rate0.0013333333333333335, loss:2.8599307537078857
batch i:1390
learning rate0.0013333333333333335, loss:2.9320926666259766
batch i:1391
learning rate0.0013333333333333335, loss:2.9936671257019043
batch i:1392
learning rate0.0013333333333333335, loss:3.0471301078796387
batch i:1393
learning rate0.0013333333333333335, loss:2.8269333839416504
batch i:1394
learning rate0.0013333333333333335, loss:2.9290401935577393
batch i:1395
learning rate0.0013333333333333335, loss:2.9320406913757324
batch i:1396
learning rate0.0013333333333333335, loss:2.956284999847412
batch i:1397
learning rate0.0013333333333333335, loss:2.855672836303711
batch i:1398
learning rate0.0013333333333333335, loss:2.8623886108398438
batch i:1399
learning rate0.0013333333333333335, loss:2.9044876098632812
batch i:1400
learning rate0.0013333333333333335, loss:2.9598021507263184
batch i:1401
learning rate0.0013333333333333335, loss:2.8741393089294434
batch i:1402
learning rate0.0013333333333333335, loss:2.878340721130371
batch i:1403
learning rate0.0013333333333333335, loss:2.8881192207336426
batch i:1404
learning rate0.0013333333333333335, loss:2.8651909828186035
batch i:1405
learning rate0.0013333333333333335, loss:2.8729491233825684
batch i:1406
learning rate0.0013333333333333335, loss:2.9089183807373047
batch i:1407
learning rate0.0013333333333333335, loss:2.8995187282562256
batch i:1408
learning rate0.0013333333333333335, loss:2.8931503295898438
batch i:1409
learning rate0.0013333333333333335, loss:2.933396339416504
batch i:1410
learning rate0.0013333333333333335, loss:2.8686819076538086
batch i:1411
learning rate0.0013333333333333335, loss:2.924269437789917
batch i:1412
learning rate0.0013333333333333335, loss:2.8416411876678467
batch i:1413
learning rate0.0013333333333333335, loss:2.942610263824463
batch i:1414
learning rate0.0013333333333333335, loss:2.910515308380127
batch i:1415
learning rate0.0013333333333333335, loss:2.883476734161377
batch i:1416
learning rate0.0013333333333333335, loss:2.9541759490966797
batch i:1417
learning rate0.0013333333333333335, loss:2.883051872253418
batch i:1418
learning rate0.0013333333333333335, loss:3.0074214935302734
batch i:1419
learning rate0.0013333333333333335, loss:2.9172914028167725
batch i:1420
learning rate0.0013333333333333335, loss:2.8996338844299316
batch i:1421
learning rate0.0013333333333333335, loss:2.8826308250427246
batch i:1422
learning rate0.0013333333333333335, loss:2.823953151702881
batch i:1423
learning rate0.0013333333333333335, loss:2.9547555446624756
batch i:1424
learning rate0.0013333333333333335, loss:2.909440040588379
batch i:1425
learning rate0.0013333333333333335, loss:2.8935484886169434
batch i:1426
learning rate0.0013333333333333335, loss:2.977843999862671
batch i:1427
learning rate0.0013333333333333335, loss:2.9730544090270996
batch i:1428
learning rate0.0013333333333333335, loss:2.9481992721557617
batch i:1429
learning rate0.0013333333333333335, loss:2.918538808822632
batch i:1430
learning rate0.0013333333333333335, loss:2.886842727661133
batch i:1431
learning rate0.0013333333333333335, loss:2.983159065246582
batch i:1432
learning rate0.0013333333333333335, loss:3.049098491668701
batch i:1433
learning rate0.0013333333333333335, loss:2.9629664421081543
batch i:1434
learning rate0.0013333333333333335, loss:2.884171485900879
batch i:1435
learning rate0.0013333333333333335, loss:2.978724956512451
batch i:1436
learning rate0.0013333333333333335, loss:2.946438789367676
batch i:1437
learning rate0.0013333333333333335, loss:2.9403817653656006
batch i:1438
learning rate0.0013333333333333335, loss:2.8614749908447266
batch i:1439
learning rate0.0013333333333333335, loss:2.8557181358337402
batch i:1440
learning rate0.0013333333333333335, loss:2.904818534851074
batch i:1441
learning rate0.0013333333333333335, loss:2.8665480613708496
batch i:1442
learning rate0.0013333333333333335, loss:2.927267551422119
batch i:1443
learning rate0.0013333333333333335, loss:2.9231131076812744
batch i:1444
learning rate0.0013333333333333335, loss:2.890676975250244
batch i:1445
learning rate0.0013333333333333335, loss:2.938196897506714
batch i:1446
learning rate0.0013333333333333335, loss:2.9328627586364746
batch i:1447
learning rate0.0013333333333333335, loss:2.965259552001953
batch i:1448
learning rate0.0013333333333333335, loss:2.9510440826416016
batch i:1449
learning rate0.0013333333333333335, loss:2.914085865020752
batch i:1450
learning rate0.0013333333333333335, loss:2.9144110679626465
batch i:1451
learning rate0.0013333333333333335, loss:2.7778537273406982
batch i:1452
learning rate0.0013333333333333335, loss:2.8702785968780518
batch i:1453
learning rate0.0013333333333333335, loss:2.8943159580230713
batch i:1454
learning rate0.0013333333333333335, loss:2.920149326324463
batch i:1455
learning rate0.0013333333333333335, loss:2.9104056358337402
batch i:1456
learning rate0.0013333333333333335, loss:2.898165225982666
batch i:1457
learning rate0.0013333333333333335, loss:2.9043612480163574
batch i:1458
learning rate0.0013333333333333335, loss:2.9712319374084473
batch i:1459
learning rate0.0013333333333333335, loss:2.9209227561950684
batch i:1460
learning rate0.0013333333333333335, loss:2.980851173400879
batch i:1461
learning rate0.0013333333333333335, loss:2.9702951908111572
batch i:1462
learning rate0.0013333333333333335, loss:2.9832143783569336
batch i:1463
learning rate0.0013333333333333335, loss:2.9853456020355225
batch i:1464
learning rate0.0013333333333333335, loss:2.912137508392334
batch i:1465
learning rate0.0013333333333333335, loss:2.961909770965576
batch i:1466
learning rate0.0013333333333333335, loss:2.8417716026306152
batch i:1467
learning rate0.0013333333333333335, loss:2.937160015106201
batch i:1468
learning rate0.0013333333333333335, loss:2.869875907897949
batch i:1469
learning rate0.0013333333333333335, loss:2.868696928024292
batch i:1470
learning rate0.0013333333333333335, loss:2.9348795413970947
batch i:1471
learning rate0.0013333333333333335, loss:2.9188971519470215
batch i:1472
learning rate0.0013333333333333335, loss:2.8870630264282227
batch i:1473
learning rate0.0013333333333333335, loss:2.938690185546875
batch i:1474
learning rate0.0013333333333333335, loss:2.8781628608703613
batch i:1475
learning rate0.0013333333333333335, loss:2.879140853881836
batch i:1476
learning rate0.0013333333333333335, loss:2.872504234313965
batch i:1477
learning rate0.0013333333333333335, loss:2.8072474002838135
batch i:1478
learning rate0.0013333333333333335, loss:2.9202122688293457
batch i:1479
learning rate0.0013333333333333335, loss:2.8601109981536865
batch i:1480
learning rate0.0013333333333333335, loss:2.925025224685669
batch i:1481
learning rate0.0013333333333333335, loss:2.9055280685424805
batch i:1482
learning rate0.0013333333333333335, loss:2.8230929374694824
batch i:1483
learning rate0.0013333333333333335, loss:2.9739367961883545
batch i:1484
learning rate0.0013333333333333335, loss:2.8376519680023193
batch i:1485
learning rate0.0013333333333333335, loss:2.886904001235962
batch i:1486
learning rate0.0013333333333333335, loss:2.848294258117676
batch i:1487
learning rate0.0013333333333333335, loss:2.947213649749756
batch i:1488
learning rate0.0013333333333333335, loss:2.9178998470306396
batch i:1489
learning rate0.0013333333333333335, loss:2.8044791221618652
batch i:1490
learning rate0.0013333333333333335, loss:2.857126235961914
batch i:1491
learning rate0.0013333333333333335, loss:2.9087672233581543
batch i:1492
learning rate0.0013333333333333335, loss:2.9108214378356934
batch i:1493
learning rate0.0013333333333333335, loss:2.973391532897949
batch i:1494
learning rate0.0013333333333333335, loss:2.862722635269165
batch i:1495
learning rate0.0013333333333333335, loss:2.776618003845215
batch i:1496
learning rate0.0013333333333333335, loss:2.865993022918701
batch i:1497
learning rate0.0013333333333333335, loss:2.981414318084717
batch i:1498
learning rate0.0013333333333333335, loss:2.8361692428588867
batch i:1499
learning rate0.0013333333333333335, loss:2.8974556922912598
batch i:1500
learning rate0.0013333333333333335, loss:2.77547025680542
current self-play batch: 1500
num_playouts:2000, win: 10, lose: 0, tie:0
average time: 431.85240960121155
New best policy from pure MCTS
